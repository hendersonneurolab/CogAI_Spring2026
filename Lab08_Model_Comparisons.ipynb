{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hendersonneurolab/CogAI_Fall2025/blob/master/Lab08_Model_Comparisons.ipynb)\n","\n","## Week 8: Compare different fMRI encoding models.\n","\n","This week we will continue working with fMRI encoding models, building on what we covered in Lab 7. We will use data from the Natural Scenes Dataset (NSD), and features from the AlexNet DNN model at various layers. We will explore how to compare performance of different models using variance partitioning, and how to plot data on a cortical surface map using the software library PyCortex.\n","\n","**Learning Objectives**\n","- Understand the basic procedure involved in variance partitioning, and how to compute the unique variance and shared variance between models.\n","- Know how to interpret results when plotted on a flattened cortical surface."],"metadata":{"id":"nbtEAybij279"}},{"cell_type":"markdown","source":["NOTE: before you start, make sure your runtime is set to T4: GPU. (Use menu in the top right, \"change runtime type\")."],"metadata":{"id":"ASKWItAqjMQt"}},{"cell_type":"markdown","source":["###Step 1: Installation and importing"],"metadata":{"id":"TUudpEoClPil"}},{"cell_type":"markdown","source":["First, we need to install PyCortex and some dependencies.\n","These are not included with Colab by default. They will be needed in order to make brain map plots. This part should take ~2 minutes, let the instructor know if you encounter any issues."],"metadata":{"id":"E8VznyzAikl-"}},{"cell_type":"code","source":["import time\n","st = time.time()\n","# First, install some required dependencies\n","# !pip install -U setuptools wheel numpy cython\n","# Install the latest release of pycortex from pip\n","!pip install -U pycortex\n","print('elapsed = %.2f sec'%(time.time() - st))"],"metadata":{"id":"EQAejoR6FmH3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This installs inkscape, it will be needed for the flatmap plots.\n","st = time.time()\n","# Install Inkscape\n","!apt-get update\n","!apt-get install -y inkscape\n","# Verify installation\n","!inkscape --version\n","print('elapsed = %.2f sec'%(time.time() - st))"],"metadata":{"id":"H0ifw82Yib8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now proceed with imports of other packages."],"metadata":{"id":"EaLzPZfPjEDN"}},{"cell_type":"code","source":["import numpy as np\n","import urllib.request\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import scipy\n","import os, sys\n","import h5py\n","import time\n","import torch\n","import zipfile\n","import copy\n","import warnings\n","import shutil\n","warnings.filterwarnings('ignore')\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"5eFcRNENAnAD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Step 2: Download and load the data.\n","\n","Like last week, we're using NSD data and features that are computed from AlexNet."],"metadata":{"id":"OuI7K6flH5ro"}},{"cell_type":"markdown","source":["\n","First, mount the Google Drive storage."],"metadata":{"id":"0iQMw9EuV90p"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# Navigate to the Colab Notebooks folder\n","colab_notebooks_path = '/content/drive/MyDrive/Colab Notebooks/'\n","os.chdir(colab_notebooks_path)\n","os.makedirs('CogAI', exist_ok=True)\n","os.makedirs('CogAI/data', exist_ok=True)\n","data_folder = os.path.join(colab_notebooks_path, 'CogAI', 'data')\n","print(data_folder)"],"metadata":{"id":"mvWAbPsc1lKw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download the \"PyCortex database\" files (pycortex_db). These are files that PyCortex needs in order to make plots of data on an individual subject's cortical surface. They include information about the subject's cortical surface anatomy, and how to map from volume space to surface space.\n","\n","You can learn more about PyCortex here: https://gallantlab.org/pycortex/install.html"],"metadata":{"id":"6bKexgoLkKSX"}},{"cell_type":"code","source":["# Info about pycortex database (need this to make the brain map plots)\n","dbox_link = 'https://www.dropbox.com/scl/fi/9737r06i53b0x672qnns9/pycortex_subj01.zip?rlkey=61vjlcv2p5dp0o05ff1bd0lnj&st=knf4uh33&dl=1'\n","filename = os.path.join(data_folder, 'S1_pycortex.zip')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)\n","\n","# unzip this file\n","st = time.time()\n","cortex_data_path = os.path.join(data_folder, 'pycortex_db')\n","folder_unzipped = cortex_data_path\n","with zipfile.ZipFile(filename, 'r') as zip_ref:\n","    zip_ref.extractall(folder_unzipped)\n","print('elapsed = %.2f seconds'%(time.time() - st))"],"metadata":{"id":"OGI3SaIRGNzd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download a pycortex configuration file\n","dbox_link = 'https://www.dropbox.com/scl/fi/3ircd1y0s5jznpiyk2i1d/pycortex_config.cfg?rlkey=mz0gqvwlxoekdz3nxp1xowmmo&st=hv9fdozs&dl=1'\n","filename = os.path.join(data_folder, 'pycortex_config.cfg')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)\n","\n","# We're replacing the default pycortex config file with this version, which sets our subject database folder.\n","# When pycortex is imported, it should use this file.\n","config_file = os.path.join(data_folder, 'pycortex_config.cfg')\n","default_config_path = '/root/.config/pycortex/options.cfg'\n","os.makedirs(os.path.dirname(default_config_path), exist_ok=True)\n","shutil.copyfile(config_file, default_config_path)"],"metadata":{"id":"_pFYLBwkdyI6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now import PyCortex:"],"metadata":{"id":"rUUXY9n_lC1m"}},{"cell_type":"code","source":["import cortex\n","cortex.db.filestore, cortex.db.subjects"],"metadata":{"id":"B-905QJveOru"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download data files for this exercise. Note that several of these files were used in last week's lab too (Lab07), so you might already have them in your Drive storage."],"metadata":{"id":"UfqwHo01kjj0"}},{"cell_type":"code","source":["# Info about ROIs\n","dbox_link = 'https://www.dropbox.com/scl/fi/kilrzj841mrpm17aj9gid/S1_voxel_roi_info.npy?rlkey=jgt1zje70ta8qpmaib8kmjskp&st=n9b3bxft&dl=1'\n","filename = os.path.join(data_folder, 'S1_voxel_roi_info.npy')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"IyZyF0ND5CoB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Info about images\n","dbox_link = 'https://www.dropbox.com/scl/fi/caabn3on6l9q8w32uxq8z/S1_image_info.csv?rlkey=cwb4mfruzcyrozrmdrfgerpp2&st=fg9i8ml3&dl=1'\n","filename = os.path.join(data_folder, 'S1_image_info.csv')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"WvLXJ6rT4z6X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["fMRI data: this one can take a while to download, but shouldn't take more than 2 minutes. If it's taking too long, check with the professor."],"metadata":{"id":"md2A4kgOk37c"}},{"cell_type":"code","source":["# fMRI data\n","dbox_link = 'https://www.dropbox.com/scl/fi/e040hit5avrptp4hbnijy/S1_betas_avg_bigmask.hdf5?rlkey=s8bpoara1fln1dhf1ydjpuldn&st=323ct1jm&dl=1'\n","filename = os.path.join(data_folder, 'S1_betas_avg_bigmask.hdf5')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"dtq7XKDp1VjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download DNN features:"],"metadata":{"id":"8h0dVsKvkswC"}},{"cell_type":"code","source":["dbox_links = ['https://www.dropbox.com/scl/fi/ck4al0qbe0tuljrky48uh/NSD_S1_ims224pix_Conv1_ReLU_pca100.hdf5?rlkey=6ppjrfkaii7z7k5hiz3zutzqf&st=fobms0tb&dl=1', \\\n","              'https://www.dropbox.com/scl/fi/ilr537zl5wzkv9hacostg/NSD_S1_ims224pix_Conv2_ReLU_pca100.hdf5?rlkey=hsgr41ppossjsl1cqxp97eee3&st=mym3o32m&dl=1', \\\n","              'https://www.dropbox.com/scl/fi/s6sdl8gsx1nzkykjdl6a8/NSD_S1_ims224pix_Conv3_ReLU_pca100.hdf5?rlkey=3xpos62w09s3wrkc87w18yadr&st=eu01sdzv&dl=1', \\\n","              'https://www.dropbox.com/scl/fi/rwvz3m7hij6dptmw58nfg/NSD_S1_ims224pix_Conv4_ReLU_pca100.hdf5?rlkey=rie88fpywjn2ncpw9sridme19&st=60jf3zep&dl=1', \\\n","              'https://www.dropbox.com/scl/fi/a8ksxnq380ycop4cc0cmh/NSD_S1_ims224pix_Conv5_ReLU_pca100.hdf5?rlkey=pc4k8mfaafh7t9dnwygpsnt3n&st=wgju9hox&dl=1']\n","layers = ['Conv1', 'Conv2', 'Conv3', 'Conv4', 'Conv5']\n","\n","for link, layer in zip(dbox_links, layers):\n","\n","  filename = os.path.join(data_folder, 'S1_%s_pca100.hdf5'%layer)\n","  # filename = os.path.join(data_folder, 'S1_%s.hdf5'%layer)\n","  if not os.path.exists(filename):\n","    st = time.time()\n","    print('downloading to %s...'%filename)\n","    urllib.request.urlretrieve(link, filename)\n","    print('elapsed = %.2f seconds'%(time.time() - st))\n","  else:\n","    print('We already have: %s'%filename)"],"metadata":{"id":"cUwQ7uop5Wjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that the files are downloaded, we need to load them into Python.\n"],"metadata":{"id":"lQ1ZRIjU7HHT"}},{"cell_type":"markdown","source":["First, load the **image information**. This is a .csv file that contains info about all the images shown."],"metadata":{"id":"Y677VPjk64Rk"}},{"cell_type":"code","source":["info_fn = os.path.join(data_folder, 'S1_image_info.csv')\n","print(info_fn)\n","info = pd.read_csv(info_fn)\n","# this df has [10,000] elements. Each element is 1 unique image.\n","# it contains info about the images and where they came from (within the MS COCO dataset).\n","# the rows of this correspond exactly to the features files (which will be loaded below).\n","\n","image_order = np.array(info['unique_ims'])\n","n_reps = np.array(info['n_reps'])"],"metadata":{"id":"8E868yQ-B26i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Next, load the **fMRI data**.\n","\n","The data (betas_avg_bigmask) is organized as: [images x voxels]\n","\n","Each image was shown multiple times, and these values capture average response to each image.\n","\n","Keep in mind this is already several steps of preprocessing removed from the raw data. Steps like motion correction have already been performed to improve signal quality.\n","\n","Single-trial beta weights were already extracted using a GLM analysis (similar to what we did last week).\n","\n","Data have also been z-scored, within each session, and the beta weights for repetitions of each image have been averaged.\n","\n","Note also that the voxels in this matrix are not the entire brain. They represent a wide portion of visual cortex, including all voxels with reliable signal.\n","   \n"],"metadata":{"id":"6W_BGkrSl_Aj"}},{"cell_type":"code","source":["data_filename = os.path.join(data_folder, 'S1_betas_avg_bigmask.hdf5')\n","print(data_filename)\n","\n","t = time.time()\n","with h5py.File(data_filename, 'r') as data_set:\n","    values = np.copy(data_set['/betas'])\n","    data_set.close()\n","elapsed = time.time() - t\n","print('Took %.5f seconds to load file'%elapsed)\n","\n","# Some of these values may be nans, only for some subjects\n","# this is for subjects who didn't complete all 40 sessions of NSD experiment.\n","# make sure we remove the nans now.\n","# for subject 1: we should have all the data, no nans.\n","good_values = ~np.isnan(values[:,0])\n","print(values.shape)\n","print(np.sum(~good_values))\n","\n","voxel_data = values[good_values,:]\n","print(voxel_data.shape)\n","\n","# check that nans are exactly where we expect\n","# nans happen when n_reps=0\n","assert(np.all(good_values[n_reps>0]))\n","assert(np.all(~good_values[n_reps==0]))"],"metadata":{"id":"kIwt3yTB7EC_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load info about the **ROIs (regions of interest)** in this dataset. Conveniently, the labels for these regions are already provided for us."],"metadata":{"id":"aShMdaYNrVL6"}},{"cell_type":"code","source":["# ROI = region of interest. These are visual areas we want to focus on for analysis.\n","fn = os.path.join(data_folder, 'S1_voxel_roi_info.npy')\n","print(fn)\n","rinfo = np.load(fn, allow_pickle=True).item()\n","# this is a dictionary that contains information about which voxels our data will include.\n","# voxel_mask is the whole set of voxels we're focusing on. basically all of visual cortex.\n","voxel_mask = rinfo['voxel_mask']\n","\n","# noise ceiling: this is already computed, this tells us the maximum explainable variance in the data.\n","# like a \"ceiling\" for encoding model performance.\n","noise_ceiling = rinfo['noise_ceiling_avgreps'] / 100\n","\n"],"metadata":{"id":"TCN3IruBrUWB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the **DNN features (activations)**, which will be used to construct the encoding model.\n","\n","These are features from AlexNet, a large CNN model pre-trained on ImageNet. We pre-computed these features ahead of time, from several different layers of the model.\n","\n","These features are organized as [n_images x n_features], where the images are in the same order as the fMRI data. So, each row in the features matrices corresponds to one row in the fMRI data matrix."],"metadata":{"id":"d1j9HrI67mfY"}},{"cell_type":"code","source":["fdata = dict([])\n","for layer in layers:\n","  filename = os.path.join(data_folder, 'S1_%s_pca100.hdf5'%layer)\n","  print(filename)\n","  with h5py.File(filename, 'r') as f:\n","      # Explore the file structure\n","      print(\"Keys in file:\", list(f.keys()))\n","\n","      # # Load your data (adjust based on your file structure)\n","      data = np.array(f['features'])\n","      fdata[layer] = data\n","\n","[f.shape for f in fdata.values()]"],"metadata":{"id":"_Z4BQidA7ntt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Step 3: Set up the functions needed for ridge regression.\n","\n","This part is similar to what we did in the last lab.\n","\n","During ridge regression, we typically split our data into 3 independent sets of images:\n","\n","- **Training data:** used to fit the model weights\n","- **Holdout data** (nested validation): used to choose best pRF and ridge parameters\n","- **Validation data**: held out until the very end, used to compute the validation set $R^2$.\n"],"metadata":{"id":"okOvE71yJFXG"}},{"cell_type":"markdown","source":["Split the data into training, holdout, and testing data."],"metadata":{"id":"S21xlBqtIpwH"}},{"cell_type":"code","source":["# fixed random seed, to make sure shuffling is repeatable\n","rndseeds = [171301, 42102, 490304, 521005, 11407, 501610, 552211, 450013, 824387]\n","subject = 1\n","si = subject-1 # remember python is zero-indexed. but subjects are one-indexed.\n","\n","# Always holding out 1000 \"shared images\", which were seen by all NSD participants, as\n","# the validation set.\n","val_inds = np.array(info['shared1000'])\n","\n","# Then take a random 10% of the remaining data, as the nested \"holdout\" set.\n","# Holdout set is used to choose ridge parameters and pRF parameters.\n","# You could experiment with different % holdout, 10% usually works well.\n","pct_holdout = 0.10\n","n_images_total = info.shape[0]\n","n_images_notval = np.sum(~val_inds);\n","n_images_holdout = int(np.ceil(n_images_notval*pct_holdout))\n","n_images_trn = n_images_notval - n_images_holdout\n","\n","inds_notval = np.where(~val_inds)[0]\n","np.random.seed(rndseeds[si])\n","np.random.shuffle(inds_notval) # this is the only random part\n","\n","inds_trn = inds_notval[0:n_images_trn]\n","inds_holdout = inds_notval[n_images_trn:]\n","assert(len(inds_holdout)==n_images_holdout)\n","\n","trn_inds = np.isin(np.arange(0, n_images_total), inds_trn)\n","holdout_inds = np.isin(np.arange(0, n_images_total), inds_holdout)\n","\n","# remove nan rows here\n","trn_inds = trn_inds[good_values]\n","val_inds = val_inds[good_values]\n","holdout_inds = holdout_inds[good_values]\n","\n","# apply these indices to split the voxel data and image labels.\n","voxel_data_trn = voxel_data[trn_inds, :]\n","voxel_data_val = voxel_data[val_inds, :]\n","voxel_data_holdout = voxel_data[holdout_inds, :]\n","\n","n_voxels = voxel_data_trn.shape[1]\n","print(voxel_data_trn.shape, voxel_data_val.shape, voxel_data_holdout.shape)\n","\n","image_order_use = image_order[good_values]\n","\n","image_inds_trn = image_order_use[trn_inds]\n","image_inds_val = image_order_use[val_inds]\n","image_inds_holdout = image_order_use[holdout_inds]"],"metadata":{"id":"vs4WKXWlCAd8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make a function that splits the features into training, holdout, and validation partitions, and z-scores the features."],"metadata":{"id":"tVNXf1fpJU7r"}},{"cell_type":"code","source":["def split_normalize_feats(f):\n","\n","    f_trn = f[trn_inds,:]\n","    f_val = f[val_inds,:]\n","    f_out = f[holdout_inds,:]\n","\n","    # Z-score the data - this is a step that helps with fit stability.\n","    # I'm computing the normalization parameters (mean and std) on my training data only\n","    # (plus the nested held-out partition), but not the val set.\n","    # this helps reduce leakage of data between train and val partitions.\n","    # then apply those same normalization parameters to the val set too.\n","    f_concat = np.concatenate([f_trn, f_out], axis=0)\n","    # f_concat = f_trn\n","\n","    features_m = np.mean(f_concat, axis=0, keepdims=True) #[:trn_size]\n","    # print(features_m[0,0:10])\n","    features_s = np.std(f_concat, axis=0, keepdims=True) + 1e-6\n","\n","    f_trn -= features_m\n","    f_trn /= features_s\n","    f_out -= features_m\n","    f_out /= features_s\n","    f_val -= features_m\n","    f_val /= features_s\n","\n","    # add the intercept: a column of ones\n","    f_trn = np.concatenate([f_trn, np.ones(shape=(len(f_trn), 1), dtype=f_trn.dtype)], axis=1)\n","    f_out = np.concatenate([f_out, np.ones(shape=(len(f_out), 1), dtype=f_out.dtype)], axis=1)\n","    f_val = np.concatenate([f_val, np.ones(shape=(len(f_val), 1), dtype=f_val.dtype)], axis=1)\n","\n","    #\n","    return f_trn, f_val, f_out"],"metadata":{"id":"urmFQ20pAoc3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the candidate lambda values for ridge regression."],"metadata":{"id":"FsxRUboBJnvo"}},{"cell_type":"code","source":["# lambda is the ridge penalty, bigger = more regularization\n","n_lambdas = 20\n","lambdas = np.logspace(np.log(0.0001),np.log(10**10+0.01),n_lambdas, dtype=np.float32, base=np.e) - 0.01"],"metadata":{"id":"_qZcppKyCxrK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write a function that returns the set of voxels in any ROI."],"metadata":{"id":"FkSbEzeuzzur"}},{"cell_type":"code","source":["\n","def get_roi_vox(roi_name = 'FFA-2'):\n","\n","  if roi_name in rinfo['ret_prf_roi_names']:\n","    ind_num = rinfo['ret_prf_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_retino'][voxel_mask]==ind_num\n","\n","  elif roi_name in rinfo['floc_face_roi_names']:\n","    ind_num = rinfo['floc_face_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_face'][voxel_mask]==ind_num\n","\n","  elif roi_name in rinfo['floc_place_roi_names']:\n","    ind_num = rinfo['floc_place_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_place'][voxel_mask]==ind_num\n","\n","  elif roi_name in rinfo['floc_body_roi_names']:\n","    ind_num = rinfo['floc_body_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_body'][voxel_mask]==ind_num\n","\n","\n","  elif roi_name=='all':\n","    # return all of vis cortex, very big\n","    roi_inds = np.ones((np.sum(rinfo['voxel_mask']),), dtype=bool)\n","\n","\n","  return roi_inds\n"],"metadata":{"id":"Lt7AhL_FzmCF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["List all of the possible ROIs in the dataset. Retinotopic ROIS (ret_prf) are early visual areas defined using retinotopic mapping. Face-selective, place-selective and body-selective ROIs are higher visual areas defined using category localizers."],"metadata":{"id":"cQ7hGWcRmlx5"}},{"cell_type":"code","source":["# Retinotopic ROI definitions:\n","print(rinfo['ret_prf_roi_names'])\n","# Face-selective ROI definitions:\n","print(rinfo['floc_face_roi_names'])\n","# Place-selective ROI definitions:\n","print(rinfo['floc_place_roi_names'])\n","# Body-selective ROI definitions:\n","print(rinfo['floc_body_roi_names'])"],"metadata":{"id":"uSGvvURem6aZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make a function to compute $R^2$."],"metadata":{"id":"_9Kl-b0HJ_kA"}},{"cell_type":"code","source":["def get_r2(actual,predicted):\n","    \"\"\"\n","    This computes the coefficient of determination (R2).\n","    Always goes along first dimension (i.e. the trials/samples dimension)\n","    \"\"\"\n","    ssres = np.sum(np.power((predicted - actual),2), axis=0);\n","    sstot = np.sum(np.power((actual - np.mean(actual, axis=0)),2), axis=0);\n","    r2 = 1-(ssres/sstot)\n","\n","    return r2"],"metadata":{"id":"XqzkLXu1C1dK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a function to solve the ridge regression problem. You can look back to Lab 7 for a reminder on the details of this.\n","\n","This function will take as inputs:\n","- **xtrn**: DNN features from a layer of interest. [n_images x n_features]\n","- **vtrn**: Neural data from a brain region of interest. [n_images x n_voxels]\n","- **lambdas**: The candidate values for $\\lambda$, the ridge penalty parameter."],"metadata":{"id":"-H0baUqpKDiL"}},{"cell_type":"code","source":["def solve_ridge_fast(xtrn, vtrn, lambdas):\n","\n","  n_features = xtrn.shape[1]\n","  # xT * x\n","  mult = xtrn.T @ xtrn\n","\n","  # make an identity matrix\n","  ridge_term = torch.eye(xtrn.size()[1], device=device, dtype=torch.float64)\n","\n","  # make versions of this matrix that are adjusted by each possible lambda value\n","  # this is: (X^T @ X + lambda*I)^(-1)\n","  # first dim is the different lambda values.\n","  lambda_matrices = torch.stack([(mult+ridge_term*l).inverse() \\\n","            for l in lambdas], axis=0)\n","\n","  cofactor = torch.tensordot(lambda_matrices, xtrn, dims=[[2],[1]])\n","\n","  # solve for weights\n","  weights = torch.tensordot(cofactor, vtrn, dims=[[2], [0]]) # [#lambdas, #feature, #voxel]\n","\n","  # predict the response on held-out data, using features from held-out data (xout)\n","  pred = torch.tensordot(xout, weights, dims=[[1],[1]]) # [#samples, #lambdas, #voxels]\n","\n","  # compute loss for held-out data\n","  # this will tell us the loss for each of the possible lambda values\n","  loss = torch.sum(torch.pow(vout[:,None,:] - pred, 2), dim=0) # [#lambdas, #voxels]\n","  loss = loss.cpu().numpy()\n","\n","  weights_use = torch.zeros((n_features, vtrn.shape[1]),device=device, dtype=weights.dtype)\n","\n","  # for each voxel, find its best weights\n","  for vi in range(vtrn.shape[1]):\n","    # choose the best lambda value, based on min loss\n","    best_lambda_ind = np.argmin(loss[:,vi])\n","    best_lambda = lambdas[best_lambda_ind]\n","    # print(vi, best_lambda_ind, best_lambda)\n","    weights_use[:, vi] = weights[best_lambda_ind,:,vi]\n","\n","  return weights_use"],"metadata":{"id":"Pifkl1wCC4ut"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Step 4: Fit models for different brain areas and different layers."],"metadata":{"id":"3tBv-ZUbMTZj"}},{"cell_type":"markdown","source":["First, let's run through the fitting for one region and one layer:"],"metadata":{"id":"CmjDXWhHK96X"}},{"cell_type":"code","source":["# pick an ROI\n","target_roi_name = 'V1v'\n","voxel_inds = get_roi_vox(roi_name = target_roi_name)\n","n_voxels = np.sum(voxel_inds)\n","\n","# v is our fMRI data, these are the 3 different splits\n","vtrn = torch.Tensor(voxel_data_trn[:,voxel_inds]).to(device).to(torch.float64)\n","vout = torch.Tensor(voxel_data_holdout[:,voxel_inds]).to(device).to(torch.float64)\n","vval = torch.Tensor(voxel_data_val[:,voxel_inds]).to(device).to(torch.float64)\n","\n","# pick a DNN layer\n","layer_name = 'Conv1'\n","\n","# pull out the features from this DNN layer\n","f = fdata[layer_name]\n","\n","f_trn, f_val, f_out = split_normalize_feats(f)\n","\n","# x is the DNN features, split into partitions\n","xtrn = torch.Tensor(f_trn).to(device).to(torch.float64)\n","xout = torch.Tensor(f_out).to(device).to(torch.float64)\n","xval = torch.Tensor(f_val).to(device).to(torch.float64)\n","\n","# Fit the encoding model weights\n","st = time.time()\n","weights_use = solve_ridge_fast(xtrn, vtrn, lambdas)\n","elapsed = time.time() - st\n","print('elapsed = %.5f s'%elapsed)\n","\n","# predict voxel response in held-out validation data.\n","# yhat = X @ W\n","pred = xval @ weights_use\n","\n","# remember to turn these back into numpy, from torch.\n","# sometimes tensors will give errors in your subsequent numpy code.\n","actual_array = vval.cpu().numpy()\n","pred_array = pred.cpu().numpy()\n","\n","r2 = get_r2(actual_array, pred_array)\n","\n","print(r2.shape)"],"metadata":{"id":"qpD1GudILI4K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 1:***\n","\n","Run the encoding model fitting for different AlexNet layers. Try: \"Conv1\",\"Conv2\",\"Conv3\",\"Conv4\",\"Conv5\". To do this, start by copying the code from above and modifying, you may want to use a loop.\n","\n","Print out the median $R^2$ for each layer.\n","\n","How do the results compare for different layers? Does this make sense?\n"],"metadata":{"id":"qMKRQfW_MaZj"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"yN8129qiMw3P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"_ZE7NIRoMzEO"}},{"cell_type":"markdown","source":["Now, let's try running this same code for a larger group of voxels. Instead of just V1v, we can gather all the voxels in visual cortex, and fit at the same time.\n","\n","NOTE that this part can take a longer time than the above code. Make sure you're attached to the T4: GPU runtime type."],"metadata":{"id":"7Yp4sJoJNEmy"}},{"cell_type":"code","source":["# pick an ROI\n","target_roi_name = 'all' # if you pass in all, we get all visual cortex voxels.\n","voxel_inds = get_roi_vox(roi_name = target_roi_name)\n","n_voxels = np.sum(voxel_inds)\n","\n","# v is our fMRI data, these are the 3 different splits\n","vtrn = torch.Tensor(voxel_data_trn[:,voxel_inds]).to(device).to(torch.float64)\n","vout = torch.Tensor(voxel_data_holdout[:,voxel_inds]).to(device).to(torch.float64)\n","vval = torch.Tensor(voxel_data_val[:,voxel_inds]).to(device).to(torch.float64)\n","\n","# looping over the different layers, saving R2 for each layer.\n","r2_each_layer = np.zeros((len(layers), n_voxels))\n","\n","for li, layer_name in enumerate(layers):\n","\n","  # pull out the features from this DNN layer\n","  f = fdata[layer_name]\n","\n","  f_trn, f_val, f_out = split_normalize_feats(f)\n","\n","  # x is the DNN features, split into partitions\n","  xtrn = torch.Tensor(f_trn).to(device).to(torch.float64)\n","  xout = torch.Tensor(f_out).to(device).to(torch.float64)\n","  xval = torch.Tensor(f_val).to(device).to(torch.float64)\n","\n","  # Fit the encoding model weights\n","  st = time.time()\n","  weights_use = solve_ridge_fast(xtrn, vtrn, lambdas)\n","  elapsed = time.time() - st\n","  print('elapsed = %.5f s'%elapsed)\n","\n","  # predict voxel response in held-out validation data.\n","  # yhat = X @ W\n","  pred = xval @ weights_use\n","\n","  # remember to turn these back into numpy, from torch.\n","  # sometimes tensors will give errors in your subsequent numpy code.\n","  actual_array = vval.cpu().numpy()\n","  pred_array = pred.cpu().numpy()\n","\n","  r2 = get_r2(actual_array, pred_array)\n","\n","  r2_each_layer[li,:] = r2\n","\n","  print('\\nPerformance for %s, %s layer:'%(target_roi_name, layer_name))\n","  print('\\nMin R2: %.3f'%np.min(r2))\n","  print('Max R2: %.3f'%np.max(r2))\n","  print('Median R2: %.3f'%np.median(r2))\n"],"metadata":{"id":"Vz4od0vyNRN0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 2:***\n","\n","For each voxel, calculate which of the AlexNet layers resulted in the best $R^2$. You can use the variable \"r2_each_layer\" for this.\n","\n","For each layer, print out how many of the voxels have that layer as their best layer."],"metadata":{"id":"maHXmib2Ogu7"}},{"cell_type":"code","source":["#[answer here]"],"metadata":{"id":"aKGVw_5IO0qc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"ONJieGOmO29m"}},{"cell_type":"markdown","source":["Now let's visualize these results on the cortical surface. We will use PyCortex for this. In order to do this we need to know which voxels in the matrix we've been working with, correspond to which positions on the cortical surface. In PyCortex, there is a transformation file (.xfm) which provides that information. We need to re-organize the data slightly for this:"],"metadata":{"id":"qEgY_2Z6ObyQ"}},{"cell_type":"code","source":["# this is the data we want to plot: best layer for each cortical voxel\n","# it's an integer 0-4\n","best_layer_each_vox = np.argmax(r2_each_layer, axis=0)\n","\n","# need to put these values into a larger list\n","my_values = np.full(shape = voxel_inds.shape, fill_value = np.nan)\n","my_values[voxel_inds] = best_layer_each_vox\n","\n","# more parameters needed for this\n","xfmname = 'func1pt8_to_anat0pt8_autoFSbbr'\n","# ^this is the pre-computed transform we will use - mapping from functional to anatomical surface space.\n","substr = 'subj%02d'%subject\n","voxel_mask = rinfo['voxel_mask'] # all vis cortex voxels to whole brain mask\n","vol_shape = rinfo['brain_nii_shape'] # size of the 3D volume\n","mask_3d = np.reshape(voxel_mask, vol_shape, order='C') # 3D mask, indicating which voxels are in our matrix.\n"],"metadata":{"id":"ktHltWDuED0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Put the data into a 3D volume: has values for the voxels that we analyzed here, and NaNs for other voxels."],"metadata":{"id":"DzEKGgnoQXiW"}},{"cell_type":"code","source":["def get_full_volume(values, voxel_mask, shape):\n","    \"\"\"\n","    For PyCortex: Put values for voxels that were analyzed back into their\n","    correct coordinates in full volume space matrix.\n","    \"\"\"\n","    voxel_mask_3d = np.reshape(voxel_mask, shape)\n","    full_vals = copy.deepcopy(voxel_mask_3d).astype('float64')\n","    full_vals[voxel_mask_3d==0] = np.nan\n","    full_vals[voxel_mask_3d==1] = values\n","\n","    full_vals = np.moveaxis(full_vals, [0,1,2], [2,1,0])\n","\n","    return full_vals\n","\n","v = get_full_volume(my_values, voxel_mask, vol_shape)"],"metadata":{"id":"WZ8mZnmdRGra"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now make the PyCortex flatmap plot.\n","\n","In this plot, the colors correspond to different numbers, which indicate different AlexNet layers.\n","\n","Blue = 0 = Conv1, and so on."],"metadata":{"id":"AoxGrPobQ4S-"}},{"cell_type":"code","source":["plt.rcParams.update({'font.size': 18})\n","\n","vol = cortex.Volume(data = v, cmap='BPROG', subject=substr, \\\n","                                           vmin=-0.5, vmax=4.5,\\\n","                                           xfmname=xfmname, mask=mask_3d)\n","\n","fig = cortex.quickflat.make_figure(vol, with_curvature=True, with_labels=True)"],"metadata":{"id":"Ja7aB4zVQm4o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","***Question 3:***\n","\n","What patterns do you notice in this surface plot?\n","\n","Which colors are highest in early visual versus higher visual areas? Why?\n","\n","Are there any exceptions to this pattern?"],"metadata":{"id":"S20poZ0dQ7md"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"BS7Xft-nRhqd"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"wu-c1EzSRjTU"}},{"cell_type":"markdown","source":["###Step 5: Run variance partition analysis."],"metadata":{"id":"A2OYfG6dRmyR"}},{"cell_type":"markdown","source":["When comparing the performance of different encoding models, one thing we often notice is that multiple models can have high $R^2$ for a given voxel. This can create ambiguity in interpretation, because we don't know if they are both explaining the same component of the voxel's response, or independent components.\n","\n","\n","- **Variance partitioning** provides a tool for dissecting how much of the variance explained is shared versus unique between two (or more) models.\n","\n","\n","To do this, we start by constructing larger models that consist of multiple feature spaces. In this case they will be different layers of the same DNN, but the same principle can be applied to comparing different DNNs. The features from the layers will be concatenated together. Then, we construct sub-models by \"leaving out\" one set of features at a time. The theory is that if we combine two layers, $R^2$ is generally better than either layer alone, and if we leave out one of the layers, $R^2$ will drop.\n","\n","- The drop in $R^2$ (i.e. variance explained) when leaving out a specific layer indicates the **unique variance** for that layer.\n","\n","Mathematically:\n","\n","**Given:**\n","- $R^2_A$ = $R^2$ from model with only predictor A\n","- $R^2_B$ = $R^2$ from model with only predictor B  \n","- $R^2_{AB}$ = $R^2$ from combined model with both predictors A and B\n","\n","\n","**Unique variance explained by A:**\n","\n","$$R^2_{\\text{unique A}} = R^2_{AB} - R^2_B$$\n","\n","**Unique variance explained by B:**\n","\n","$$R^2_{\\text{unique B}} = R^2_{AB} - R^2_A$$\n","\n","**Shared variance:**\n","\n","$$R^2_{\\text{shared}} = R^2_{AB} - R^2_{\\text{unique A}} - R^2_{\\text{unique B}}$$"],"metadata":{"id":"AcCQVbrVRtva"}},{"cell_type":"markdown","source":["Let's see how this works in code:"],"metadata":{"id":"CBuiN4RyUUAs"}},{"cell_type":"code","source":["# pick an ROI\n","# target_roi_name = 'EBA'\n","target_roi_name = 'all'\n","\n","voxel_inds = get_roi_vox(roi_name = target_roi_name)\n","n_voxels = np.sum(voxel_inds)\n","\n","# v is our fMRI data, these are the 3 different splits\n","vtrn = torch.Tensor(voxel_data_trn[:,voxel_inds]).to(device).to(torch.float64)\n","vout = torch.Tensor(voxel_data_holdout[:,voxel_inds]).to(device).to(torch.float64)\n","vval = torch.Tensor(voxel_data_val[:,voxel_inds]).to(device).to(torch.float64)"],"metadata":{"id":"YOPykotMTx8N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pick two DNN layers to compare:"],"metadata":{"id":"7FQtZkyYUaR-"}},{"cell_type":"code","source":["layer_name_A = 'Conv1'\n","layer_name_B = 'Conv4'\n","\n","# pull out the features from each DNN layer: layer A and layer B\n","f_A = fdata[layer_name_A]\n","f_B = fdata[layer_name_B]\n","\n","# Make a set of \"concatenated\" features, combining across both sets.\n","f_concat = np.concatenate([f_A, f_B], axis=1)\n","print(f_A.shape, f_B.shape, f_concat.shape)\n"],"metadata":{"id":"5BDF44LoUYQO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now run the encoding model with each of these feature sets"],"metadata":{"id":"VaUCkTYoUouM"}},{"cell_type":"code","source":["f = f_A\n","\n","f_trn, f_val, f_out = split_normalize_feats(f)\n","\n","# x is the DNN features, split into partitions\n","xtrn = torch.Tensor(f_trn).to(device).to(torch.float64)\n","xout = torch.Tensor(f_out).to(device).to(torch.float64)\n","xval = torch.Tensor(f_val).to(device).to(torch.float64)\n","\n","# Fit the encoding model weights\n","st = time.time()\n","weights_use = solve_ridge_fast(xtrn, vtrn, lambdas)\n","elapsed = time.time() - st\n","print('elapsed = %.5f s'%elapsed)\n","\n","# predict voxel response in held-out validation data.\n","# yhat = X @ W\n","pred = xval @ weights_use\n","actual_array = vval.cpu().numpy()\n","pred_array = pred.cpu().numpy()\n","\n","r2 = get_r2(actual_array, pred_array)\n","\n","r2_A = r2"],"metadata":{"id":"cc7iQirGUmql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f = f_B\n","\n","f_trn, f_val, f_out = split_normalize_feats(f)\n","\n","# x is the DNN features, split into partitions\n","xtrn = torch.Tensor(f_trn).to(device).to(torch.float64)\n","xout = torch.Tensor(f_out).to(device).to(torch.float64)\n","xval = torch.Tensor(f_val).to(device).to(torch.float64)\n","\n","# Fit the encoding model weights\n","st = time.time()\n","weights_use = solve_ridge_fast(xtrn, vtrn, lambdas)\n","elapsed = time.time() - st\n","print('elapsed = %.5f s'%elapsed)\n","\n","# predict voxel response in held-out validation data.\n","# yhat = X @ W\n","pred = xval @ weights_use\n","actual_array = vval.cpu().numpy()\n","pred_array = pred.cpu().numpy()\n","\n","r2 = get_r2(actual_array, pred_array)\n","\n","r2_B = r2"],"metadata":{"id":"Df8PeEjNU__A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f = f_concat\n","\n","f_trn, f_val, f_out = split_normalize_feats(f)\n","\n","# x is the DNN features, split into partitions\n","xtrn = torch.Tensor(f_trn).to(device).to(torch.float64)\n","xout = torch.Tensor(f_out).to(device).to(torch.float64)\n","xval = torch.Tensor(f_val).to(device).to(torch.float64)\n","\n","# Fit the encoding model weights\n","st = time.time()\n","weights_use = solve_ridge_fast(xtrn, vtrn, lambdas)\n","elapsed = time.time() - st\n","print('elapsed = %.5f s'%elapsed)\n","\n","# predict voxel response in held-out validation data.\n","# yhat = X @ W\n","pred = xval @ weights_use\n","actual_array = vval.cpu().numpy()\n","pred_array = pred.cpu().numpy()\n","\n","r2 = get_r2(actual_array, pred_array)\n","\n","r2_concat = r2"],"metadata":{"id":"27s5imaMVDsw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 4:***\n","\n","Based on the formulas and code above, compute the unique variance for feature set A, the unique variance for feature set B, and the shared variance between A and B.\n","\n","For each voxel, take the sum of [unique A, unique B, shared variance]. What do these values add up to?\n","\n","Print out the minimum, maximum, and median values of unique and shared variance, across voxels.\n","\n"],"metadata":{"id":"zXYVkdPqVo9q"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"M-lzo10xV8OB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"blSdAhlYV95f"}},{"cell_type":"markdown","source":["Now let's visualize the variance partitioning on a brain map.\n","\n","First plot the overall performance of the concatenated model (feature set A + feature set B).\n"],"metadata":{"id":"00imimM2Wjyx"}},{"cell_type":"code","source":["plt.rcParams.update({'font.size': 18})\n","\n","vals = r2_concat\n","\n","# need to put these values into a larger list\n","my_values = np.full(shape = voxel_inds.shape, fill_value = np.nan)\n","my_values[voxel_inds] = vals\n","\n","v = get_full_volume(my_values, voxel_mask, vol_shape)\n","\n","vol = cortex.Volume(data = v, cmap='Blues', subject=substr, \\\n","                                          vmin=0, vmax=0.4,\\\n","                                          xfmname=xfmname, mask=mask_3d)\n","\n","fig = cortex.quickflat.make_figure(vol, with_curvature=True, with_labels=True)\n","plt.title('Combined A+B model performance')"],"metadata":{"id":"8kWtIK43aH_b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the unique variance for each model:"],"metadata":{"id":"Kbhgmf_Ya3vw"}},{"cell_type":"code","source":["plt.rcParams.update({'font.size': 18})\n","\n","unique_A = r2_concat - r2_B\n","unique_B = r2_concat - r2_A\n","\n","shared = r2_concat - unique_A - unique_B\n","\n","vals = unique_A\n","\n","# need to put these values into a larger list\n","my_values = np.full(shape = voxel_inds.shape, fill_value = np.nan)\n","my_values[voxel_inds] = vals\n","\n","v = get_full_volume(my_values, voxel_mask, vol_shape)\n","\n","vol = cortex.Volume(data = v, cmap='Blues', subject=substr, \\\n","                                          vmin=0, vmax=0.4,\\\n","                                          xfmname=xfmname, mask=mask_3d)\n","\n","fig = cortex.quickflat.make_figure(vol, with_curvature=True, with_labels=True)\n","plt.title('Unique Variance: %s layer'%layer_name_A)"],"metadata":{"id":"YRNcBgCXWxm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.rcParams.update({'font.size': 18})\n","\n","unique_A = r2_concat - r2_B\n","unique_B = r2_concat - r2_A\n","\n","shared = r2_concat - unique_A - unique_B\n","\n","vals = unique_B\n","\n","# need to put these values into a larger list\n","my_values = np.full(shape = voxel_inds.shape, fill_value = np.nan)\n","my_values[voxel_inds] = vals\n","\n","v = get_full_volume(my_values, voxel_mask, vol_shape)\n","\n","vol = cortex.Volume(data = v, cmap='Blues', subject=substr, \\\n","                                          vmin=0, vmax=0.4,\\\n","                                          xfmname=xfmname, mask=mask_3d)\n","\n","fig = cortex.quickflat.make_figure(vol, with_curvature=True, with_labels=True)\n","plt.title('Unique Variance: %s layer'%layer_name_B)"],"metadata":{"id":"Zbmt98a0ZypB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.rcParams.update({'font.size': 18})\n","\n","unique_A = r2_concat - r2_B\n","unique_B = r2_concat - r2_A\n","\n","shared = r2_concat - unique_A - unique_B\n","\n","vals = shared\n","\n","# need to put these values into a larger list\n","my_values = np.full(shape = voxel_inds.shape, fill_value = np.nan)\n","my_values[voxel_inds] = vals\n","\n","v = get_full_volume(my_values, voxel_mask, vol_shape)\n","\n","vol = cortex.Volume(data = v, cmap='Blues', subject=substr, \\\n","                                          vmin=0, vmax=0.4,\\\n","                                          xfmname=xfmname, mask=mask_3d)\n","\n","fig = cortex.quickflat.make_figure(vol, with_curvature=True, with_labels=True)\n","plt.title('Shared Variance')"],"metadata":{"id":"OS2qi_5bZ0cL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 5:***\n","\n","What patterns do you notice in the unique variance and shared variance plots across the brain?\n","\n","Where is the unique variance highest for each layer? Where is the shared variance highest?\n","\n","\n"],"metadata":{"id":"JI1zlGaNbNSA"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"jJuIuUSGbbFh"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"u1XiJGR_bdDi"}},{"cell_type":"markdown","source":["***Question 6:***\n","\n","Try re-running the variance partitioning for a different pair of layers. (Change \"layer_name_A\" and \"layer_name_B\" above).\n","\n","What patterns do you notice for this new pair of layers? Are there any pairs of layers that have especially high shared variance with each other?"],"metadata":{"id":"UoMmYNjsbekr"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"QsrcYaY8b1DQ"}},{"cell_type":"markdown","source":["\n","---\n","\n"],"metadata":{"id":"GrlO3RBzb2v1"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"1f9NlmiBrFdXbDEZgVtLzqO7GDyVu8iB1","timestamp":1759854758070},{"file_id":"1DtFzI_0NLMqd0X9-7YDbUDj8KEmcr9Cg","timestamp":1759107107904},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/cad5020cab595c3bf83a518b7e4d4125/neural_style_tutorial.ipynb","timestamp":1755445164657}],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}