{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hendersonneurolab/CogAI_Fall2025/blob/master/Lab06_fMRI_Intro.ipynb)\n","\n","## Week 6: Working with fMRI data\n","\n","In this tutorial, we'll learn how to work with functional MRI data. We'll load some example data, and make some visualizations. We will also learn how to construct a general linear model (GLM) and compute beta weights for voxels.\n","\n","Learning Objectives:\n","- Gain familiarity with the format and dimensions of functional MRI data, and how to work with this data.\n","- Understand the hemodynamic response function, and how it affects the timecourse in fMRI data.\n","- Know how to use a simple t-test to find active voxels in an fMRI experiment.\n","\n","\n","\n","Credit: some of this code and example data was originally generated by [Leila Wehbe](https://www.cs.cmu.edu/~lwehbe/)."],"metadata":{"id":"nbtEAybij279"}},{"cell_type":"code","source":["import numpy as np\n","import urllib.request\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import scipy"],"metadata":{"id":"5eFcRNENAnAD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First, let's download some example data that we can work with."],"metadata":{"id":"0iQMw9EuV90p"}},{"cell_type":"code","source":["# data example\n","dbox_link1 = 'https://www.dropbox.com/scl/fi/1irp9n4zmjmuociha8dqa/example_data_01.npz?rlkey=sbupp8wdhce4a86c6we9qo1fy&st=f5f787f1&dl=1'\n","\n","# mask\n","dbox_link2 = 'https://www.dropbox.com/scl/fi/p26wbp5hmjy9d07ibximz/s01_mask.npy?rlkey=ot0mxlpda68fxv2lyvivt8fzn&st=kif9minu&dl=1'\n","\n","# experiment design file\n","dbox_link3 = 'https://www.dropbox.com/scl/fi/i8drkfb2ra8f8n7j3pw03/experiment_design.npz?rlkey=igwqu8ldtus0m7eg8xprm5u0k&st=idzruvfd&dl=1'\n","\n","# data file\n","dbox_link4 = 'https://www.dropbox.com/scl/fi/087etb9idyt9equu9x6an/sub01_categories1.npy?rlkey=qfmozcqryt2fd0yta1cq8w86m&st=04sfwc4m&dl=1'\n"],"metadata":{"id":"pji1Li1Q-Abb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = dbox_link1\n","with urllib.request.urlopen(url) as response:\n","    buffer = BytesIO(response.read())\n","    ex_data = np.load(buffer)\n","\n","url = dbox_link2\n","with urllib.request.urlopen(url) as response:\n","    buffer = BytesIO(response.read())\n","    mask = np.load(buffer)\n","\n","url = dbox_link3\n","with urllib.request.urlopen(url) as response:\n","    buffer = BytesIO(response.read())\n","    design = np.load(buffer)\n","\n","url = dbox_link4\n","with urllib.request.urlopen(url) as response:\n","    buffer = BytesIO(response.read())\n","    data = np.load(buffer)\n","    data = data.astype('float32')\n","    data = data.T"],"metadata":{"id":"8E868yQ-B26i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('mask.shape : ', mask.shape)\n","print('data.shape : ', data.shape)\n","print('design.keys() : ', design.keys())"],"metadata":{"id":"IaX4AFUTKjDh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the TR, or repetition time, a.k.a. the sampling rate\n","TR = 2.0 # one measurement every 2 seconds"],"metadata":{"id":"txPT7BwZIcQx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part 1: Visualizing the data."],"metadata":{"id":"B6K9eMVcWsT5"}},{"cell_type":"markdown","source":["Let's visualize the data in slice form.\n","For now, we will look at just the first timepoint. We can specify an x, y, z coordinate for the slices.\n","\n","- x = left/right axis (makes a sagittal slice)\n","- y = back/front axis (makes a coronal slice)\n","- z = up/down axis (makes a horizontal slice)\n","\n","Units are in \"percent signal change\" (notice how large the numbers are)."],"metadata":{"id":"CEx191CHN8ek"}},{"cell_type":"code","source":["\n","slice_xyz = [15, 50, 40]\n","\n","plt.figure(figsize=(14, 6))\n","\n","my_slice = data[0, :, :, slice_xyz[2]]\n","\n","plt.subplot(1,3,1)\n","plt.imshow(my_slice, cmap='viridis', aspect=2)\n","plt.title('Sagittal Slice (x=%d)'%slice_xyz[0])\n","plt.gca().invert_yaxis()\n","plt.colorbar()\n","\n","my_slice = data[0, :, slice_xyz[1], :]\n","\n","plt.subplot(1,3,2)\n","plt.imshow(my_slice, cmap='viridis', aspect=2)\n","plt.gca().invert_yaxis()\n","plt.title('Coronal Slice (y=%d)'%slice_xyz[1])\n","plt.colorbar()\n","\n","my_slice = data[0, slice_xyz[0], :, :]\n","\n","plt.subplot(1,3,3)\n","plt.imshow(my_slice, cmap='viridis', aspect=1)\n","plt.title('Horizontal Slice (z=%d)'%slice_xyz[2])\n","plt.colorbar()"],"metadata":{"id":"E8IS2gYzLqJj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make some plotting helper functions here."],"metadata":{"id":"k176eyZxWVXH"}},{"cell_type":"code","source":["def get_any_slice(volume, slice_number, dimension):\n","    \"\"\"Given an integer and a 3D volume, this function returns the data of\n","    that horizontal slice \"\"\"\n","    if dimension == 0:\n","        img = volume[slice_number, :, :]\n","    elif dimension == 1:\n","        img = volume[:, slice_number, :]\n","    elif dimension == 2:\n","        img = volume[:, :, slice_number]\n","    return img\n","\n","def plot_any_slice(volume, slice_number, dimension, origin = 'lower', interpolation='nearest', aspect='equal',\n","                cmap='viridis', vmin=0, vmax=2000):\n","    img = get_any_slice(volume, slice_number, dimension)\n","    if dimension>0:\n","      aspect=2\n","    _ = plt.imshow(img, origin = origin, interpolation=interpolation, aspect=aspect,\n","                cmap=cmap, vmin=vmin, vmax=vmax)\n","\n","    _ = plt.axis('off')\n","\n","\n","def plot_all_slices(volume, slice_dimension, nrows, ncols , cmap = 'viridis', vmin=0, vmax=2000,\n","                     origin = 'lower', interpolation='nearest', aspect='equal' ):\n","    fig = plt.figure(figsize = (12, 12))\n","    n_slices = volume.shape[slice_dimension]\n","    for s in range(n_slices):\n","        ax = fig.add_subplot(nrows, ncols, s+1)\n","        plot_any_slice(volume, s, slice_dimension, cmap = cmap, vmin= vmin, vmax = vmax,\n","                          origin = origin, interpolation = interpolation,aspect = aspect)\n","\n"],"metadata":{"id":"P2Z4H579PbAB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now visualize all the slices. This gives us a sense of the overall brain anatomy."],"metadata":{"id":"maHPSRvWWYyq"}},{"cell_type":"code","source":["plot_all_slices(data[0], 0, 5, 6)\n","plt.suptitle('Horizontal slices');"],"metadata":{"id":"Ql49I1P4PncX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_all_slices(data[0], 2, 10, 10)\n","plt.suptitle('Sagittal slices');"],"metadata":{"id":"YLexMSVtP6OQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_all_slices(data[0], 1, 10, 10)\n","plt.suptitle('Coronal slices');"],"metadata":{"id":"yhoKFcftQQyP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make a histogram of the values in the data. Here, we're plotting all the values from the above plots."],"metadata":{"id":"Cih918CQXQLs"}},{"cell_type":"code","source":["# Make a histogram of the data\n","print(data.shape)\n","plt.figure()\n","plt.hist(data.flatten(), bins=50)\n","plt.show()"],"metadata":{"id":"WkVxbRJnJtVv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 1:***\n","\n","What does the distribution of the data look like? Why are there so many zeros?"],"metadata":{"id":"5hz1R3xEJyHu"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"PqLtrIxnJ5xF"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"uQwoeWxPJ7p4"}},{"cell_type":"markdown","source":["So far, we've been working with all the voxels in the brain. For most analyses, we actually want to work with just the voxels in the cortex. We loaded a mask earlier that has all our cortical voxels labeled. Let's inspect it now."],"metadata":{"id":"0R56ROASXjEl"}},{"cell_type":"code","source":["# Display the same information as the brain mask above\n","cortical_voxels =mask\n","print(cortical_voxels.dtype)\n","print(cortical_voxels.shape)\n","print(cortical_voxels.sum())\n","print(cortical_voxels.mean())"],"metadata":{"id":"uvyaMJO0W0ba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are 38543 voxels in the cortical mask. Let's focus on these for our remaining analysis."],"metadata":{"id":"g1xiRFhWYJKJ"}},{"cell_type":"code","source":["print('Raw data shape: ')\n","print(data.shape)\n","print('Mask shape: ')\n","print(mask.shape)\n","data_masked = data[:,mask]\n","data_masked.shape\n","print('Masked data shape: ')\n","print(data_masked.shape)"],"metadata":{"id":"bi0wPWaEX9Cm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make a histogram of the data\n","# data_mask\n","print(data_masked.shape)\n","plt.figure()\n","plt.hist(data_masked.flatten(), bins=50)\n","plt.show()"],"metadata":{"id":"40YjA-UkX5qK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 2:***\n","\n","Now that we have applied the mask, what does the distribution of the data look like? How is this different from the raw data?"],"metadata":{"id":"H-kZPTzPYWmj"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"Yn4lAyL8YWmj"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"AcESklDaYWmj"}},{"cell_type":"markdown","source":["Now let's plot a few individual voxels. We can see that their distributions are different, some have larger signal than others."],"metadata":{"id":"BiSStzk0ZSkw"}},{"cell_type":"code","source":["vv1 = 0; vv2 = 2; vv3 = 10\n","voxel_a = data_masked[:,vv1]\n","voxel_b = data_masked[:,vv2]\n","voxel_c = data_masked[:,vv3]\n","print('Mean A: %.2f'%np.mean(voxel_a))\n","print('Mean B: %.2f'%np.mean(voxel_b))\n","print('Mean C: %.2f'%np.mean(voxel_c))\n","_ = plt.hist(voxel_a, label='Voxel A')\n","_ = plt.hist(voxel_b, label='Voxel B')\n","_ = plt.hist(voxel_c, label='Voxel C')\n","plt.legend()"],"metadata":{"id":"q4YoEo5pY_YG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In general, the mean differences between voxels are not something we care about that much, for our analyses. We care a lot about the *differences* in response for individual voxels, as in their relative responses to different experimental conditions. In order to isolate these relative differences, we typically will z-score each voxel's response before we proceed with our analyses."],"metadata":{"id":"Lrw_Ve5vYqnO"}},{"cell_type":"code","source":["data_z_masked = np.nan_to_num(scipy.stats.zscore(data, axis=0))[:,mask]"],"metadata":{"id":"TVxwugLnRp0c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's plot how the z-scoring changed things..."],"metadata":{"id":"C-1PbrSPaF3T"}},{"cell_type":"code","source":["vv1 = 0; vv2 = 2; vv3 = 10\n","voxel_a = data_z_masked[:,vv1]\n","voxel_b = data_z_masked[:,vv2]\n","voxel_c = data_z_masked[:,vv3]\n","print('Mean A: %.2f'%np.mean(voxel_a))\n","print('Mean B: %.2f'%np.mean(voxel_b))\n","print('Mean C: %.2f'%np.mean(voxel_c))\n","_ = plt.hist(voxel_a, label='Voxel A')\n","_ = plt.hist(voxel_b, label='Voxel B')\n","_ = plt.hist(voxel_c, label='Voxel C')\n","plt.legend()"],"metadata":{"id":"2LygzcFKaAn6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And here's another way to view it. In this plot, each column is a voxel, and each row is a timepoint.\n","We can see that in the raw data, most of the variance is across voxels (vertical lines). But in the z-scored data, we have removed this across-voxel variance, so we can see more of the across-time variance."],"metadata":{"id":"r2EltinhaLd-"}},{"cell_type":"code","source":["plt.figure(figsize=(12, 4))\n","plt.pcolormesh(data_masked)\n","plt.xlabel('Voxels')\n","plt.ylabel('TRs')\n","plt.colorbar()\n","plt.title('Raw data')\n","\n","plt.figure(figsize=(12, 4))\n","plt.pcolormesh(data_z_masked)\n","plt.xlabel('Voxels')\n","plt.ylabel('TRs')\n","plt.colorbar()\n","plt.title('Z-Scored')"],"metadata":{"id":"jda2Am0aRNlt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's plot a few voxel timecourses, and see how the data look."],"metadata":{"id":"njEnONkxaxWa"}},{"cell_type":"code","source":["vox2plot = [500, 1000, 1200]\n","\n","for vv in vox2plot:\n","  plt.figure(figsize=(12, 4))\n","  plt.plot(data_z_masked[:,vv])\n","\n","  plt.xlabel('TRs')\n","  plt.ylabel('fMRI activity')\n","  plt.title('Voxel number %d'%vv)"],"metadata":{"id":"7dwu8Hy3KFg5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part 2: Analyze the data with a general linear model (GLM)."],"metadata":{"id":"VJKKjnMnVeQD"}},{"cell_type":"markdown","source":["First, let's visualize the experiment design. This is an experiment that includes responses to images in 5 different categories. There are blocks of time where the participant is shown images in the same category, lasting 20 seconds each. Hence, it is a *block design.*\n","\n","Between the blocks, there is a period where no images is shown, which allows us to estimate the \"baseline\" signal.\n","\n","Typically, an experiment like this will be used to localize regions of interest (ROIs) that are selective for each category.\n","\n","There are 3 runs, and each run has 120 TRs.\n","\n","In this context a \"run\" refers to one length of time over which fMRI data is continuously collected. Usually they are a few minutes long, and there can be many runs per session."],"metadata":{"id":"gIe6qwGnDpJJ"}},{"cell_type":"code","source":["print('Experiment design variables: ', design.keys())\n","conditions = design['conditions'].tolist()\n","print('Conditions: ', conditions)"],"metadata":{"id":"oO44VQyxDmsi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize the structure on each run. Note that the first few seconds have been trimmed off, so the run starts with some condition already \"on\"."],"metadata":{"id":"at-voLV_c_Ca"}},{"cell_type":"code","source":["plt.figure(figsize=(12, 4))\n","\n","for rr, run in enumerate(['run1','run2','run3']):\n","\n","  plt.subplot(1,3,rr+1)\n","  for i in range(5):\n","    evt_on = design[run][:,i]\n","    cond = evt_on\n","    plt.plot(cond+i+0.2*i, label = conditions[i], lw=2)\n","    # plt.plot(time_axis, cond, label = conditions[i], lw=2)\n","  plt.xlabel('TRs')\n","  plt.title('Run %d'%rr)\n","  _ = plt.legend(frameon=False, bbox_to_anchor=(1.2, 1))\n","\n"],"metadata":{"id":"edW3GQmHbfqA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also concatenate the runs and view it as one big experiment:"],"metadata":{"id":"vQAGRQTleKlm"}},{"cell_type":"code","source":["design_all_runs = np.vstack([design[r] for r in ['run1','run2','run3']])\n","plt.figure(figsize=(12,4))\n","for i, (cond, label) in enumerate(zip(design_all_runs.T, conditions)):\n","    plt.plot(cond+i+0.2*i, label=label, lw=2)\n","    plt.xlabel('TR number')\n","plt.title('Condition labels')\n","_ = plt.legend(frameon=False, bbox_to_anchor=(1.2, 1))\n","\n","plt.axvline(120, color=[0.8, 0.8, 0.8], zorder=-10)\n","plt.axvline(240, color=[0.8, 0.8, 0.8], zorder=-10)"],"metadata":{"id":"8ewm5snbbdKY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 3:***\n","\n","If the TR length is 2 seconds. How many total seconds of data do we have, across these 3 runs?"],"metadata":{"id":"oexMScMie4Xq"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"MssLrqYn7RIJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"UqH2EolZe4Xq"}},{"cell_type":"markdown","source":["Using this experimental design information, we can now create our general linear model. We will be modeling each voxel's response as a sum of predictors, which correspond to the different conditions (image categories).\n","\n","For this to work well, we also need to account for the delay in each voxel's response, by modeling the **hemodynamic response function (HRF).**\n","\n","The HRF is a function that captures the timecourse of fMRI response (Blood-oxygen-level-dependent / BOLD response) following a burst of neural activity. From past research, we know the approximate shape of this function, and can model it using a gamma distribution.\n","\n","Let's first create a function that makes a canonical HRF function.\n"],"metadata":{"id":"EMGhmvStfDLd"}},{"cell_type":"code","source":["def generate_hrf(shape='twogamma', tr=1, pttp=5, nttp=15, pos_neg_ratio=6, onset=0, pdsp=1, ndsp=1, t=None):\n","    # can alter these parameters to change the shape.\n","    if t is None:\n","        t = np.arange(0, (onset + 2 * (nttp + 1)), tr) - onset\n","    else:\n","        t = np.arange(np.min(t), np.max(t), tr) - onset;\n","\n","    # Create filter\n","    h = np.zeros((len(t), ))\n","    if shape.lower()=='boynton':\n","        # boynton (single-gamma) HRF\n","        h = scipy.stats.gamma.pdf(t, pttp + 1, pdsp)\n","    elif shape.lower()=='twogamma':\n","        gpos = scipy.stats.gamma.pdf(t, pttp + 1, pdsp)\n","        gneg = scipy.stats.gamma.pdf(t, nttp + 1, ndsp) / pos_neg_ratio\n","        h =  gpos-gneg\n","    h /= np.sum(h)\n","    return t, h"],"metadata":{"id":"XTPGkxELC8jk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get a canonical HRF\n","tr = 2\n","t_hrf, hrf_1 = generate_hrf(shape='twogamma', tr=tr, \\\n","                            pttp=5, nttp=15, \\\n","                            pos_neg_ratio=6, onset=0, \\\n","                            pdsp=1, ndsp=1, t=None)\n","print('hrf_1 shape is', hrf_1.shape)\n","\n","plt.figure(figsize=(4,3))\n","__ = plt.plot(t_hrf, hrf_1, '.-')\n","plt.title(\"Canonical HRF\")\n","plt.xlabel(\"Time (s)\")\n","plt.ylabel(\"Normalized amplitude\")"],"metadata":{"id":"ivGEo9OMC6VE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 4:***\n","\n","Make a new code block, and create new versions of the HRF. Change the parameters in the generate_hrf function. Try changing: *pttp*, *pos_neg_ratio*, and the other parameters. Try setting \"shape\" to \"boynton\".\n","\n","What does each parameter do to the HRF shape?"],"metadata":{"id":"cuev0TvygqWy"}},{"cell_type":"code","source":["#[answer here]"],"metadata":{"id":"CcLC-T7Cg-Tc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"vJ9PlCwegqWz"}},{"cell_type":"markdown","source":["Now we'll use convolution to predict the response of a voxel given our proposed HRF function.\n","\n","We can start with a simulation. We will create a timeseries of simulated event onsets. For example, these could represent image onsets. Each event onset is modeled as a \"delta function\". This is a function with zeros everywhere, but 1 at the event time. It represents an infinitely short event. In theory, if the neural activity in the brain is captured by a delta function, then the fMRI response to it will look exactly like the HRF. If there are several events, we would see several HRF-like responses.\n","\n","If we **convolve** our timeseries of delta functions with the HRF, we can get a prediction of what a voxel response should look like.\n","\n","This predicted response is what we expect to get for an ideal voxel that is perfectly responsive to the simulated events, and has zero noise in its responses.\n","\n","Note that this form of convolution is not unlike what we have seen earlier in the class, with convolutional neural networks (CNNs). Recall how in a 2D CNN, we took some 2D filter, and we slid it across a 2D image, taking the dot product at each location to get our feature map responses. This is a similar idea, only now it's in 1D only. The HRF is like the filter, and we are sliding it over the event timeseries to get the predicted response."],"metadata":{"id":"1vSU-y2XhXJi"}},{"cell_type":"code","source":["# how many total timepoints to simulate?\n","n_timepts = 360\n","# this is where the events happen\n","evt_onsets = [50, 80, 200]\n","\n","time_axis = np.arange(n_timepts,)\n","\n","# now we're creating simulated event sequence\n","evt_times = np.zeros((n_timepts,))\n","for ee in evt_onsets:\n","  evt_times[ee] = 1\n","\n","# now we convolve event times with HRF (np.convolve)\n","# make sure to trim off the last timepoints, which go outside event sequence\n","convolved_resp = np.convolve(evt_times, hrf_1, mode = 'full')[0:n_timepts]\n","\n","ylims = [-0.5,1.2]\n","\n","# and plot\n","plt.figure(figsize=(8, 3))\n","plt.plot(time_axis, evt_times, '.-', color='k')\n","plt.title('Events')\n","plt.ylim(ylims)\n","plt.xlim([-5, n_timepts+5])\n","plt.vlines(evt_onsets, ymin = ylims[0], ymax=ylims[1], color=[0.8, 0.8, 0.8], zorder=-10)\n","\n","plt.figure(figsize=(8, 3))\n","plt.plot(time_axis, convolved_resp)\n","plt.title('Convolved response (predicted BOLD)')\n","plt.ylim(ylims)\n","plt.xlim([-5, n_timepts+5])\n","plt.vlines(evt_onsets, ymin = ylims[0], ymax=ylims[1], color=[0.8, 0.8, 0.8], zorder=-10)\n"],"metadata":{"id":"dSlJ4CE_Ewy1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 5:***\n","\n","Try making your own simulated series of events (start by copying some of the code above), and see how the sequence changes. What happens if you make several events very close together?\n"],"metadata":{"id":"5n2W1FDVjc9X"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"jtD7aG8Ejfuf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"FkG046kwjc9X"}},{"cell_type":"markdown","source":["Now we will apply this same idea to the actual events in our data sequence. We have a series of conditions and the timing of when they were presented. We'll convolve each condition sequence with the HRF, to generate convolved predictors."],"metadata":{"id":"SWyZF063jZmz"}},{"cell_type":"code","source":["D = design_all_runs\n","t_hrf, hrf_1 = generate_hrf(tr=2)\n","n, d = D.shape\n","\n","conv_D = np.zeros_like(D)\n","for i in range(d):\n","    conv_D[:,i] = np.convolve(D[:,i], hrf_1)[:n]\n","\n","\n","plt.figure(figsize=(12,4))\n","for i, (cond, label) in enumerate(zip(D.T, conditions)):\n","    plt.plot(cond+i+0.2*i, label=label, lw=2)\n","\n","plt.title('Original predictors')\n","_ = plt.legend(frameon=False, bbox_to_anchor=(1.4, 1))\n","plt.axvline(120, color=[0.8, 0.8, 0.8], zorder=-10)\n","plt.axvline(240, color=[0.8, 0.8, 0.8], zorder=-10)\n","\n","\n","plt.figure(figsize=(12,4))\n","for i, (cond, label) in enumerate(zip(conv_D.T, conditions)):\n","    plt.plot(cond+i+0.2*i, label=label, lw=2)\n","\n","plt.title('Convolved predictors')\n","_ = plt.legend(frameon=False, bbox_to_anchor=(1.4, 1))\n","plt.axvline(120, color=[0.8, 0.8, 0.8], zorder=-10)\n","plt.axvline(240, color=[0.8, 0.8, 0.8], zorder=-10)"],"metadata":{"id":"Mg3QUAiDncyT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For each voxel, we will model it as a weighted sum of the predictors above (i.e., the colored lines). We are assuming that the voxel is linearly related to each of the experimental conditions.\n","\n","We write the linear model equation as:\n","\n","\\begin{align}\n","Y^{v} =  {\\bf X} W^v +\\epsilon^v\n","\\end{align}\n","\n","Since this model exist for every function, we can write it as a multiple regression function:\n","\\begin{align}\n","{\\bf Y} =  {\\bf X} {\\bf W} +\\boldsymbol\\epsilon\n","\\end{align}\n","\n","In the above:\n","- ${\\bf Y}$ is [nTRs x nVoxels]\n","- ${\\bf X}$ is [nTRs x nConditions]\n","- ${\\bf W}$ is [nConditions x nVoxels]\n","- $\\epsilon$ is [nTRs x nVoxels]\n","\n","We want to solve for ${\\bf W}$, which contains our **beta weights**. In this case we have 5 conditions, so we will get 5 beta weights per voxel: one for each condition.\n","\n","To solve this, we can use ordinary least squares (OLS). The solution will be:\n","\n","\\begin{align}\n","{{\\bf W} = ({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top {\\bf Y}\\\\}\n","\\end{align}\n","\n","Notice also that each voxel's parameters are estimated independently from each other: each column of ${\\bf W}$ corresponds to the parameters of one voxel $v$, and it is obtained by multipling the matrix $({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top$ with the ${\\bf Y}$ column that corresponds to voxel $v$.\n"],"metadata":{"id":"8-vMeYfGoeoJ"}},{"cell_type":"code","source":["# Compute W (beta weights)\n","\n","X = conv_D\n","Y = data_z_masked\n","\n","XtX_inv = np.linalg.inv(np.dot(X.T, X))\n","beta = np.dot(XtX_inv, np.dot(X.T, Y))\n","\n","print('betas shape: ')\n","print(beta.shape)"],"metadata":{"id":"e43upcn6TVPm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now compute some other metrics.\n","\n","We can convert our beta weights into t-statistics, which reflect the estimate of each parameter normalized by the standard error.\n","\n","$$t = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}$$\n","\n","We can also compute $R^2$, or the proportion of variance explained. $R^2$ is a measure of overall fit quality.\n","\n","$$R^2 = \\frac{\\text{variance explained by model}}{\\text{total variance}}$$\n","\n"],"metadata":{"id":"sExtf8ZczXPf"}},{"cell_type":"code","source":["\n","# Compute residuals\n","Y_pred = np.dot(X, beta)\n","residuals = Y - Y_pred\n","\n","# Degrees of freedom\n","n = X.shape[0]  # number of observations\n","p = X.shape[1]  # number of parameters\n","df = n - p\n","\n","# Residual sum of squares\n","RSS = np.sum(residuals**2, axis=0)\n","\n","# Residual variance (MSE)\n","sigma2 = RSS / df\n","\n","# Standard errors of beta\n","# SE(beta) = sqrt(sigma^2 * diag(XtX_inv))\n","var_beta = sigma2 * np.diag(XtX_inv)[:, np.newaxis]  # Shape: (p, 1) or (p, n_voxels)\n","se_beta = np.sqrt(var_beta)\n","\n","# t-statistics\n","t_stats = beta / se_beta\n","\n","# Compute R^2 (variance explained)\n","TSS = np.sum((Y - np.mean(Y, axis=0))**2, axis=0)\n","R2 = 1 - RSS / TSS"],"metadata":{"id":"F4tO8V2Ov1nm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 6:***\n","\n","Print out the minimum, mean, and maximum values of $R^2$."],"metadata":{"id":"UqPSSzTY6eLL"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"H5fZFtK36i1I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"AuJGNptq6eLM"}},{"cell_type":"markdown","source":["Plot an example timecourse, and see how good the fit is.\n","\n","***Note:*** this fitting was all done using the same data for training and testing.\n","In practice, we'd really want to evaluate R2 on an independent test set. These values may be inflated, due to being computed on the same data."],"metadata":{"id":"REK9EH87z28s"}},{"cell_type":"code","source":["good_vox = np.flip(np.argsort(R2))[0:1]\n","# I'm choosing the best voxel for this - not all will be so good\n","\n","for vv in good_vox:\n","\n","  plt.figure()\n","  plt.plot(Y[:,vv])\n","  plt.plot(Y_pred[:,vv])\n","\n","  plt.title('voxel %d, R2 = %.2f'%(vv, R2[vv]))\n","\n","  plt.legend(['Actual', 'Predicted'], loc='upper left')"],"metadata":{"id":"z5RWBfUYULLJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's put the voxels back into volumetric space, so we can see where the best R2 is.\n","\n","These plots are showing a heatmap of R2 values."],"metadata":{"id":"PlU-iTgW1BxQ"}},{"cell_type":"code","source":["\n","mask_vals = cortical_voxels.copy().astype(np.float32)\n","mask_vals[mask_vals>0] = R2\n","mask_vals[mask_vals==0] = np.nan\n","\n","# can change these coords to get slightly different brain regions.\n","slice_xyz = [15, 50, 40]\n","\n","plt.figure(figsize=(14, 6))\n","\n","my_slice = mask_vals[:, :, slice_xyz[2]]\n","\n","plt.subplot(1,3,1)\n","plt.imshow(my_slice, cmap='viridis', aspect=2)\n","plt.title('Sagittal Slice (x=%d)'%slice_xyz[0])\n","plt.gca().invert_yaxis()\n","plt.colorbar()\n","\n","my_slice = mask_vals[:, slice_xyz[1], :]\n","\n","plt.subplot(1,3,2)\n","plt.imshow(my_slice, cmap='viridis', aspect=2)\n","plt.gca().invert_yaxis()\n","plt.title('Coronal Slice (y=%d)'%slice_xyz[1])\n","plt.colorbar()\n","\n","my_slice = mask_vals[slice_xyz[0], :, :]\n","\n","plt.subplot(1,3,3)\n","plt.imshow(my_slice, cmap='viridis', aspect=1)\n","plt.title('Horizontal Slice (z=%d)'%slice_xyz[2])\n","plt.colorbar()\n","\n","plt.suptitle('R2 values')\n"],"metadata":{"id":"Wnznx87WxOl9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 7:***\n","\n","Which parts of the brain have the best $R^2$ values? Does this make sense?\n"],"metadata":{"id":"yjMA-EVG25rB"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"2oOgJy0r3AWr"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"gg9ewX9725rC"}},{"cell_type":"markdown","source":["Finally, let's compare the activity between conditions, using a **contrast**. This is similar to a t-test.\n","\n","We will create a vector over our conditions, which specifies the comparison to perform.\n","\n","Let's start by contrasting face selectivity versus place selectivity. For this, we put a 1 for faces, a -1 for places, and a 0 for everything else. Basically, this means you are taking the beta weight for faces minus the beta weight for places.\n","\n","The variable \"conditions\" tells us what the conditions are:"],"metadata":{"id":"gxr9vV_73N_V"}},{"cell_type":"code","source":["conditions"],"metadata":{"id":"q9BvvQAX33Ly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this defines what contrast / test we want to run.\n","contrast_vector = np.array([0, 1, 0, -1, 0])"],"metadata":{"id":"Erf6Qzyz4CGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["contrast_est = np.dot(contrast_vector.T, beta)  # Shape: (1, n_voxels)\n","\n","# Compute standard error of contrast: SE(c' * beta) = sqrt(sigma^2 * c' * (X'X)^-1 * c)\n","var_contrast = sigma2 * np.dot(np.dot(contrast_vector.T, XtX_inv), contrast_vector)  # Shape: (1, n_voxels)\n","se_contrast = np.sqrt(var_contrast)\n","\n","# Compute t-statistic for contrast\n","t_stats_contrast = contrast_est / se_contrast\n","t_stats_contrast = t_stats_contrast.squeeze()  # Shape: (n_voxels,)\n","\n","print(f\"\\nContrast t-statistics shape: {t_stats_contrast.shape}\")\n","print(f\"Mean contrast t-stat: {np.mean(t_stats_contrast):.2f}\")\n","print(f\"Number of voxels with |t| > 2: {np.sum(np.abs(t_stats_contrast) > 2)}\")"],"metadata":{"id":"V-l0OSpC3EF-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now plot this in volumetric space.\n","Notice that we can see a stripe of yellow (positive t) and dark blue (negative t) along on the ventral surface, in the sagittal view. This corresponds to face-selective and scene-selective regions."],"metadata":{"id":"ddOr7AEX8eP3"}},{"cell_type":"code","source":["\n","mask_vals = cortical_voxels.copy().astype(np.float32)\n","mask_vals[mask_vals>0] = t_stats_contrast\n","mask_vals[mask_vals==0] = np.nan\n","\n","# can change these coords to get slightly different brain regions.\n","slice_xyz = [15, 50, 40]\n","\n","plt.figure(figsize=(14, 6))\n","\n","my_slice = mask_vals[:, :, slice_xyz[2]]\n","\n","plt.subplot(1,3,1)\n","plt.imshow(my_slice, cmap='viridis', aspect=2)\n","plt.title('Sagittal Slice (x=%d)'%slice_xyz[0])\n","plt.gca().invert_yaxis()\n","plt.colorbar()\n","\n","my_slice = mask_vals[:, slice_xyz[1], :]\n","\n","plt.subplot(1,3,2)\n","plt.imshow(my_slice, cmap='viridis', aspect=2)\n","plt.gca().invert_yaxis()\n","plt.title('Coronal Slice (y=%d)'%slice_xyz[1])\n","plt.colorbar()\n","\n","my_slice = mask_vals[slice_xyz[0], :, :]\n","\n","plt.subplot(1,3,3)\n","plt.imshow(my_slice, cmap='viridis', aspect=1)\n","plt.title('Horizontal Slice (z=%d)'%slice_xyz[2])\n","plt.colorbar()\n","\n","plt.suptitle('T-Statistics')\n"],"metadata":{"id":"4WdgSCfn4g_U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 8:***\n","\n","Copy the code above, and modify it to do a contrast between [object] > [scrambled]. What do the results look like? Which areas are highlighted?\n","\n","*Bonus:* do you know the names of the areas this contrast can be used to define?\n"],"metadata":{"id":"ay30NZAL5xXW"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"gah9L0pi5xXX"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"AhuoTF1n5xXX"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"1DtFzI_0NLMqd0X9-7YDbUDj8KEmcr9Cg","timestamp":1759107107904},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/cad5020cab595c3bf83a518b7e4d4125/neural_style_tutorial.ipynb","timestamp":1755445164657}],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}