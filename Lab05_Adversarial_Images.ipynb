{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hendersonneurolab/CogAI_Fall2025/blob/master/Lab05_Adversarial_Images.ipynb)\n","\n","## Week 5: Adversarial image synthesis\n","\n","In this tutorial, we'll generate adversarial examples for DNNs. These are images that look the same as a target image to a human, but are classified differently by the DNN. We will use a simple implementation of the iterative fast gradient sign method (iFGSM).\n","\n","**Learning objectives:**\n","- Understand how gradient descent in pixel space can be used to generate adversarial images.\n","- Know the difference between targeted and untargeted attacks.\n","- Understand how hyperparameters can alter the outcome of adversarial image synthesis.\n","\n"],"metadata":{"id":"nbtEAybij279"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLvyFfm_r2s0"},"outputs":[],"source":["import os\n","import requests\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Check if CUDA (GPU) is available - this will speed up training significantly\n","# If it says \"cpu\", use the menu at top right to select: \"change runtime type\"\n","# Then choose: T4 GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","if torch.cuda.is_available():\n","    print(f'GPU: {torch.cuda.get_device_name(0)}')\n"]},{"cell_type":"markdown","source":["**Step 1: Setup, loading images and models.**"],"metadata":{"id":"TjBbakWnE516"}},{"cell_type":"code","source":["# Helper function for image downloads\n","def download_image(url, filepath):\n","    try:\n","        headers = {\n","              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n","          }\n","        response = requests.get(url, headers=headers, timeout=10)\n","        # response = requests.get(url, timeout=10)\n","        response.raise_for_status()  # Raises exception for bad status codes\n","\n","        # Verify it's an image\n","        content_type = response.headers.get('content-type', '')\n","        if not content_type.startswith('image/'):\n","            print(f\"Warning: Content-Type is {content_type}, not an image\")\n","            return False\n","\n","        with open(filepath, 'wb') as f:\n","            f.write(response.content)\n","\n","        # Verify file size\n","        if os.path.getsize(filepath) < 100:  # Very small files are likely errors\n","            print(\"Warning: Downloaded file is suspiciously small\")\n","            return False\n","\n","        return True\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Request failed: {e}\")\n","        return False"],"metadata":{"id":"RAYYSOGX1FE9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First, mount your Google Drive (if not already mounted)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to the Colab Notebooks folder\n","colab_notebooks_path = '/content/drive/MyDrive/Colab Notebooks/'\n","os.chdir(colab_notebooks_path)\n","os.makedirs('CogAI', exist_ok=True)\n","os.makedirs('CogAI/images', exist_ok=True)\n","\n","images_folder = os.path.join(colab_notebooks_path, 'CogAI', 'images')\n","print(images_folder)"],"metadata":{"id":"5K8dOyLNsKQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Your image URLs\n","# These are just images from the internet - you can use your own too.\n","image_urls = [\"https://cdn.britannica.com/20/194520-050-DCAE62F1/New-World-Sylvilagus-cottontail-rabbits.jpg\"]\n","names = ['rabbit.jpg']\n","\n","for url, name in zip(image_urls, names):\n","\n","  # filename = url.split(os.sep)[-1]\n","  file_path = os.path.join(images_folder, name)\n","  print(file_path)\n","  success = download_image(url, file_path)\n","  print('Success = %s'%success)\n"],"metadata":{"id":"EcoMO1GBtVgX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creating loading and transform functions for the images."],"metadata":{"id":"xB5iZ4Ph2SZm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQcMr04cr2s1"},"outputs":[],"source":["# desired size of the output image\n","imsize = 224\n","\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","loader = transforms.Compose([\n","    transforms.Resize(imsize),  # scale imported image\n","    transforms.CenterCrop(imsize),  # crop imported image\n","    transforms.ToTensor(), # transform it into a torch tensor\n","    transforms.Normalize(mean=mean,\n","                        std=std)])\n","\n","\n","def image_loader(image_name):\n","    image = Image.open(image_name)\n","    # fake batch dimension required to fit network's input dimensions\n","    image = loader(image).unsqueeze(0)\n","    return image.to(device, torch.float)\n","\n","image_path = os.path.join(images_folder, \"rabbit.jpg\")\n","print('Orig image: %s'%image_path)\n","\n","orig_img = image_loader(image_path)\n"]},{"cell_type":"markdown","source":["Parameters for the normalization: we'll use these during synthesis to set image range boundaries."],"metadata":{"id":"z_2am9R2FiyH"}},{"cell_type":"code","source":["normalized_min = [(0 - m) / s for m, s in zip(mean, std)]  # What 0 becomes when normalized\n","normalized_max = [(1 - m) / s for m, s in zip(mean, std)]  # What 1 becomes when normalized\n","min_tensor = torch.Tensor(normalized_min).to(device).view(1, 3, 1, 1)\n","max_tensor = torch.Tensor(normalized_max).to(device).view(1, 3, 1, 1)"],"metadata":{"id":"TlUTe6Iozach"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7JHS2Sszr2s1"},"source":["Plot the image, verify it looks right."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceNHAYvur2s1"},"outputs":[],"source":["unloader = transforms.Compose([transforms.Normalize(mean=[-m/s for m, s in zip(mean, std)],\n","                                                    std=[1/s for s in std]),\n","                               transforms.ToPILImage()])\n","\n","plt.figure()\n","orig_img_pil = unloader(orig_img[0])\n","plt.imshow(orig_img_pil)\n","plt.title('My Original Image')"]},{"cell_type":"markdown","source":["Load pre-trained models."],"metadata":{"id":"0RKv1U7rGYAX"}},{"cell_type":"code","source":["# These are CNNs - you can try anything from this page: https://docs.pytorch.org/vision/stable/models\n","resnet18 = torchvision.models.resnet18(pretrained=True).eval().to(device)\n","vgg16 = torchvision.models.vgg16(pretrained=True).eval().to(device)\n","densenet = torchvision.models.densenet121(pretrained=True).eval().to(device)\n"],"metadata":{"id":"rcZMKvT2GWmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading info about the ImageNet object categories here.\n","# Because this model was trained on ImageNet, it outputs labels 1-1000, which\n","# correspond to object categories.\n","url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n","response = requests.get(url)\n","labels = response.text.strip().split('\\n')\n","labels = np.array(labels)\n","labels.shape\n"],"metadata":{"id":"N-osaxhmujix"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get predictions on the real image, with a standard network. The network should get this right!"],"metadata":{"id":"NMNYmYq2Gt9k"}},{"cell_type":"code","source":["model = resnet18\n","\n","with torch.no_grad():\n","  # get the logits (pre-softmax)\n","  preds = model(orig_img)\n","  # get the probabilities\n","  true_probs = F.softmax(preds, dim=1)\n","  true_probs = true_probs.detach().cpu().numpy()\n","  true_probs = np.squeeze(true_probs)\n","\n","# get highest prob labels\n","top_5_inds = np.flip(np.argsort(true_probs))[0:5]\n","print('Top 5 labels:')\n","print(labels[top_5_inds])\n","print('Probability:')\n","print(np.round(true_probs[top_5_inds],2)) # prob assigned to each\n","true_categ_ind = top_5_inds[0]\n","\n","print('\\nIndex of true label:')\n","print(true_categ_ind)"],"metadata":{"id":"ZYKKGPyjukiY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 2: Run an untargeted attack**\n","\n","Here, we're going to disrupt the network's predictions by reducing its probability of predicting the correct category.\n"],"metadata":{"id":"CzJERkzyC9Ui"}},{"cell_type":"code","source":["alpha = 0.20 # size of perturbation steps\n","epsilon = 1.0 # the maximum we're allowed to diverge from original pixels\n","# note the units are arbitrary here, because the image values are normalized\n","# during the perturbation procedure.\n","\n","n_iters = 10\n","\n","adv_image = orig_img.clone()\n","model = resnet18\n","\n","true_label = torch.tensor([true_categ_ind]).to(device)\n","\n","# Enable gradients for input\n","adv_image.requires_grad_()\n","\n","with torch.no_grad():\n","    adv_output = model(adv_image)\n","    probs = F.softmax(adv_output, dim=1)\n","    adv_ind = torch.argmax(probs, dim=1)[0] # which index is max pred\n","\n","print('Before perturbation: %s, %.2f prob'%(labels[adv_ind], probs[0,adv_ind]) )\n","\n","ims = [adv_image.clone()]\n","perts = [torch.zeros_like(adv_image)]\n","\n","for ii in range(n_iters):\n","\n","  # enable gradients again here\n","  adv_image.requires_grad_()\n","\n","  # Forward pass (getting logits)\n","  output = model(adv_image)\n","\n","  # convert logits to probs\n","  probs = F.softmax(output, dim=1)\n","  # Loss: this is how much weight the network assigns to the \"true\" category here\n","  loss = - torch.log(probs[0, true_label])\n","  # This is negative bc we're minimizing it - so we're minimizing P(correct)\n","  # loss = F.cross_entropy(output, true_label) # also equivalent\n","\n","  # Backward pass\n","  model.zero_grad()\n","  loss.backward()\n","\n","  # Update the image in \"adversarial\" direction\n","  sign_data_grad = adv_image.grad.sign() # sign: which direction to perturb\n","  adv_image = adv_image.detach() + alpha * sign_data_grad\n","\n","  # we clamp the pixels here: keeping perturbations in a small range.\n","  adv_image = torch.clamp(adv_image, orig_img - epsilon, orig_img + epsilon)\n","\n","  # clamp to normal image range\n","  adv_image = torch.clamp(adv_image, min_tensor, max_tensor)\n","\n","  with torch.no_grad():\n","    adv_output = model(adv_image)\n","    probs = F.softmax(adv_output, dim=1)\n","    adv_ind = torch.argmax(probs, dim=1)[0] # which index is max pred\n","\n","  print('Iteration %d: %s, %.2f prob'%(ii, labels[adv_ind], probs[0,adv_ind]) )\n","\n","  ims += [adv_image.clone()]\n","  perts += [sign_data_grad.clone()]\n","  print(torch.max(sign_data_grad))\n","\n","\n","with torch.no_grad():\n","    adv_output = model(adv_image)\n","    probs = F.softmax(adv_output, dim=1)\n","    adv_ind = torch.argmax(probs, dim=1)[0] # which index is max pred\n","\n","print('Final result: %s, %.2f prob'%(labels[adv_ind], probs[0,adv_ind]) )\n","\n"],"metadata":{"id":"yPBI6Ou5C8Km"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["View the transformations over time:"],"metadata":{"id":"f-6HNPfyN3Dc"}},{"cell_type":"code","source":["plt.figure(figsize=(12, 12))\n","n_plots = int(np.ceil(np.sqrt(n_iters+1)))\n","\n","for ii in range(n_iters+1):\n","\n","  plt.subplot(n_plots, n_plots, ii+1)\n","\n","  adv_img_pil = unloader(ims[ii][0])\n","  plt.imshow(adv_img_pil)\n","  plt.title('Step %d'%ii)\n","  plt.axis('off')"],"metadata":{"id":"fSWvKlASM29c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["View the gradient sign over time (this is the perturbation pattern applied at each step)."],"metadata":{"id":"to6PTRW7ObUF"}},{"cell_type":"code","source":["plt.figure(figsize=(12, 12))\n","n_plots = int(np.ceil(np.sqrt(n_iters+1)))\n","\n","for ii in range(n_iters+1):\n","\n","  plt.subplot(n_plots, n_plots, ii+1)\n","\n","  p = perts[ii][0]\n","  p_pil = unloader(p)\n","  plt.imshow(p_pil)\n","  plt.title('Step %d: gradient sign'%(ii,))\n","  plt.axis('off')"],"metadata":{"id":"dZpqvjh6N6jq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see what the final image looks like."],"metadata":{"id":"18vpJ16yMUph"}},{"cell_type":"code","source":["\n","plt.figure(figsize=(8,4))\n","\n","plt.subplot(1,2,1)\n","orig_img_pil = unloader(orig_img[0])\n","plt.imshow(orig_img_pil)\n","plt.title(labels[true_categ_ind])\n","plt.title('%s, %.2f prob'%(labels[true_categ_ind], true_probs[true_categ_ind]))\n","\n","plt.subplot(1,2,2)\n","adv_img_pil = unloader(adv_image[0])\n","plt.imshow(adv_img_pil)\n","plt.title('%s, %.2f prob'%(labels[adv_ind], probs[0,adv_ind]))"],"metadata":{"id":"q-6AaIPSMT2C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 1:***\n","\n","Now try re-running the above procedure, but modifying each of the following:\n","- alpha\n","- epsilon\n","- n_iters\n","\n","What effects does each of these parameters have on the final result, and on the results over the course of optimization?\n"],"metadata":{"id":"BH-IGP91PahT"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"6FV8_OLCPahU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"rUi0X1yBPahU"}},{"cell_type":"markdown","source":["**Step 3: Targeted attack.**\n","\n","Next, let's try a targeted adversarial attack. In a targeted attack, we're pushing the image to look like a specified target category.\n","\n","You can specify any category in ImageNet here. Let's try \"refrigerator\" to start with...\n"],"metadata":{"id":"1Ynjwg20D_ac"}},{"cell_type":"code","source":["target_categ_name = 'refrigerator'\n","# target_categ_name = 'speedboat'\n","# variable \"labels\" has all the possible target categories we can perturb toward\n","target_categ_ind = np.where(target_categ_name==labels)[0][0]\n","target_categ_ind"],"metadata":{"id":"wi-CpNJYvNIr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["alpha = 0.20 # size of perturbation steps\n","epsilon = 1.0 # the maximum we're allowed to diverge from original pixels\n","# note the units are arbitrary here, because the image values are normalized\n","# during the perturbation procedure.\n","\n","n_iters = 10\n","# n_iters = 2\n","\n","adv_image = orig_img.clone()\n","model = resnet18\n","\n","target_label = torch.tensor([target_categ_ind]).to(device)\n","\n","# Enable gradients for input\n","adv_image.requires_grad_()\n","\n","with torch.no_grad():\n","    adv_output = model(adv_image)\n","    probs = F.softmax(adv_output, dim=1)\n","    adv_ind = torch.argmax(probs, dim=1)[0] # which index is max pred\n","\n","print('Before perturbation: %s, %.2f prob'%(labels[adv_ind], probs[0,adv_ind]) )\n","\n","ims = [adv_image.clone()]\n","perts = [torch.zeros_like(adv_image)]\n","\n","for ii in range(n_iters):\n","\n","  # enable gradients again here\n","  adv_image.requires_grad_()\n","\n","  # Forward pass (getting logits)\n","  output = model(adv_image)\n","\n","  # convert logits to probs\n","  probs = F.softmax(output, dim=1)\n","  # Loss: this is how much weight the network assigns to the \"false\" category here\n","  loss = torch.log(probs[0, target_label])\n","  # This is positive bc we're maximizing it - so we're maximizing P(incorrect)\n","  # loss = -F.cross_entropy(output, target_label) # also equivalent\n","\n","  # Backward pass\n","  model.zero_grad()\n","  loss.backward()\n","\n","  # Update the image in \"adversarial\" direction\n","  sign_data_grad = adv_image.grad.sign() # sign: which direction to perturb\n","  adv_image = adv_image.detach() + alpha * sign_data_grad\n","\n","  # we clamp the pixels here: keeping perturbations in a small range.\n","  adv_image = torch.clamp(adv_image, orig_img - epsilon, orig_img + epsilon)\n","\n","  # clamp to normal image range\n","  adv_image = torch.clamp(adv_image, min_tensor, max_tensor)\n","\n","  with torch.no_grad():\n","    adv_output = model(adv_image)\n","    probs = F.softmax(adv_output, dim=1)\n","    adv_ind = torch.argmax(probs, dim=1)[0] # which index is max pred\n","\n","  print('Iteration %d: %s, %.2f prob'%(ii, labels[adv_ind], probs[0,adv_ind]) )\n","\n","  ims += [adv_image.clone()]\n","  perts += [sign_data_grad.clone()]\n","\n","\n","with torch.no_grad():\n","    adv_output = model(adv_image)\n","    probs = F.softmax(adv_output, dim=1)\n","    adv_ind = torch.argmax(probs, dim=1)[0] # which index is max pred\n","\n","print('Final result: %s, %.2f prob'%(labels[adv_ind], probs[0,adv_ind]) )"],"metadata":{"id":"r4fGLg8UQIj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12, 12))\n","n_plots = int(np.ceil(np.sqrt(n_iters+1)))\n","\n","for ii in range(n_iters+1):\n","\n","  plt.subplot(n_plots, n_plots, ii+1)\n","\n","  adv_img_pil = unloader(ims[ii][0])\n","  plt.imshow(adv_img_pil)\n","  plt.title('Step %d'%ii)\n","  plt.axis('off')"],"metadata":{"id":"46linUu5QSlf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["View the gradient sign over time (this is the perturbation pattern applied at each step)."],"metadata":{"id":"WLxdvBh-QSlf"}},{"cell_type":"code","source":["plt.figure(figsize=(12, 12))\n","n_plots = int(np.ceil(np.sqrt(n_iters+1)))\n","\n","for ii in range(n_iters+1):\n","\n","  plt.subplot(n_plots, n_plots, ii+1)\n","\n","  p = perts[ii][0]\n","  p_pil = unloader(p)\n","  plt.imshow(p_pil)\n","  plt.title('Step %d: gradient sign'%(ii,))\n","  plt.axis('off')"],"metadata":{"id":"7IfC6tJ1QSlf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see what the final image looks like:"],"metadata":{"id":"yLPPHwnfLeP2"}},{"cell_type":"code","source":["\n","plt.figure(figsize=(8,4))\n","\n","plt.subplot(1,2,1)\n","orig_img_pil = unloader(orig_img[0])\n","plt.imshow(orig_img_pil)\n","plt.title(labels[true_categ_ind])\n","plt.title('%s, %.2f prob'%(labels[true_categ_ind], true_probs[true_categ_ind]))\n","\n","plt.subplot(1,2,2)\n","adv_img_pil = unloader(adv_image[0])\n","plt.imshow(adv_img_pil)\n","plt.title('%s, %.2f prob'%(labels[adv_ind], probs[0,adv_ind]))"],"metadata":{"id":"AjcWLobtCWzK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 2:***\n","\n","Run a new version of the targeted attack, but targeting toward a different category. Do the results look different, either in the image itself or the perturbations (gradient sign plots)?\n"],"metadata":{"id":"uxeTHeDpRich"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"c91PgikPRici"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 3:***\n","\n","Now that you've generated adversarial images using ResNet-18, try testing your image using the other networks we loaded earlier (VGG-16, and DenseNet). What predictions do those networks give? Why do you think this happens?\n"],"metadata":{"id":"nqUHoL7FCckz"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"AKC1vdHcCber"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 4:***\n","\n","Now modify the code to generate images that are adversarial for VGG-16 and DenseNet instead of ResNet-18.\n","\n","Does the outcome of the adversarial synthesis change at all?\n","\n","What happens when you ask ResNet-18 to classify the images from those other networks?\n","\n"],"metadata":{"id":"037w2VJSREYg"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"_4ZkRay8REYh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"SXdR0pKOREYh"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/cad5020cab595c3bf83a518b7e4d4125/neural_style_tutorial.ipynb","timestamp":1755445164657}],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}