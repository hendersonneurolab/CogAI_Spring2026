{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN5Jy5Pc3RJ3moLcN4nwbU0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hendersonneurolab/CogAI_Fall2025/blob/master/Lab04_Multimodal_CLIP.ipynb)\n","\n","## Lab Exercise 4: Language and multimodal models\n","\n","In this tutorial, we'll work with a more advanced multimodal DNN, the CLIP model. This is a powerful model that takes in both language and image inputs, and maps them into a shared embedding space. We will:\n","\n","1. Load the text encoder of a pre-trained CLIP model, and visualize how it processes a sentence, including tokenization and self-attention.\n","2. Use the vision encoder and text encoder together to explore zero-shot image classification.\n","\n","**Learning objectives:**\n","- Understand the input and outputs of a language model.\n","- Develop intuition for self-attention in a language model.\n","- Understand how models integrate vision and language inputs, and how this can be used to build a decoder.\n","\n","\n","More info about CLIP implementation can be found here: https://huggingface.co/docs/transformers/en/model_doc/clip.\n","We're using an implementation of the model from the HuggingFace model repository.\n","\n","More info about CLIP (Contrastive Language-Image Pretraining) here:\n","https://arxiv.org/abs/2103.00020\n","\n"],"metadata":{"id":"_Je8gf6lkEP_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mGCwUVsfDC0"},"outputs":[],"source":["# Install required packages\n","# (note that we need the ! before this line, this tells colab to run this as a shell command)\n","# !pip install torch torchvision transformers tokenizers matplotlib seaborn\n","!pip install transformers tokenizers\n","# we're installing transformers and tokenizers from the HuggingFace library here\n","# https://huggingface.co/docs/transformers/en/index\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from transformers import CLIPTokenizer, CLIPTextModel, CLIPProcessor, CLIPModel, CLIPImageProcessor\n","from collections import Counter\n","import pandas as pd\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","import os\n","\n","# Set up plotting style\n","plt.style.use('default')\n","sns.set_palette(\"husl\")\n","\n","# Check if GPU is available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","source":["**Step 1: Prepare a sentence for the language encoder.**\n","\n","In order for the DNN to process a sentence input, we first need to convert it into a series of numbers. To do this, we'll load a function called a \"tokenizer\". This is a function that was learned during construction of the CLIP model, and is specific to the dataset it was trained on. It maps from each word (or sometimes, just a part of longer words), to a number in a fixed set of numbers. The set of possible numbers is the \"vocabulary size\" of the tokenizer.\n","\n","Note: we might get a warning in this cell, like \"The secret `HF_TOKEN` does not exist\". You can safely ignore this!"],"metadata":{"id":"AFWEtj0Ms-GC"}},{"cell_type":"code","source":["# Load CLIP's tokenizer: this is the part that converts a sentence into tokens\n","# Like \"pre-processing\" the sentence for our language model.\n","\n","# When we specify \"clip-vit-base-patch32\", this is the name of a pre-trained model.\n","# We'll use the same tokenizer that was used during training of this model.\n","# In this case it's learned using Byte Pair Encoding.\n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","# how many possible words/word parts can we represent?\n","print('Vocabulary size: %d'%tokenizer.vocab_size)\n","\n","# what is the max length of an input to this model?\n","print('Model max length: %d'%tokenizer.model_max_length)"],"metadata":{"id":"ejyEBpmNgLDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's enter an example sentence - you can try your own too.\n","sentence = \"The brain is superlatively cool.\"\n","#  and we love to study it.\"\n","# sentence = 'Neural networks are trained on lots of'\n","print(f\"Original sentence: '{sentence}'\")\n","\n","# Tokenize the sentence\n","# The </w> signals the end of a word.\n","# Notice that long words like \"superlatively\" get broken into a few parts.\n","# The </w> only comes at the end of a whole word.\n","tokens = tokenizer.tokenize(sentence)\n","print('Number of tokens in my sentence: %d'%len(tokens))\n","# print(f\"Tokens: {tokens}\")\n","\n","# Convert tokens to IDs\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","# print(f\"Token IDs: {token_ids}\")\n","\n","pd.DataFrame(np.array([tokens, token_ids]).T, index=np.arange(len(tokens)), columns = ['Token','Token ID'])"],"metadata":{"id":"p3RYMUC635G9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The tokenizer also has elements for special sentence parts, like start and end of the sequence. Let's explore these now:"],"metadata":{"id":"LoiG6djzGQky"}},{"cell_type":"code","source":["\n","print(\"Special tokens in CLIP:\")\n","print(f\"Start of sequence: '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})\")\n","print(f\"End of sequence: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n","print(f\"Padding token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n","\n","# See how they're added to our sentence\n","full_encoding = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n","print(f\"\\nFull token sequence: {full_encoding['input_ids'][0].tolist()}\")\n","\n","# Decode to see the full sequence\n","decoded_tokens = tokenizer.convert_ids_to_tokens(full_encoding['input_ids'][0])\n","print(f\"Decoded tokens: {decoded_tokens}\")\n","\n","pd.DataFrame(np.array([full_encoding['input_ids'][0].tolist(), decoded_tokens]).T, index=np.arange(len(decoded_tokens)), columns = ['Token','Token ID'])"],"metadata":{"id":"A60ldT2nlODA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","***Question 1:***\n","\n","\n","Try entering your own sentence above (\"sentence = \"). Print out the following: the full list of tokens in your sentence, the full list of token IDs in your sentence, and the total number of tokens in your sentence.\n"],"metadata":{"id":"KzfTz9LfiYvF"}},{"cell_type":"code","source":["# answer here"],"metadata":{"id":"8GfGF9N5IOTF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"YuZRTk6jIRrU"}},{"cell_type":"code","source":["# back to original sentence\n","sentence = \"The brain is superlatively cool.\""],"metadata":{"id":"021ep-hjIbCs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 2: Load the CLIP text encoder.**\n","\n","Recall that CLIP has two branches: a vision encoder and text encoder, which converge on a shared latent space. For now, let's load just the text encoder portion. It has a transformer architecture. We'll load a set of weights from a pre-trained model."],"metadata":{"id":"knvShOML6lfw"}},{"cell_type":"code","source":["# Load CLIP model\n","model_name = \"openai/clip-vit-base-patch32\"\n","text_model = CLIPTextModel.from_pretrained(model_name, attn_implementation=\"eager\").to(device)\n","\n","# Let's examine the model architecture\n","print(\"CLIP Text Encoder Architecture:\")\n","print(text_model)\n","\n","# Key parameters\n","print(f\"\\nKey parameters:\")\n","print(f\"Vocabulary size: {text_model.config.vocab_size}\")\n","print(f\"Hidden size (embedding dimension): {text_model.config.hidden_size}\")\n","print(f\"Number of attention heads: {text_model.config.num_attention_heads}\")\n","print(f\"Number of transformer layers: {text_model.config.num_hidden_layers}\")\n","print(f\"Maximum sequence length: {text_model.config.max_position_embeddings}\")"],"metadata":{"id":"OXFY80vilT0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute the first layer embeddings: here we're taking the token IDs, and linearly mapping these into a lower dimensional vector embedding. In this case it's 512-dimensional.\n","\n","The other element that is needed here is a position embedding. This is an additional vector that specifies the position of each word in the sentence. Without this, the model doesn't have information about relative positions of words. The position embedding is learned during training, and it gets added onto the embedding of each token."],"metadata":{"id":"3CZkra1-S_RU"}},{"cell_type":"code","source":["model = text_model\n","\n","# Tokenize\n","inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","\n","# Get the embedding layer (first layer of the model)\n","with torch.no_grad(): # no_grad: disables computation of model gradients. Because we're not training, just evaluating.\n","    # Access the embeddings layer\n","    token_embeddings = model.text_model.embeddings.token_embedding(inputs['input_ids'])\n","    position_embeddings = model.text_model.embeddings.position_embedding(\n","        torch.arange(inputs['input_ids'].shape[1], device=device).unsqueeze(0)\n","    )\n","    # Position embeddings: store information about token sequence.\n","    # A different vector is added onto each token, depending on its position.\n","\n","    # Final embeddings = token embeddings + position embeddings\n","    final_embeddings = token_embeddings + position_embeddings\n","\n","print(f\"Input shape: {inputs['input_ids'].shape}\")\n","print(f\"Token embeddings shape: {token_embeddings.shape}\")\n","print(f\"Position embeddings shape: {position_embeddings.shape}\")\n","print(f\"Final embeddings shape: {final_embeddings.shape}\")\n"],"metadata":{"id":"yBMCmhuT-9xz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's visualize these embeddings. Each row is an embedding vector for one token. The color indicates the value.\n","\n","We can also plot the similarity between position embeddings for each pair of tokens (bottom plot)."],"metadata":{"id":"fT9XzeRYUTiQ"}},{"cell_type":"code","source":["e = position_embeddings[0].numpy()\n","c = np.corrcoef(e)\n","\n","plt.figure(figsize=(12,4))\n","plt.pcolormesh(e)\n","plt.colorbar()\n","plt.title('Position embedding vectors')\n","plt.xlabel('Embedding dimension')\n","plt.ylabel('Input token position')\n","\n","plt.figure()\n","plt.pcolormesh(c)\n","plt.colorbar()\n","plt.title('Similarity between position embeddings for each token position')\n","plt.xlabel('Input token position')\n","plt.ylabel('Input token position')\n"],"metadata":{"id":"YP4wqnh4Qlnx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","***Question 2:***\n","\n","What do you notice about the similarity matrix? Which pairs of tokens are most similar? Why?"],"metadata":{"id":"n2iEHzlJJ-Zf"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"9l4-qUkHKINM"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"sax8FXTuKawj"}},{"cell_type":"markdown","source":["**Step 3: Implement the self-attention layers**\n","\n","The transformer is composed of many self-attention layers, which each implement an \"attention\" operation multiple times.\n","\n","As an analogy for this, think of searching for a book at the library:\n","\n","- QUERY (Q): What you're looking for ('information about cats')\n","- KEY (K): The index/catalog of each book ('this book contains: animals, pets, cats...')\n","- VALUE (V): The actual content of each book\n","\n","Attention process:\n","1. Compare your QUERY with each book's KEY (what topics it covers)\n","2. Books with relevant KEYs get higher attention scores\n","3. Take a weighted combination of the VALUES based on attention scores\n","\n","In transformers:\n","- Each word has a Query: 'What should I pay attention to?'\n","- Each word has a Key: 'What information do I contain?'\n","- Each word has a Value: 'What information should I contribute?'"],"metadata":{"id":"Xol_-Zn5SnxH"}},{"cell_type":"markdown","source":["In this plot, we're going to visualize the weights of one attention head in one layer. These are determined based on similarity between the queries and keys for each token. How much do we \"weight\" each word earlier in the sentence, when we're processing the current word?"],"metadata":{"id":"fjhPf5caaNKb"}},{"cell_type":"code","source":["layer_idx = 5  # Which layer to visualize\n","\n","head_idx = 0   # Which head within the layer to visualize\n","\n","# We need to modify the model to output attention weights\n","with torch.no_grad():\n","    outputs = model(inputs['input_ids'], output_attentions=True)\n","\n","# Get attention weights from specified layer\n","attention_weights = outputs.attentions[layer_idx][0]  # Remove batch dimension\n","\n","tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","\n","# Get specific attention head\n","head_attention = attention_weights[head_idx].cpu().numpy()\n","\n","# Create heatmap\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(head_attention,\n","            xticklabels=tokens,\n","            yticklabels=tokens,\n","            cmap='Blues',\n","            annot=True,\n","            fmt='.2f',\n","            cbar_kws={'label': 'Attention Weight'})\n","\n","plt.title(f'Self-Attention Weights\\nLayer {layer_idx}, Head {head_idx}')\n","plt.xlabel('Keys (attending to)')\n","plt.ylabel('Queries (attending from)')\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0)\n","plt.tight_layout()\n","plt.show()\n","\n","# Notice there's a lot of weight assigned to the very first token <startoftext>\n","# This is sometimes called \"attention sink\"\n","# Start token is like a default target for attention, when no other strong relationships exist."],"metadata":{"id":"gkJ_eMf6l_HS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 3:***\n","\n","Which values are 0 in the matrix above? Why are these zeros?\n","\n"],"metadata":{"id":"er5LEuPELqbz"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"K-6UgjpxLxu4"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"JtSjg4SeLzLY"}},{"cell_type":"markdown","source":["That's just one attention head - there are 8 different ones per layer. Each head can extract different types of information from the sentence. For example, one head could be involved in processing syntax, and another the conceptual meaning of words.\n","\n","Now let's plot several heads:"],"metadata":{"id":"RpwZUQTgak9V"}},{"cell_type":"code","source":["layer_idx = 11  # Which layer to visualize\n","# layer_idx = 0;\n","\n","for head_idx in range(4):\n","\n","  # Get attention weights from specified layer\n","  attention_weights = outputs.attentions[layer_idx][0]  # Remove batch dimension\n","\n","  # Get specific attention head\n","  head_attention = attention_weights[head_idx].cpu().numpy()\n","\n","  # Create heatmap\n","  plt.figure(figsize=(8, 6))\n","  sns.heatmap(head_attention,\n","              xticklabels=tokens,\n","              yticklabels=tokens,\n","              cmap='Blues',\n","              annot=True,\n","              fmt='.2f',\n","              cbar_kws={'label': 'Attention Weight'})\n","\n","  plt.title(f'Self-Attention Weights\\nLayer {layer_idx}, Head {head_idx}')\n","  plt.xlabel('Keys (attending to)')\n","  plt.ylabel('Queries (attending from)')\n","  plt.xticks(rotation=45, ha='right')\n","  plt.yticks(rotation=0)\n","  plt.tight_layout()\n","  plt.show()\n"],"metadata":{"id":"0StBPR4ra63u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---"],"metadata":{"id":"FDAwvIkruFl0"}},{"cell_type":"markdown","source":["***Question 4:***\n","\n","What do you think the different attention heads might correspond to in this example? What kinds of information could they each be integrating or processing?\n"],"metadata":{"id":"7wFMbbMZuMTI"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"iinQffOLb3Br"}},{"cell_type":"markdown","source":["\n","---"],"metadata":{"id":"upZwq1LvuLIZ"}},{"cell_type":"markdown","source":["***Question 5:***\n","Try changing the value of layer_idx in the code above. How do the self-attention weight matrices change? Are there any differences you can notice between earlier and later model layers?"],"metadata":{"id":"09f_wPBCcACm"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"4ejn5TXEcACm"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"wjgE8EFGcACm"}},{"cell_type":"markdown","source":["***Question 6:***\n","Go back to the top of the notebook, and change the input sentence (\"sentence = \") to something else. Try both long and short sentences. Do you notice any additional patterns in the attention heads?"],"metadata":{"id":"YdY3z6BdipKT"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"kwyt_bLHipKT"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"81whD53oipKT"}},{"cell_type":"markdown","source":["**Step 4: Extracting the final text embedding.**\n","\n","After passing the input through each attention layer, we have an embedding corresponding to the whole sentence."],"metadata":{"id":"EatmXDlDf-tE"}},{"cell_type":"code","source":["sentence = 'The brain is superlatively cool.'\n","inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","\n","outputs = text_model(inputs['input_ids'])\n","\n","e = outputs.last_hidden_state[0].detach().numpy()\n","# Get out the last hidden state from the model.\n","# this provides a single 512-D vector for each sentence token.\n","# To capture the entire sentence, we'll just take the last row of this (last token)\n","# This corresponds to the end-of-text token.\n","# It includes information about the entire input.\n","eot_token_pos = inputs['attention_mask'][0].sum() - 1\n","sentence_embedding = e[eot_token_pos]\n","\n","plt.figure(figsize=(12,4))\n","plt.plot(sentence_embedding)\n","# plt.plot(sentence_embedding2)\n","plt.title('Final sentence embedding')\n","plt.xlabel('Embedding dimension')\n","plt.ylabel('Value')\n","plt.show()"],"metadata":{"id":"bzVPQk6TzVe2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 5: Loading the complete model: vision + text**\n","\n","Now that we've explored the language encoder, let's look at how it works along with the vision encoder.\n","The vision encoder in this version uses a Vision Transformer architecture (ViT); this is different than the CNNs we've looked at previously. It is similar to the text encoder architecture, in that it uses the same self-attention operation. But it works on visual inputs."],"metadata":{"id":"0KIYepajGX-g"}},{"cell_type":"code","source":["# Load the complete CLIP model (both encoders)\n","model_name = \"openai/clip-vit-base-patch32\"\n","\n","print(f\"Loading CLIP model: {model_name}\")\n","clip_model = CLIPModel.from_pretrained(model_name).to(device)\n","# clip_processor = CLIPProcessor.from_pretrained(model_name)\n","image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"],"metadata":{"id":"Jwwhn6TNFZTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's see what's inside the model.\n","# it includes both a text model and a vision model component.\n","clip_model.text_model, clip_model.vision_model\n"],"metadata":{"id":"_f1YOuBTHHPA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To handle images, mounting your google drive folder here\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to the Colab Notebooks folder\n","colab_notebooks_path = '/content/drive/MyDrive/Colab Notebooks/'\n","os.chdir(colab_notebooks_path)\n","os.makedirs('CogAI', exist_ok=True)\n","os.makedirs('CogAI/images', exist_ok=True)\n","\n","images_folder = os.path.join(colab_notebooks_path, 'CogAI', 'images')\n","print(images_folder)"],"metadata":{"id":"cseFXletRE6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper function for image downloads\n","def download_image(url, filepath):\n","    try:\n","        headers = {\n","              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n","          }\n","        response = requests.get(url, headers=headers, timeout=10)\n","        # response = requests.get(url, timeout=10)\n","        response.raise_for_status()  # Raises exception for bad status codes\n","\n","        # Verify it's an image\n","        content_type = response.headers.get('content-type', '')\n","        if not content_type.startswith('image/'):\n","            print(f\"Warning: Content-Type is {content_type}, not an image\")\n","            return False\n","\n","        with open(filepath, 'wb') as f:\n","            f.write(response.content)\n","\n","        # Verify file size\n","        if os.path.getsize(filepath) < 100:  # Very small files are likely errors\n","            print(\"Warning: Downloaded file is suspiciously small\")\n","            return False\n","\n","        return True\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Request failed: {e}\")\n","        return False"],"metadata":{"id":"nn_CSBA2RLPQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Your image URLs\n","# These are just images from the internet - you can use your own too.\n","image_urls = {\n","    \"cat\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Kittyply_edit1.jpg/256px-Kittyply_edit1.jpg\",\n","    # \"dog\": \"https://upload.wikimedia.org/wikipedia/commons/9/90/Labrador_Retriever_portrait.jpg\",\n","    # \"car\": \"https://upload.wikimedia.org/wikipedia/commons/c/c3/2019_Toyota_Corolla_XSE_%28MZEA12L%29_in_Blue_Flame%2C_front_left.jpg\",\n","    # \"airplane\": \"https://upload.wikimedia.org/wikipedia/commons/3/36/United_Airlines_Boeing_777-200_Meulemans.jpg\",\n","}\n","\n","for name, url in image_urls.items():\n","  file_path = os.path.join(images_folder, '%s.jpg'%name)\n","  print(file_path)\n","  success = download_image(url, file_path)\n","  print('Success = %s'%success)"],"metadata":{"id":"VBYiLuTWRaE-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PIL\n","name = 'cat'\n","file_path = os.path.join(images_folder, '%s.jpg'%name)\n","image = PIL.Image.open(file_path).convert('RGB')\n","image"],"metadata":{"id":"TyhiMlnHSwUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now extract the CLIP embedding for this image.\n","\n","image_input = image_processor(image, return_tensors=\"pt\")\n","image_input = image_input.pixel_values\n","image_input = image_input.to(device)\n","\n","image_embedding = clip_model.get_image_features(pixel_values = image_input)[0]\n","# this is [512] dimensional vector"],"metadata":{"id":"pBtqBypHLfFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's plot this embedding\n","\n","plt.figure(figsize=(12,4))\n","plt.plot(image_embedding.detach().numpy())\n","# plt.plot(image_embedding2)\n","plt.title('Final image embedding')\n","plt.xlabel('Embedding dimension')\n","\n","plt.ylabel('Value')\n","plt.show()"],"metadata":{"id":"qRctmrIQQ0cR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's create a caption for our example image..."],"metadata":{"id":"xNTfz5QgRRs2"}},{"cell_type":"code","source":["my_caption = 'A photo of a brown cat with long whiskers'"],"metadata":{"id":"xiYeYLZLPVtY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now run this caption through the text encoder to get a caption embedding."],"metadata":{"id":"2TbZQZN1RcUA"}},{"cell_type":"code","source":["# Tokenize\n","inputs = tokenizer(my_caption, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","caption_embedding= clip_model.get_text_features(inputs['input_ids'])[0]"],"metadata":{"id":"rhRB-xlIRmNC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's plot these embeddings on top of each other. How similar are they?\n","Remember, the whole point of this model is to create embeddings for an image and its caption that are very similar."],"metadata":{"id":"Q1PPNd7HUDib"}},{"cell_type":"code","source":["\n","plt.figure(figsize=(12,4))\n","\n","plt.plot(image_embedding.detach().numpy())\n","plt.plot(caption_embedding.detach().numpy())\n","plt.title('Image and caption embeddings, correlation=%.3f'%np.corrcoef(image_embedding.detach().numpy(), caption_embedding.detach().numpy())[0,1])\n","plt.xlabel('Embedding dimension')\n","plt.ylabel('Value')\n","# plt.show()\n","\n","plt.legend(['Image','Caption'])"],"metadata":{"id":"UNWN8PRaUCE9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 6: Use this to create a zero-shot image classifier**\n","\n","Zero-shot refers to being able to classify things it wasn't directly trained on. We can create any set of classes we want and use any image we want. The model can use what it already knows about language, images, and their relationship, to perform classification.\n","\n","The key idea is that we will create a set of possible captions for the image, and extract embeddings for each. Then we'll decide which caption has the most similar embedding to our image."],"metadata":{"id":"JGrO0W6qUVMf"}},{"cell_type":"code","source":["# these can be anything you want - try making them very similar to make it hard\n","candidate_labels = ['A photo of a brown cat with long whiskers',\n","                  'A unicorn on a green lawn',\n","                  'A piece of pizza',\n","                  'A large tiger in the jungle',\n","                  'A white cat with short whiskers',\n","                  'A medium sized orange kitten']\n","\n","\n","name = 'cat'\n","file_path = os.path.join(images_folder, '%s.jpg'%name)\n","image = PIL.Image.open(file_path).convert('RGB')\n"],"metadata":{"id":"qOaslDu_Uwsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now we run the classifier, by passing images and captions into model.\n","\n","with torch.no_grad():\n","  inputs = tokenizer(candidate_labels, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","  caption_embeddings= clip_model.get_text_features(**inputs)\n","  caption_embeddings = torch.nn.functional.normalize(caption_embeddings, dim=-1) # L2 normalization\n","\n","  image_input = image_processor(image, return_tensors=\"pt\")\n","  image_input = image_input.pixel_values\n","  image_input = image_input.to(device)\n","\n","  image_embedding = clip_model.get_image_features(pixel_values = image_input)\n","  image_embedding = torch.nn.functional.normalize(image_embedding, dim=-1) # L2 normalization\n","\n","  # compute embedding similarity: dot product\n","  similarity = image_embedding @ caption_embeddings.T\n","  scaled_similarity = similarity * clip_model.logit_scale.exp() # scaling parameter for CLIP\n","\n","  # Convert to probabilities using softmax\n","  probabilities = F.softmax(scaled_similarity, dim=1)  # [1, 4]\n","\n","print(probabilities)"],"metadata":{"id":"dP2_02cJcIZ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---"],"metadata":{"id":"tYCFToyHhNvT"}},{"cell_type":"markdown","source":["***Question 7:***\n","\n","Write some code to summarize the results of the classifer (print or table form). What probability did it assign to each caption? What was its top prediction?"],"metadata":{"id":"vsQqSIlghNvU"}},{"cell_type":"code","source":["# answer here"],"metadata":{"id":"fCrl1lfBhetw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---"],"metadata":{"id":"xirQqRKkhNvU"}},{"cell_type":"markdown","source":["***Question 8:***\n","\n","Make your own zero-shot classifier, for an example image and set of captions of your choosing. Can you find an image that is very difficult to classify? Consider trying: non-natural images like cartoons, or sets of captions that are semantically similar."],"metadata":{"id":"VlqvzrbLhwas"}},{"cell_type":"code","source":["# answer here"],"metadata":{"id":"vL7Sb5nzihpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---"],"metadata":{"id":"lF5IVbAFhwat"}}]}