{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hendersonneurolab/CogAI_Fall2025/blob/master/Lab09_Neural_Decoding.ipynb)\n","\n","## Week 9: Decoding variables from fMRI data.\n","\n","In this lab, we'll learn how to construct a decoder to read out information from population-level neural activation patterns. This is a form of multi-voxel pattern analysis (MVPA). We will use a linear decoder, a tool that is commonly used in neuroscience to measure the information about a variable that is represented by a neural population. As in the previous exercises, we'll use data from the Natural Scenes Dataset (NSD).\n","\n","**Learning Objectives:**\n","- Understand the steps involved in constructing and evaluating a classifier using neural data.\n","- Know how to interpret the accuracy of neural decoders, and compare results between brain regions.\n"],"metadata":{"id":"nbtEAybij279"}},{"cell_type":"markdown","source":["###Step 1: Importing libraries and downloading data."],"metadata":{"id":"TUudpEoClPil"}},{"cell_type":"code","source":["import numpy as np\n","import urllib.request\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import scipy\n","import os, sys\n","import h5py\n","import time\n","import torch\n","import zipfile\n","import copy\n","import warnings\n","import shutil\n","import sklearn\n","import sklearn.svm, sklearn.discriminant_analysis, sklearn.linear_model\n","warnings.filterwarnings('ignore')\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"5eFcRNENAnAD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","First, mount the Google Drive storage."],"metadata":{"id":"0iQMw9EuV90p"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# Navigate to the Colab Notebooks folder\n","colab_notebooks_path = '/content/drive/MyDrive/Colab Notebooks/'\n","os.chdir(colab_notebooks_path)\n","os.makedirs('CogAI', exist_ok=True)\n","os.makedirs('CogAI/data', exist_ok=True)\n","data_folder = os.path.join(colab_notebooks_path, 'CogAI', 'data')\n","print(data_folder)"],"metadata":{"id":"mvWAbPsc1lKw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download data files for this exercise. Note that several of these files were used in previous week's lab too (Lab07), so you might already have them in your Drive storage."],"metadata":{"id":"UfqwHo01kjj0"}},{"cell_type":"code","source":["# Info about ROIs\n","dbox_link = 'https://www.dropbox.com/scl/fi/kilrzj841mrpm17aj9gid/S1_voxel_roi_info.npy?rlkey=jgt1zje70ta8qpmaib8kmjskp&st=n9b3bxft&dl=1'\n","filename = os.path.join(data_folder, 'S1_voxel_roi_info.npy')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"IyZyF0ND5CoB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Info about images\n","dbox_link = 'https://www.dropbox.com/scl/fi/caabn3on6l9q8w32uxq8z/S1_image_info.csv?rlkey=cwb4mfruzcyrozrmdrfgerpp2&st=fg9i8ml3&dl=1'\n","filename = os.path.join(data_folder, 'S1_image_info.csv')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"WvLXJ6rT4z6X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["fMRI data: this one can take a while to download, but shouldn't take more than 2 minutes. If it's taking too long, check with the professor."],"metadata":{"id":"md2A4kgOk37c"}},{"cell_type":"code","source":["# fMRI data\n","dbox_link = 'https://www.dropbox.com/scl/fi/e040hit5avrptp4hbnijy/S1_betas_avg_bigmask.hdf5?rlkey=s8bpoara1fln1dhf1ydjpuldn&st=323ct1jm&dl=1'\n","filename = os.path.join(data_folder, 'S1_betas_avg_bigmask.hdf5')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"dtq7XKDp1VjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download category labels for the images.\n","\n","Recall from the NSD paper, that all the images in this dataset come from the MS COCO dataset, and include annotations in many different categories. Here, we'll download a simple version of these annotations.\n","\n","More info about COCO can be found here: https://cocodataset.org/#explore"],"metadata":{"id":"vCVHl0wjfFQ2"}},{"cell_type":"code","source":["dbox_link = 'https://www.dropbox.com/scl/fi/81gok5e9abzeq45oz4wq6/S1_cocolabs_binary.csv?rlkey=fpxzptb96h6poznf2345lcnzz&st=g8dizxxm&dl=1'\n","filename = os.path.join(data_folder, 'S1_cocolabs_binary.csv')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"ADTZdQwJB68y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download the COCO images themselves, for this subject."],"metadata":{"id":"4Uamku5rfc3x"}},{"cell_type":"code","source":["dbox_link = 'https://www.dropbox.com/scl/fi/vfoeto2lrpoz2ik2642nn/S1_stimuli_224.h5py?rlkey=v74bvk0he1qi4tx1mmjgjk70j&st=ohc37b2b&dl=1'\n","filename = os.path.join(data_folder, 'S1_stimuli_224.h5py')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)\n"],"metadata":{"id":"7IrFZjdA66j1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Step 2: Load and inspect the data."],"metadata":{"id":"LEhy7D0HhB9X"}},{"cell_type":"markdown","source":["Now that the files are downloaded, we need to load them into Python.\n"],"metadata":{"id":"lQ1ZRIjU7HHT"}},{"cell_type":"markdown","source":["First, load the **NSD images**. This is a big matrix [10,000 x 3 x 224 x 224]. It contains the pixel values for all 10,000 images in the experiment. The 10,000 images are in the same order as in the fMRI data matrix."],"metadata":{"id":"Y677VPjk64Rk"}},{"cell_type":"code","source":["data_filename = os.path.join(data_folder, 'S1_stimuli_224.h5py')\n","print(data_filename)\n","\n","t = time.time()\n","with h5py.File(data_filename, 'r') as data_set:\n","  print(data_set.keys())\n","  images = np.copy(data_set['/stimuli'])\n","  data_set.close()\n","elapsed = time.time() - t\n","print('Took %.5f seconds to load file'%elapsed)"],"metadata":{"id":"rb1ZYsxU7D7w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next load the **COCO category labels** for the images.\n","\n","These are stored as a pandas dataframe, where each column is one category.\n","\n","They are binary labels, which indicate whether the category is present or absent in each image. 1=present, 0=absent.\n","\n","Note that images can have more than one category in them. For example, an image of a person on a boat would be labeled with \"person\" and \"vehicle\"."],"metadata":{"id":"mO-V273ehIdd"}},{"cell_type":"code","source":["fn = os.path.join(data_folder, 'S1_cocolabs_binary.csv')\n","print(fn)\n","cocolabs = pd.read_csv(fn, index_col=0)\n","cocolabs = cocolabs.iloc[:,0:12]\n","# I'm pulling out the first 12 columns here, which correspond to the \"supercategories\" in COCO.\n","# The remaining columns label the more fine-grained categories.\n","\n","cocolabs"],"metadata":{"id":"onS-jyVhCWqV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To understand this, let's plot a random image and its associated COCO labels. You can run this cell a few times to get different images."],"metadata":{"id":"Q22x8uorgYI2"}},{"cell_type":"code","source":["# ii = 2343\n","ii = np.random.choice(np.arange(10000),1)\n","img=np.moveaxis(images[ii[0],:,:,:], [0], [2])\n","plt.imshow(img)\n","\n","# pull out corresponding row of the dataframe.\n","cocolabs.iloc[ii]"],"metadata":{"id":"Xt-eujBP8Xxb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Next, load the **fMRI data**.\n","\n","The data (betas_avg_bigmask) is organized as: [images x voxels]\n","\n","Each image was shown multiple times, and these values capture average response to each image.\n","\n","Keep in mind this is already several steps of preprocessing removed from the raw data. Steps like motion correction have already been performed to improve signal quality. Single-trial beta weights were already extracted using a GLM analysis. Data have also been z-scored, within each session, and the beta weights for repetitions of each image have been averaged.\n","\n","Note also that the voxels in this matrix are not the entire brain. They represent a wide portion of visual cortex, including all voxels with reliable signal.\n","   \n"],"metadata":{"id":"6W_BGkrSl_Aj"}},{"cell_type":"code","source":["data_filename = os.path.join(data_folder, 'S1_betas_avg_bigmask.hdf5')\n","print(data_filename)\n","\n","t = time.time()\n","with h5py.File(data_filename, 'r') as data_set:\n","    voxel_data = np.copy(data_set['/betas'])\n","    data_set.close()\n","elapsed = time.time() - t\n","print('Took %.5f seconds to load file'%elapsed)"],"metadata":{"id":"kIwt3yTB7EC_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load info about the **ROIs (regions of interest)** in this dataset."],"metadata":{"id":"aShMdaYNrVL6"}},{"cell_type":"code","source":["# ROI = region of interest. These are visual areas we want to focus on for analysis.\n","fn = os.path.join(data_folder, 'S1_voxel_roi_info.npy')\n","print(fn)\n","rinfo = np.load(fn, allow_pickle=True).item()\n","# this is a dictionary that contains information about which voxels our data will include."],"metadata":{"id":"TCN3IruBrUWB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make a function to pull out voxels in a target ROI."],"metadata":{"id":"iWXYhl1fiokO"}},{"cell_type":"code","source":["def get_roi_vox(roi_name = 'FFA-2'):\n","\n","  voxel_mask = rinfo['voxel_mask']\n","\n","  if roi_name in rinfo['ret_prf_roi_names']:\n","    ind_num = rinfo['ret_prf_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_retino'][voxel_mask]==ind_num\n","\n","  elif roi_name in rinfo['floc_face_roi_names']:\n","    ind_num = rinfo['floc_face_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_face'][voxel_mask]==ind_num\n","\n","  elif roi_name in rinfo['floc_place_roi_names']:\n","    ind_num = rinfo['floc_place_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_place'][voxel_mask]==ind_num\n","\n","  elif roi_name in rinfo['floc_body_roi_names']:\n","    ind_num = rinfo['floc_body_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_body'][voxel_mask]==ind_num\n","\n","  elif roi_name=='all':\n","    # return all of vis cortex, very big\n","    roi_inds = np.ones((np.sum(rinfo['voxel_mask']),), dtype=bool)\n","\n","  return roi_inds"],"metadata":{"id":"XSVru98i4M50"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Step 3: Classify animal vs. vehicle images.\n","\n","We will start by constructing a binary classifier, which aims to use the fMRI activation patterns evoked by each image to decode the category of the viewed image.\n","\n","Because the images in COCO are complex natural scenes, there are many possible ways we can classify them. They often have more than one category label.\n","\n","As a starting point, we can define a binary distinction between images with animals and images with vehicles. Animals and vehicles are both visually and semantically distinct, so we should expect this classifier to do well. Other distinctions between more similar categories may be more difficult."],"metadata":{"id":"kH9YgGRyeWSP"}},{"cell_type":"markdown","source":["First let's define the animal and vehicle labels."],"metadata":{"id":"clCAQG9UZout"}},{"cell_type":"code","source":["has_animal = np.array(cocolabs['animal']).astype(bool)\n","has_vehicle = np.array(cocolabs['vehicle']).astype(bool)\n","\n","# Count the number that have one category, both, or neither.\n","print('Animal only: %d'%np.sum(has_animal))\n","print('Vehicle only: %d'%np.sum(has_vehicle))\n","print('Both: %d'%np.sum(has_animal & has_vehicle))\n","print('Neither: %d'%np.sum(~has_animal & ~has_vehicle))\n"],"metadata":{"id":"zPlyTY-bZyCh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We see that there are some images with both categories present, and some with neither category present. For the simplest implementation of a binary classifier, we want to ignore these \"ambiguous\" images, and only focus on the\n"," \"unambiguous\" images, which have only animal or vehicle.\n","\n"," Let's pull out those unambiguous images only."],"metadata":{"id":"ymEaQxp2aDFa"}},{"cell_type":"code","source":["ambiguous = (has_animal & has_vehicle) | (~has_animal & ~has_vehicle)\n","\n","print('%d ambiguous images'%np.sum(ambiguous))\n","\n","labels = has_animal.astype(float)\n","# Setting the ambiguous ones to NaN here.\n","labels[ambiguous] = np.nan\n","\n","print('%d unambiguous images'%np.sum(~np.isnan(labels)))"],"metadata":{"id":"hjr2uOAHGiwT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Decide how many cross-validation folds we want to use.\n","\n","Recall some key terms here:\n","\n","- **Cross-validation:** When we train a decoder (or, encoding model) on one set of data (*training set*) and test it on an independent set of data (*test set*). This is an essential step in machine learning, which ensures that we aren't just memorizing noise in the training data.\n","- **Folds:** During cross-validation, we split the data into train and test sets multiple times. Each time, we hold out some percent of the data, and we do this repeatedly until every trial has been in the test set exactly once. For example, in 4-fold cross validation, we would hold out 25% of the data on each fold, and would do that 4x, with a different 25% each time.\n","\n","There is a trade-off here where more folds can lead to better performance, because it uses more data per fold. But fewer folds will be faster, so let's try just 4 here."],"metadata":{"id":"2bWJBewDk7Od"}},{"cell_type":"code","source":["n_cv = 4"],"metadata":{"id":"XUQwHOWdGiwU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Balancing the labels:** for the binary classifier to be completely unbiased, we ideally would like have exactly half the trials in each label group (i.e., 50% animal and 50% vehicle).\n","\n","In reality, there are rarely exactly half the trials in each category, so the classes are \"unbalanced\". There are various ways to get around this problem of unbalanced classes. Here we'll address this by randomly sub-sampling the trials in the larger category, so that it has the same number of trials as the smaller category.\n","\n","Write a function to do this:"],"metadata":{"id":"rx-y9uiKaUe_"}},{"cell_type":"code","source":["def make_balanced_labels(labels, n_cv = 4):\n","\n","  # let's figure out how to make a balanced set of images.\n","  un, n_per_class = np.unique(labels[~np.isnan(labels)], return_counts=True)\n","  n_classes = len(un)\n","\n","  # how many are in the smaller class?\n","  small_class = np.min(n_per_class)\n","  # trim this many to make it an even multiple of n_cv, just makes it easier.\n","  remove_each = int(np.mod(small_class, n_cv))\n","  small_class = small_class - remove_each\n","\n","  # I'm going to randomly sample \"n\" from the larger class, to balance them.\n","  # seed the random number generator, so you get the same result every time.\n","  np.random.seed(213321)\n","  inds_use = []\n","  # loop over the classes\n","  for uu in un:\n","\n","    # for each class, randomly sample \"n\" samples\n","    inds_all = np.where(labels==uu)[0]\n","    inds_samp = np.random.choice(inds_all, size=small_class, replace=False)\n","    inds_use += [inds_samp]\n","\n","  # This is the indices we will use to make the balanced set.\n","  inds_use = np.concatenate(inds_use)\n","  assert(len(np.unique(inds_use))==len(inds_use)) # make sure we didn't mess this up\n","\n","  # Adjusted set of labels for the images\n","  labels_use = labels[inds_use]\n","  un, n_per_class = np.unique(labels_use, return_counts=True)\n","  assert(np.all(n_per_class==n_per_class[0]))\n","\n","  return labels_use, inds_use"],"metadata":{"id":"3P3lPmziTTVk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Original counts in each group:')\n","print('%d images in label 0'%np.sum(labels==0))\n","print('%d images in label 1'%np.sum(labels==1))\n","\n","labels_use, inds_use = make_balanced_labels(labels)\n","\n","print('\\nAdjusted counts in each group:')\n","print('%d images in label 0'%np.sum(labels_use==0))\n","print('%d images in label 1'%np.sum(labels_use==1))\n"],"metadata":{"id":"HobrruAqTVXs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The variable \"labels_use\" now contains the labels that have been adjusted to have 50% per category.\n","\n","The variable \"inds_use\" now provides an index that tells us how to get the balanced set of data out of our original data matrices (which have 10,000 elements).\n","\n"],"metadata":{"id":"Rl4dYqPpoFoI"}},{"cell_type":"markdown","source":["As a sanity check - let's plot some example images in each of the categories.\n","- If you run this cell multiple times, you'll get a different set of random images. Note how much variability there is in these images."],"metadata":{"id":"wFUkKUJEaax_"}},{"cell_type":"code","source":["names = ['vehicle','animal']\n","# np.random.seed(234235)\n","\n","for li, ll in enumerate(names):\n","\n","  inds = np.where(labels_use==li)[0]\n","  inds = np.random.choice(inds, size=4, replace=False)\n","\n","  # \"inds\" only indexes into the subset of trials in \"labels_use\"\n","  # inds_for_full_array now provides index back into the original 10,000 images.\n","  inds_for_full_array = inds_use[inds]\n","\n","  plt.figure(figsize=(8, 3))\n","  pi=0\n","\n","  for ii in inds_for_full_array:\n","\n","    image = np.moveaxis(images[ii], [0],[2])\n","\n","    pi+=1\n","    plt.subplot(1,4,pi)\n","    plt.imshow(image)\n","    plt.axis('off')\n","    # plt.title('image %d'%ii)\n","\n","  plt.suptitle(ll, y=0.85)\n","  plt.tight_layout()\n"],"metadata":{"id":"qtYAfttZZ7be"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, define the cross-validation (CV) indices. This will be an vector of values [n_trials] long, which tells us on which cross-validation fold the trials should serve as the testing set."],"metadata":{"id":"gZAWduzUcDKj"}},{"cell_type":"code","source":["def make_crossval_inds(labels_use, n_cv = 4):\n","\n","  # fixed random seed to make sure shuffling is repeatable\n","  rndseeds = 53455\n","\n","  n_trials = len(labels_use)\n","  n_classes = len(np.unique(labels_use))\n","\n","  # how many trials will participate in each CV fold\n","  n_per_cv = int(np.ceil(n_trials/n_cv))\n","\n","  # initialize the cv_inds vector\n","  cv_inds = np.zeros_like(labels_use)\n","\n","  # looping over label values (cv inds will be balanced within each label value)\n","  for li, ll in enumerate(np.unique(labels_use)):\n","    inds = labels_use==ll\n","    c = np.repeat(np.arange(n_cv), int(n_per_cv/n_classes))\n","    cv_inds[inds] = c[np.random.permutation(len(c))] # randomize within this group\n","\n","  return cv_inds"],"metadata":{"id":"0YRn4qfGEGP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cv_inds = make_crossval_inds(labels_use, n_cv)\n","# print unique values in cv_inds\n","np.unique(cv_inds, return_counts=True)"],"metadata":{"id":"1lqiIJdzUKIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that the labels are handled, we're ready for the fMRI data. Choose one ROI to focus on here."],"metadata":{"id":"cwR3jS_cbS0A"}},{"cell_type":"code","source":["# roi_name = 'V1v'\n","roi_name = 'FFA-1'\n","roi_inds = np.copy(get_roi_vox(roi_name))\n","\n","rdat = voxel_data[:,roi_inds]\n","rdat = rdat[inds_use,:]\n","\n","rdat.shape"],"metadata":{"id":"tNw_vjh84_nL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before decoding, we generally **subtract the mean across voxels**, from each trial. This step ensures that the decoder is picking up on information represented across multivariate patterns, as opposed to the mean signal in the ROI.\n","\n","For intuition on this - we know that some ROIs will have higher average activation for one category versus another (for example, face-selective areas like FFA have higher average response to faces than objects). If we don't subtract this \"mean\" response, than it can cause the decoding accuracy to be higher, and we don't know if decoding is caused by mean difference between categories or by pattern-level differences (some voxels go up, some go down). If we do subtract the mean, then we know that decoding is driven by a distributed pattern-level representation."],"metadata":{"id":"RVTXsNQGeP5U"}},{"cell_type":"code","source":["# subtract the mean here\n","rdat = rdat - np.tile(np.mean(rdat, axis=1, keepdims=True), [1, rdat.shape[1]])\n","\n","rdat.shape"],"metadata":{"id":"dARjOOdQePbx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we're ready to run the classifier. Let's start with one *fold* at a time (eventually, we'll want to loop over many of these folds). Start by splitting the data into training and testing sets:"],"metadata":{"id":"lKtxTAFFcRMP"}},{"cell_type":"code","source":["cv = 0\n","\n","trninds = cv_inds!=cv\n","tstinds = cv_inds==cv\n","\n","trndat = rdat[trninds,:]\n","tstdat = rdat[tstinds,:]\n","\n","trnlabs = labels_use[trninds]\n","tstlabs = labels_use[tstinds]\n","\n","# check for balanced labels\n","assert(not np.any(np.isnan(trnlabs)))\n","un, counts = np.unique(trnlabs, return_counts=True)\n","assert(counts[0]==counts[1])\n","\n","print('Training data is shape:')\n","print(trndat.shape)\n","\n","print('Testing data is shape:')\n","print(tstdat.shape)"],"metadata":{"id":"BerlaNlxV_gX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The linear classifier we're using here is a ***ridge classifier***. This is similar to the ridge regression models we've used for encoding models in previous weeks. In this case, it is using ridge regression to learn a projection from voxel response patterns to category labels.\n","\n","A benefit of the ridge classifier is that it uses regularization, ensuring that the decoder doesn't over-fit. This can be very important for fMRI decoding, because we have many voxels in our ROI, and we don't know which of these are most informative versus more noisy. A regularized classifier ensures that we focus on the most informative voxels.\n","\n","Note that this is different from how ridge is used in encoding models. In encoding, we're mapping from a set of features (DNN features) to a voxel response. In decoding, we're mapping from a voxel response pattern (many voxels) to a category output. Hence why this is decoding versus encoding.\n","\n","Note also - this is not the only kind of linear classifier that can be used here. In neuroscience, other linear methods like [support vector machines](https://scikit-learn.org/stable/modules/svm.html) and [linear discriminant classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) are often used.\n","\n","To fit the ridge classifier - we need to choose a regularization parameter (a or $\\alpha$). To choose the best a, we will fit the model for multiple candidate a values, and choose the one that results in highest decoding accuracy."],"metadata":{"id":"_Pq_VnbDWxfj"}},{"cell_type":"code","source":["# define set of candidate a values\n","a_values = np.logspace(-9, 9, 8)\n","\n","st = time.time()\n","\n","acc_each_a = np.zeros_like(a_values)\n","\n","# loop over the possible regularization penalties\n","for ai, a in enumerate(a_values):\n","\n","  model = sklearn.linear_model.RidgeClassifier(alpha = a)\n","  model.fit(trndat, trnlabs)\n","  pred = model.predict(tstdat)\n","  acc_each_a[ai] = np.mean(pred==tstlabs)\n","\n","elapsed = time.time() - st\n","print('elapsed = %.2f seconds'%(elapsed))"],"metadata":{"id":"cKBz6arYcPhU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","***Question 1:***\n","\n","Which a value should we use?\n","\n","Print out the accuracy associated with the best a value."],"metadata":{"id":"xdgkxMI2XJUU"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"96C7amTBwk8e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"OBdq_qlswnEa"}},{"cell_type":"markdown","source":["In practice, we will make this procedure slightly more complicated by using **nested cross-validation** to select the a value.\n","\n","On each cross-validation fold, we'll hold out 25% of the data as our test set, and we'll choose the a parameter just from within the 75% training set. This will be done using nested cross-validation, where we hold out a sub-set of the training data, and use this to choose the best alpha. Then once we've chosen the best alpha, we'll still evaluate performance using the test set only.\n","\n","This nested procedure provides an additional safeguard against over-fitting. We're choosing all the model parameters (including a), based only on the training data, and using the testing data only for final evaluation.\n","\n","Let's write a function to do this, including a loop over all the cross-validation folds."],"metadata":{"id":"A86KSagyXX9h"}},{"cell_type":"code","source":["def decode_loop(rdat, labels_use, cv_inds):\n","\n","  pred_labels = np.zeros_like(labels_use)\n","\n","  for cv in np.unique(cv_inds):\n","\n","    print('CV fold %d of %d'%(cv, len(np.unique(cv_inds))))\n","\n","    trninds = cv_inds!=cv\n","    tstinds = cv_inds==cv\n","\n","    trndat = rdat[trninds,:]\n","    tstdat = rdat[tstinds,:]\n","\n","    trnlabs = labels_use[trninds]\n","    tstlabs = labels_use[tstinds]\n","\n","    # check for balanced labels\n","    assert(not np.any(np.isnan(trnlabs)))\n","    un, counts = np.unique(trnlabs, return_counts=True)\n","    assert(counts[0]==counts[1])\n","\n","    print(trndat.shape, tstdat.shape)\n","\n","    # define model\n","    st = time.time()\n","    a_values = np.logspace(-9, 9, 8)\n","    model = sklearn.linear_model.RidgeClassifierCV(cv = 3, \\\n","                                                    alphas = a_values, \\\n","                                                    scoring='accuracy',\n","                                                    )\n","    model.fit(trndat, trnlabs)\n","    elapsed = time.time() - st\n","\n","    # pull out the best regularization parameter\n","    best_alpha = model.alpha_\n","\n","    # compute train and test accuracy\n","    train_acc = model.score(trndat, trnlabs)\n","    test_acc = model.score(tstdat, tstlabs)\n","\n","    print('    cv fold %d (elapsed = %.6f s): best alpha = %.5f, train acc = %.2f, test acc = %.2f'%(\n","        cv, elapsed, best_alpha, train_acc, test_acc))\n","\n","    pred = model.predict(tstdat)\n","    pred_labels[tstinds] = pred\n","\n","  return pred_labels\n","\n"],"metadata":{"id":"al-9flLMSVp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement the full decoding procedure here:"],"metadata":{"id":"JnICTl5wxzIs"}},{"cell_type":"code","source":["roi_name = 'FFA-1'\n","roi_inds = get_roi_vox(roi_name)\n","\n","rdat = voxel_data[:,roi_inds]\n","rdat = rdat[inds_use,:]\n","\n","# subtract the mean here\n","rdat = rdat - np.tile(np.mean(rdat, axis=1, keepdims=True), [1, rdat.shape[1]])\n","\n","pred_labels = decode_loop(rdat, labels_use, cv_inds)"],"metadata":{"id":"61Uczkh7YIPJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 2:***\n","\n","Compute and print out the final cross-validated decoding accuracy of the model.\n","\n","What is the \"chance\" value for this decoder? Is the obtained accuracy close to chance, or above chance?\n"],"metadata":{"id":"dcF--FM6x4q3"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"9g3Z9IA_yGMv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"phYlzheZyHi1"}},{"cell_type":"code","source":["# Retinotopic ROI definitions:\n","print(rinfo['ret_prf_roi_names'])\n","# Face-selective ROI definitions:\n","print(rinfo['floc_face_roi_names'])\n","# Place-selective ROI definitions:\n","print(rinfo['floc_place_roi_names'])\n","# Body-selective ROI definitions:\n","print(rinfo['floc_body_roi_names'])"],"metadata":{"id":"WIRmP9CHykhp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 3:***\n","\n","Run the decoder for a different ROI (see options above), by copying the code above and modifying it.\n","\n","Print out the accuracy for your chosen area.\n","\n","- Note that if you choose a larger area (more voxels = more columns), this will take a bit longer.\n","\n","Does your area have higher or lower accuracy compared to FFA-1 (tested in Question 2)? How do you interpret this difference?"],"metadata":{"id":"zf1-ARwNc6YJ"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"EleCce0FXvxz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"3tgsnsqRzI3c"}},{"cell_type":"markdown","source":["###Step 4: Classify other categories.\n","\n","Now let's see how well the classifier can handle other category distinctions.\n","\n","This will enable us to ask how information about different categories is represented across different visual regions."],"metadata":{"id":"vd-RlraPa8V6"}},{"cell_type":"markdown","source":["cocolabs.keys() has a list of all the categories we can use.\n","\n","These correspond to the \"supercategories\" in COCO (i.e., superordinate categories)."],"metadata":{"id":"mLlUVlcZTg9K"}},{"cell_type":"code","source":["list(cocolabs.keys())"],"metadata":{"id":"oxkiuhW_TkJQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make a binary food vs. not-food classifier, which will distinguish images that contain food from those without food."],"metadata":{"id":"jI7gpK7g0AHi"}},{"cell_type":"code","source":["categ_name = 'food'\n","\n","labels = np.array(cocolabs[categ_name])\n","\n","labels_use, inds_use = make_balanced_labels(labels)\n","cv_inds = make_crossval_inds(labels_use)\n","\n","roi_name = 'FFA-1'\n","roi_inds = get_roi_vox(roi_name)\n","\n","rdat = voxel_data[:,roi_inds]\n","rdat = rdat[inds_use,:]\n","\n","# subtract the mean here\n","rdat = rdat - np.tile(np.mean(rdat, axis=1, keepdims=True), [1, rdat.shape[1]])\n","\n","print('Decoding %s vs. not-%s:\\n'%(categ_name, categ_name))\n","pred_labels = decode_loop(rdat, labels_use, cv_inds)\n"],"metadata":{"id":"mk7_Ld71U-PL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 4:***\n","\n","Compute and print the final accuracy of this classifier.\n","\n","What does this tell us about the neural representation of the food category? Is this surprising?"],"metadata":{"id":"ltBWML7Q0gPj"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"szTyYWy7Vq7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"cLZzI3EM1N7A"}},{"cell_type":"markdown","source":["Next, let's visualize some of the errors made by this decoder.\n","\n","These plots will show examples of images in each category, which are either correctly classified or mis-classified by the decoder.\n"],"metadata":{"id":"NVFmC4gjb-Di"}},{"cell_type":"code","source":["is_correct = (pred_labels==labels_use).astype(int)\n","\n","categ_correct = (labels_use==1) & (is_correct==1)\n","categ_incorrect = (labels_use==1) & (is_correct==0)\n","\n","notcateg_correct = (labels_use==0) & (is_correct==1)\n","notcateg_incorrect = (labels_use==0) & (is_correct==0)\n","\n","# np.random.seed(234235)\n","\n","for inds, name in zip([categ_correct, categ_incorrect, notcateg_correct, notcateg_incorrect], \\\n","                       ['%s, correct'%categ_name, '%s, error'%categ_name, \\\n","                       'not %s, correct'%categ_name, 'not %s, error'%categ_name]):\n","\n","    inds = np.where(inds)[0]\n","    # print(len(inds))\n","    inds = np.random.choice(inds, size=4, replace=False)\n","\n","    # \"inds\" only indexes into the subset of trials in \"labels_use\"\n","    # inds_for_full_array now provides index back into the original 10,000 images.\n","    inds_for_full_array = inds_use[inds]\n","\n","    plt.figure(figsize=(8, 3))\n","    pi=0\n","\n","    for ii in inds_for_full_array:\n","\n","      # have to move the axis, want [H x W x 3]\n","      image = np.moveaxis(images[ii], [0],[2])\n","\n","      pi+=1\n","      plt.subplot(1,4,pi)\n","      plt.imshow(image)\n","      plt.axis('off')\n","      # plt.title('image %d'%ii)\n","\n","    plt.suptitle(name, y=0.85)\n","    plt.tight_layout()"],"metadata":{"id":"6p-JqMXxb6zB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 5:***\n","\n","Examine the images above, for the correct and error trials.\n","- You can try running the cell above again to get a different set of images, the code includes random sampling so it'll be different each time.\n","\n","Are there any patterns that you notice about the error trials?\n","\n","What kinds of images are hardest for this decoder?\n","\n"],"metadata":{"id":"hiPAx4Tg1fS9"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"Wev0h5CY1koo"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"Y8BhY6Z71jKW"}},{"cell_type":"markdown","source":["***Question 6:***\n","\n","By copying and pasting some of the code from above, construct a classifier for a different one of the COCO categories (e.g., animal, person, etc.)\n","\n","Run this decoder for at least 2 different ROIs, and print out the results for each ROI.\n","\n","How high is the accuracy for your chosen category?\n","\n","How does accuracy differ between regions? What does this tell you about the difference in representations between these regions?\n"],"metadata":{"id":"2NvOw0qy2Ylu"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"w6OQuU_O3DEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"pdkURVsX3Fel"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"1f9NlmiBrFdXbDEZgVtLzqO7GDyVu8iB1","timestamp":1759854758070},{"file_id":"1DtFzI_0NLMqd0X9-7YDbUDj8KEmcr9Cg","timestamp":1759107107904},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/cad5020cab595c3bf83a518b7e4d4125/neural_style_tutorial.ipynb","timestamp":1755445164657}],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}