{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1C2Uh7Uc_jH3Jj6YmT20ykScjUURlASzM","timestamp":1755825208042},{"file_id":"https://github.com/hendersonneurolab/CogAI_Fall2025/blob/master/Lab02_CNNs.ipynb","timestamp":1755825061065}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hendersonneurolab/CogAI_Spring2026/blob/master/Lab02_CNNs.ipynb)\n","\n","## Week 2: Convolutional Neural Networks (CNNs)\n","\n","In this tutorial, we'll work with convolutional neural networks.\n","\n","1. Construct a simple CNN using PyTorch and train it on the Fashion-MNIST dataset.\n","2. Work with a larger pre-trained CNN (AlexNet) and test its performance on object categorization.\n","3. Explore the internal representations of CNNs, including kernels and feature maps.\n","\n","**Learning objectives:**\n","- Understand the key steps involved in neural network training\n","- Understand the inputs and outputs to an image classifier\n","- Develop an intuition for the CNN architecture and how it works"],"metadata":{"id":"S2S5XbohVdu3"}},{"cell_type":"markdown","source":["# Part 1: Training a simple image classifier"],"metadata":{"id":"XthoCfPBqzAC"}},{"cell_type":"code","source":["# Start with import statements - we need several libraries from torch\n","# Torch = PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import models, transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","import time\n","import requests\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","from PIL import Image\n","\n","# Check if CUDA (GPU) is available - this will speed up training significantly\n","# If it says \"cpu\", use the menu at top right to select: \"change runtime type\"\n","# Then choose: T4 GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","if torch.cuda.is_available():\n","    print(f'GPU: {torch.cuda.get_device_name(0)}')\n","\n"],"metadata":{"id":"sxErdJpPV4sz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 1: Preparing the data**\n","\n","In PyTorch, there are several image datasets conveniently available by default. [Fashion-MNIST](https://docs.pytorch.org/vision/0.22/generated/torchvision.datasets.FashionMNIST.html) is one of these. It includes images of clothing items in 10 different categories.\n","\n","Our goal is to create a model that can predict which of the 10 categories each image belongs to.\n","\n","In the following code, we're going to create a \"dataset\" object that organizes the information about our dataset. Then we're going to create a \"dataloader\" object that handles the loading and batching of our images during model training.\n","\n","For more background, see: https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html"],"metadata":{"id":"OFsQrAyymgdb"}},{"cell_type":"code","source":["# Fashion-MNIST has 10 classes of clothing items\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","# this is where data files are stored.\n","# this ./data actually indicates a temp directory that will be deleted when your colab session ends.\n","data_root = './data'\n","\n","def prepare_data(batch_size=128):\n","    \"\"\"\n","    Prepares the Fashion-MNIST dataset for training and testing.\n","    \"\"\"\n","\n","    # Data preprocessing pipeline\n","    # Here we're creating a set of \"transforms\". These are functions that get\n","    # applied to each image, before it goes into the CNN.\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),                    # Convert to tensor [0,1]\n","        transforms.Normalize((0.2860,), (0.3530,))  # Normalize to mean=0, std=1\n","    ])\n","    # Why these transforms?\n","    # - ToTensor(): Converts PIL images to PyTorch tensors and scales from [0,255] to [0,1]\n","    # - Normalize(): Centers the data around 0, which helps neural networks train better\n","    #   The values (0.2860,) and (0.3530,) are the mean and std of Fashion-MNIST\n","\n","    # Download and load training data\n","    # We're making a dataset object here - this holds information about the image files.\n","    # Fashion-MNIST: 60,000 training images, 10,000 test images, 28x28 pixels each\n","    train_dataset = torchvision.datasets.FashionMNIST(\n","        root=data_root,\n","        train=True,\n","        download=True,        # Downloads dataset if not present\n","        transform=transform\n","    )\n","\n","    test_dataset = torchvision.datasets.FashionMNIST(\n","        root=data_root,\n","        train=False,\n","        download=True,\n","        transform=transform\n","    )\n","\n","    # Create DataLoader here.\n","    # DataLoader is created from the dataset object.\n","    # It handles batching, shuffling, and parallel loading\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,         # Shuffle training data each epoch\n","        num_workers=2,        # Parallel data loading (adjust based on your CPU)\n","        pin_memory=True       # Speeds up GPU transfer\n","    )\n","\n","    test_loader = DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,        # No need to shuffle test data\n","        num_workers=2,\n","        pin_memory=True\n","    )\n","\n","    return train_loader, test_loader, train_dataset, test_dataset\n","\n","\n","# Here we call the above function, making our dataset and dataloader objects.\n","train_loader, test_loader, train_dataset, test_dataset = prepare_data(batch_size=128)\n","\n"],"metadata":{"id":"Je9RtM7-gjhV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check out what's in the datasets...how big are they?\n","train_dataset, test_dataset"],"metadata":{"id":"TFCBAa5_oPwz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's view a couple of the images here...."],"metadata":{"id":"swYvnGXWsrpO"}},{"cell_type":"code","source":["\n","# Get one batch of data\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","\n","# Create a grid of subplots\n","fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n","axes = axes.ravel()\n","\n","num_samples = 8\n","for i in range(num_samples):\n","    # Convert tensor back to numpy for visualization\n","    # We need to denormalize: pixel = (normalized_pixel * std) + mean\n","    img = images[i].squeeze()\n","    img = img * 0.3530 + 0.2860  # Denormalize\n","    img = torch.clamp(img, 0, 1)  # Ensure values are in [0,1]\n","\n","    axes[i].imshow(img, cmap='gray')\n","    axes[i].set_title(f'{class_names[labels[i]]}')\n","    axes[i].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"Image shape: {images[0].shape}\")  # Should be [1, 28, 28]\n","print(f\"Batch shape: {images.shape}\")     # Should be [batch_size, 1, 28, 28]\n","print(f\"Labels shape: {labels.shape}\")    # Should be [batch_size]\n","\n"],"metadata":{"id":"lIIp3MAtmwgI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 2: Define the CNN model**\n","\n","Now we will create the architecture of the model.\n","\n","- This CNN model consists of several sequential layers: convolutional layers, followed by a nonlinearity (ReLU), followed by maxpooling layers. Last, there is a series of fully connected layers.\n","\n","Reminder of some key terms here:\n","- **Architecture**: the overall connectivity or wiring of the CNN. Components of the architecture include:\n","  - Convolutional layer: scans the image with a set of 2D filters.\n","  - ReLU: a nonlinearity that is applied after the convolutional layers, taking out any negative values.\n","  - Max-pooling layer: take maximum values within spatial regions; makes the feature map smaller.\n","  - Fully-connected layer: every unit connects to every other unit.\n","- **Weights**: the parameters of the CNN. These are the actual values that are used in each operation, such as convolutions and fully-connected layers. Weights are learned during model training.\n","\n","\n","\n","The building blocks of the model come from the `nn` module in PyTorch. This has many built in functions to create different types of layers: https://docs.pytorch.org/docs/stable/nn.html\n","\n","- The model is created as a \"FashionCNN\" object.\n","- It has a method called \"forward\". The forward method is what gets used when you run the FashionCNN model. For example if you run: `FashionCNN(x)`, then x will be input to the forward method.\n","\n"],"metadata":{"id":"vzr6XeBcmmHW"}},{"cell_type":"code","source":["\n","class FashionCNN(nn.Module):\n","    \"\"\"\n","    Our Convolutional Neural Network for Fashion-MNIST classification.\n","\n","    Architecture breakdown:\n","    - 2 Convolutional blocks (Conv -> ReLU -> MaxPool)\n","    - 3 Fully connected layers with dropout for regularization\n","    - Output layer with 10 units (one for each class)\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(FashionCNN, self).__init__()\n","\n","        # ====================================================================\n","        # CONVOLUTIONAL LAYERS\n","        # ====================================================================\n","\n","        # First convolutional block\n","        # Input: 1x28x28 (1 channel, 28x28 pixels)\n","        self.conv1 = nn.Conv2d(\n","            in_channels=1,      # Grayscale images have 1 channel\n","            out_channels=32,    # We want 32 feature maps\n","            kernel_size=3,      # 3x3 filters\n","            padding=1           # Padding to maintain spatial dimensions\n","        )\n","        # After conv1: 32x28x28\n","        # After maxpool1: 32x14x14\n","\n","        self.conv2 = nn.Conv2d(\n","            in_channels=32,\n","            out_channels=64,    # More feature maps for richer representations\n","            kernel_size=3,\n","            padding=1\n","        )\n","        # After conv2: 64x14x14\n","        # After maxpool2: 64x7x7\n","\n","        # Max pooling reduces spatial dimensions by half\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # ====================================================================\n","        # FULLY CONNECTED LAYERS\n","        # ====================================================================\n","\n","        # Calculate input size for first linear layer\n","        # After 2 max pooling operations: 28 -> 14 -> 7\n","        # So we have 64 feature maps of size 7x7 = 64 * 7 * 7 = 3136\n","\n","        self.fc1 = nn.Linear(64 * 7 * 7, 128)   # First hidden layer\n","        self.fc2 = nn.Linear(128, 64)            # Second hidden layer\n","        self.fc3 = nn.Linear(64, 10)             # Output layer (10 classes)\n","\n","        # Dropout for regularization - randomly sets some neurons to 0 during training\n","        # This prevents overfitting by making the model more robust\n","        # self.dropout = nn.Dropout(0.5)  # 50% dropout rate\n","        self.dropout = nn.Dropout(0.1)  # 10% dropout rate\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass through the network.\n","        This defines how data flows through our model.\n","        \"\"\"\n","\n","        # First convolutional block: Conv -> ReLU -> MaxPool\n","        x = self.pool(F.relu(self.conv1(x)))  # [batch, 32, 14, 14]\n","\n","        # Second convolutional block: Conv -> ReLU -> MaxPool\n","        x = self.pool(F.relu(self.conv2(x)))  # [batch, 64, 7, 7]\n","\n","        # Flatten for fully connected layers\n","        # View reshapes the tensor: -1 means \"infer this dimension\"\n","        x = x.view(-1, 64 * 7 * 7)  # [batch, 3136]\n","\n","        # First fully connected layer with ReLU and dropout\n","        x = F.relu(self.fc1(x))     # [batch, 128]\n","        x = self.dropout(x)         # Apply dropout (only during training)\n","\n","        # Second fully connected layer with ReLU and dropout\n","        x = F.relu(self.fc2(x))     # [batch, 64]\n","        x = self.dropout(x)\n","\n","        # Output layer - no activation here, we'll use CrossEntropyLoss\n","        # CrossEntropyLoss applies softmax internally\n","        x = self.fc3(x)             # [batch, 10]\n","\n","        return x\n","\n"],"metadata":{"id":"2Zza6WkBnPUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Quick test of the model: just pass one batch of images in."],"metadata":{"id":"XlVjIJE0r8c6"}},{"cell_type":"code","source":["# Make our CNN: it's an instance of this model class.\n","model = FashionCNN()\n","# Note that this model has NOT yet been trained.\n","# It is initialized with weights that are random.\n","\n","# Get one batch of data (128 images, 128 labels)\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","\n","# Forward pass: put images into model, get predictions\n","output = model(images)\n","# Convert from logits to class probabilities\n","output = F.softmax(output, dim=1)\n"],"metadata":{"id":"UUOU5X5giA-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Output contains the predicted probability assigned to each class by the model:\n","\n","[128 x 10]\n","- 128 = image batch size\n","- 10 = number of classes"],"metadata":{"id":"r9UBCK9UwwoG"}},{"cell_type":"markdown","source":["\n","---"],"metadata":{"id":"FDAwvIkruFl0"}},{"cell_type":"markdown","source":["***Question 1:***\n","\n","Write code to compute the accuracy of the model on this batch.\n","\n","Hint: Use the variables \"output\" and \"labels\". To get the predicted labels, we want to find the category the model assigned the highest probability to."],"metadata":{"id":"7wFMbbMZuMTI"}},{"cell_type":"code","source":["# answer here"],"metadata":{"id":"s9gmm59fw0qN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How well did its predicted labels align with the real labels? Does this make sense?\n","\n"],"metadata":{"id":"Hd4Y2fKgPJuN"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"HdC7UEcBPM02"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"upZwq1LvuLIZ"}},{"cell_type":"markdown","source":["**Step 3: Create training and evaluation procedures**\n","\n","Now we will create the functions needed for model training.\n","\n","Reminder of some key terms here:\n","\n","- Batch = a group of images that are processed together by the CNN.\n","- Step = processing one batch of images. A step includes:\n","  1. Forward pass: we pass the batch into the CNN, and collect outputs.\n","  2. Computing loss: we determine how accurate the outputs were.\n","  3. Computing gradients: we determine how much each weight should change to improve the loss. Uses backpropagation.\n","  4. Updating weights: we adjust the weights of the CNN, to make it better. The new weights get used on the next step.\n","- Epoch = one pass over the entire training dataset (includes multiple steps).\n","\n"],"metadata":{"id":"PLcpAQ5CnpGU"}},{"cell_type":"code","source":["# Make a function for training, just one epoch\n","\n","def train_epoch(model, train_loader, optimizer, device):\n","    \"\"\"\n","    Trains the model for one epoch (one pass through all training data).\n","\n","    Returns:\n","        avg_loss: Average loss for this epoch\n","        accuracy: Training accuracy for this epoch\n","    \"\"\"\n","    model.train()  # Set model to training mode (enables dropout, batch norm, etc.)\n","\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    # tqdm creates a progress bar\n","    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n","\n","    # loop over the whole training dataset, in batches.\n","\n","    for batch_idx, (data, target) in enumerate(progress_bar):\n","        # Move data to GPU if available\n","        data, target = data.to(device), target.to(device)\n","\n","        # Zero the gradients from the previous batch\n","        # PyTorch accumulates gradients by default\n","        optimizer.zero_grad()\n","\n","        # Step 1 - Forward pass: compute predictions\n","        output = model(data)\n","\n","        # Step 2 - Compute loss\n","        # Here, this is based on cross-entropy loss.\n","        # It's a measure of how well the output probabilities have captured\n","        # the actual labels.\n","        # CrossEntropyLoss combines softmax and negative log-likelihood\n","        # It's perfect for multi-class classification\n","        criterion = nn.CrossEntropyLoss()\n","        loss = criterion(output, target)\n","\n","        # Step 3 - Compute gradients\n","        loss.backward()\n","        # This is where we do \"backpropagation\". Sometimes called the backward pass.\n","\n","        # Step 4 - Update weights of the model, based on the gradients\n","        optimizer.step()\n","\n","\n","        # Statistics for monitoring\n","        running_loss += loss.item()\n","        _, predicted = torch.max(output.data, 1)  # Get class with highest probability\n","        total_samples += target.size(0)\n","        correct_predictions += (predicted == target).sum().item()\n","\n","        # Update progress bar\n","        progress_bar.set_postfix({\n","            'Loss': f'{loss.item():.4f}',\n","            'Acc': f'{100. * correct_predictions / total_samples:.2f}%'\n","        })\n","\n","    avg_loss = running_loss / len(train_loader)\n","    accuracy = 100. * correct_predictions / total_samples\n","\n","    return avg_loss, accuracy\n","\n"],"metadata":{"id":"RtLZHdw9qbwb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now make a function for evaluation, just for one epoch\n","\n","def evaluate_epoch(model, test_loader, device):\n","    \"\"\"\n","    Evaluates the model on test data.\n","\n","    Returns:\n","        avg_loss: Average test loss\n","        accuracy: Test accuracy\n","        class_accuracies: Accuracy for each class\n","    \"\"\"\n","    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n","\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    # Track predictions for each class\n","    class_correct = list(0. for i in range(10))\n","    class_total = list(0. for i in range(10))\n","\n","    with torch.no_grad():  # Disable gradient computation for faster inference\n","        for data, target in tqdm(test_loader, desc='Evaluating', leave=False):\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","            criterion = nn.CrossEntropyLoss()\n","            test_loss += criterion(output, target).item()\n","\n","            _, predicted = torch.max(output, 1)\n","            total += target.size(0)\n","            correct += (predicted == target).sum().item()\n","\n","            # Per-class accuracy\n","            c = (predicted == target).squeeze()\n","            for i in range(target.size(0)):\n","                label = target[i]\n","                class_correct[label] += c[i].item()\n","                class_total[label] += 1\n","\n","    avg_loss = test_loss / len(test_loader)\n","    accuracy = 100. * correct / total\n","\n","    # Calculate per-class accuracies\n","    class_accuracies = []\n","    for i in range(10):\n","        if class_total[i] > 0:\n","            class_acc = 100. * class_correct[i] / class_total[i]\n","            class_accuracies.append(class_acc)\n","        else:\n","            class_accuracies.append(0.0)\n","\n","    return avg_loss, accuracy, class_accuracies\n","\n"],"metadata":{"id":"tAMwoyAWnvDw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 4: Run the training procedure**\n","\n","\n","Here we will train the model for a number of epochs. We will use the training and eval functions that we just defined.\n","\n","Reminder of key terms here:\n","\n","- Loss: a measure of how badly the model has done on our task. Here, it's a classification task, so it captures classification accuracy, but in other scenarios it could capture some other task. The goal of training is to make the loss as low as possible.\n","- Gradients: for each model parameter, which direction should we change it to improve the model's loss?\n","- Optimizer: an algorithm that defines how we will update weights based on gradients, to improve loss.\n","\n","\n","Note: this cell can take a while to run, especially if you're using a CPU. Make sure you connect to the GPU, and it should take <5 minutes."],"metadata":{"id":"gEWY7OHhoGNt"}},{"cell_type":"code","source":["\n","# num_epochs = 10;\n","num_epochs = 5;\n","learning_rate = 0.001;\n","\n","# Move model to GPU if available\n","model = model.to(device)\n","\n","# Adam optimizer - adaptive learning rate, works well in practice\n","# Alternative: SGD with momentum\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Track training history\n","train_losses, train_accuracies = [], []\n","test_losses, test_accuracies = [], []\n","\n","# Evaluate on train and test set - before any training.\n","train_loss, train_acc, train_class_accs = evaluate_epoch(model, train_loader, device)\n","test_loss, test_acc, class_accs = evaluate_epoch(model, test_loader, device)\n","\n","# Print out how badly it did before we performed any training.\n","# This is like a \"random\" model\n","print(f'Before training - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n","print(f'Before training - Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n","\n","# Record history\n","train_losses.append(train_loss)\n","train_accuracies.append(train_acc)\n","test_losses.append(test_loss)\n","test_accuracies.append(test_acc)\n","\n","print(f\"\\nStarting training for {num_epochs} epochs...\")\n","print(f\"Training on {device}\")\n","\n","start_time = time.time()\n","\n","# loop over however many epochs we're doing\n","for epoch in range(num_epochs):\n","    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n","    print('-' * 50)\n","\n","    # Train for one epoch\n","    # Calling that function we made earlier.\n","    train_loss_running, train_acc_running = train_epoch(model, train_loader, optimizer, device)\n","\n","    # Evaluate on train and test set\n","    train_loss, train_acc, train_class_accs = evaluate_epoch(model, train_loader, device)\n","    test_loss, test_acc, class_accs = evaluate_epoch(model, test_loader, device)\n","\n","    # Record history\n","    train_losses.append(train_loss)\n","    train_accuracies.append(train_acc)\n","    test_losses.append(test_loss)\n","    test_accuracies.append(test_acc)\n","\n","    # Print epoch results\n","    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n","    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n","\n","    # Print per-class accuracies every few epochs\n","    if (epoch + 1) % 5 == 0:\n","        print(\"\\nPer-class accuracies:\")\n","        for i, acc in enumerate(class_accs):\n","            print(f'{class_names[i]}: {acc:.2f}%')\n","\n","total_time = time.time() - start_time\n","print(f'\\nTraining completed in {total_time:.2f} seconds')\n"],"metadata":{"id":"N5Q_lVrmtm_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting the final performance of the model, once it's fully trained\n","\n","_, final_accuracy, class_accuracies = evaluate_epoch(model, test_loader, device)\n","\n","print(f\"\\nFinal test accuracy: {final_accuracy:.2f}%\")\n","\n","print(\"\\nPer-class accuracies:\")\n","for i, acc in enumerate(class_accuracies):\n","    print(f\"{class_names[i]}: {acc:.2f}%\")\n"],"metadata":{"id":"eEy1wRlY7e1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the model's training progress over time\n","\n","def plot_training_history(history):\n","    \"\"\"\n","    Plots training and validation curves.\n","    \"\"\"\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n","\n","    # Loss curve\n","    epochs = range(0, len(history['train_losses']))\n","    ax1.plot(epochs, history['train_losses'], 'bo-', label='Training Loss')\n","    ax1.plot(epochs, history['test_losses'], 'ro-', label='Test Loss')\n","    ax1.set_title('Model Loss')\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('Loss')\n","    ax1.legend()\n","    ax1.grid(True)\n","\n","    # Accuracy curve\n","    ax2.plot(epochs, history['train_accuracies'], 'bo-', label='Training Accuracy')\n","    ax2.plot(epochs, history['test_accuracies'], 'ro-', label='Test Accuracy')\n","    ax2.set_title('Model Accuracy')\n","    ax2.set_xlabel('Epoch')\n","    ax2.set_ylabel('Accuracy (%)')\n","    ax2.legend()\n","    ax2.grid(True)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","history = {\n","    'train_losses': train_losses,\n","    'train_accuracies': train_accuracies,\n","    'test_losses': test_losses,\n","    'test_accuracies': test_accuracies\n","}\n","plot_training_history(history)"],"metadata":{"id":"ZyrXBVlgoHqW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---"],"metadata":{"id":"j4AOovPu4W_W"}},{"cell_type":"markdown","source":["***Question 2:***\n","\n","Describe the patterns of loss and accuracy over time. Why do they go up/down?\n","\n","Are there any differences between the values of training vs. test accuracy? Why?"],"metadata":{"id":"1PDLKxcE4W_X"}},{"cell_type":"markdown","source":["[your answer here]"],"metadata":{"id":"12qNa7cu4W_X"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"I6CDjPVL4W_Y"}},{"cell_type":"markdown","source":["# Part 2: Loading a large pre-trained model"],"metadata":{"id":"TF-9O1-fqq10"}},{"cell_type":"markdown","source":["**Step 1: Loading a pre-trained AlexNet model.**\n","\n","[AlexNet](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.alexnet.html) is a popular deep CNN architecture. It can work with larger, more complex images than the smaller model we used in Part 1. Because the model is so large and takes a lot of time and compute to train, we'll load a pre-trained model here and examine it.\n","\n","PyTorch has many built-in pretrained models that can be loaded in just a few lines.\n","\n","Learn more about the other models available here:\n","https://docs.pytorch.org/vision/main/models.html"],"metadata":{"id":"urw_BgaFUct0"}},{"cell_type":"code","source":["# Load pre-trained model\n","# It prints out the structure and composition of the model.\n","print(\"Loading AlexNet model...\")\n","model = models.alexnet(pretrained=True)\n","model.eval()  # Set to evaluation mode\n"],"metadata":{"id":"YpXmvNsC-72F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 2: Passing an example image into the model**\n","\n","Using the pre-trained model, we can examine which categories it predicts for a given image.\n","\n","First we're going to download an example image.\n"],"metadata":{"id":"FS_oC4uUYpAl"}},{"cell_type":"code","source":["# Helper function for image downloads\n","def download_image(url, filepath):\n","    try:\n","        headers = {\n","              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n","          }\n","        response = requests.get(url, headers=headers, timeout=10)\n","        # response = requests.get(url, timeout=10)\n","        response.raise_for_status()  # Raises exception for bad status codes\n","\n","        # Verify it's an image\n","        content_type = response.headers.get('content-type', '')\n","        if not content_type.startswith('image/'):\n","            print(f\"Warning: Content-Type is {content_type}, not an image\")\n","            return False\n","\n","        with open(filepath, 'wb') as f:\n","            f.write(response.content)\n","\n","        # Verify file size\n","        if os.path.getsize(filepath) < 100:  # Very small files are likely errors\n","            print(\"Warning: Downloaded file is suspiciously small\")\n","            return False\n","\n","        return True\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Request failed: {e}\")\n","        return False"],"metadata":{"id":"LNW8AW-Z_mft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount our Google Drive storage here\n","import os\n","from google.colab import drive\n","import requests\n","\n","drive.mount('/content/drive')\n","\n","# Navigate to the Colab Notebooks folder\n","colab_notebooks_path = '/content/drive/MyDrive/Colab Notebooks/'\n","os.chdir(colab_notebooks_path)\n","os.makedirs('CogAI/images', exist_ok=True)\n","images_folder = os.path.join(colab_notebooks_path, 'CogAI', 'images')\n","\n","# Your image URL\n","image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/American_Eskimo_Dog.jpg/256px-American_Eskimo_Dog.jpg'\n","# What we want to name the file\n","name = 'Dog.jpg'\n","\n","# This is the full path to the image, on drive.\n","file_path = os.path.join(images_folder, name)\n","print(file_path)\n","success = download_image(image_url, file_path)\n","print('Success = %s'%success)\n"],"metadata":{"id":"7gXQWNMS9shw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optional - upload your own image here:\n","\n","- Go to: https://drive.google.com/drive/\n","- Navigate to \"Colab Notebooks > CogAI > images\"\n","- Upload your image to this folder\n","- Edit \"name\" in the code below:"],"metadata":{"id":"FM94dq2wRwld"}},{"cell_type":"code","source":["# run this IF you uploaded your own image (otherwise, skip to next cell)\n","# What's your file called?\n","name = 'my_image1.jpg'\n","# This is the full path to the image, on drive.\n","file_path = os.path.join(images_folder, name)"],"metadata":{"id":"LPOpOB_gSBuE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load and pre-process the image for the network:"],"metadata":{"id":"i4srOQduEnzp"}},{"cell_type":"code","source":["# Define image preprocessing pipeline\n","# This ensures the image is the correct size and shape for our network.\n","# If it's the wrong shape/size, you'll get errors.\n","preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                        std=[0.229, 0.224, 0.225])\n","])\n","\n","preproc_simple = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224)\n","])\n","\n","# Load the image here:\n","image = Image.open(file_path).convert('RGB')\n","image_orig = image.copy()\n","\n","image_cropped = preproc_simple(image)\n","\n"],"metadata":{"id":"y5yVaWlgYPks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Display original image\n","plt.figure(figsize=(6, 6));\n","\n","plt.subplot(1,2,1);\n","plt.imshow(image);\n","plt.title('Original Image')\n","plt.axis('off');\n","# plt.show()\n","\n","plt.subplot(1,2,2);\n","plt.imshow(image_cropped);\n","plt.title('Resized/Cropped');\n","plt.axis('off');\n","# plt.show()\n","#\n"],"metadata":{"id":"LY5coA_tEtqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading info about the ImageNet object categories here.\n","# Because this model was trained on ImageNet, it outputs labels 1-1000, which\n","# correspond to object categories.\n","url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n","response = requests.get(url)\n","labels = response.text.strip().split('\\n')\n","labels = np.array(labels)\n","labels.shape\n","# print a few labels...\n","labels[0:10]\n"],"metadata":{"id":"YAlc8H8fBgI-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the image into a tensor format, and pre-process it for the CNN\n","input_tensor = preprocess(image).unsqueeze(0)\n","\n","# Now pass it into the model, and get the outputs\n","output = model(input_tensor)\n","\n","# Outputs consist of probabilities for each image class in imagenet.\n","# We're going to find the highest probability classes by sorting.\n","topn = torch.argsort(output, descending=True)[0][0:5]\n","\n","for ni, nn in enumerate(topn):\n","  print('Prediction #%d: %d, %s'%(ni, nn, labels[nn]))\n","\n","# Are these good predictions?"],"metadata":{"id":"jZPlqg5UZ64r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","***Question 3:***\n","\n","Go back up to the start of this section, and change the image to something else. You can find an image on google and copy the url, and enter it as `image_url`. Or, follow the instructions above to upload something from your computer.\n","\n"," Does the network make a good guess for this image? Try to find an image that the network gets incorrect."],"metadata":{"id":"6gQwPHkSbMeF"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"ED3qNu8Nb0oE"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"7RuI8InxdP0m"}},{"cell_type":"markdown","source":["\n","***Question 4:***\n","\n","Now go back to the top of the notebook (`model = models.alexnet(pretrained=True))` and change the variable `pretrained=False`. Then re-run the predictions on this image. How do the predictions change? Why?\n"],"metadata":{"id":"PzBQO5h9ay3x"}},{"cell_type":"markdown","source":["[answer here]\n"],"metadata":{"id":"QUl0bViFbzk4"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"HQsHWfvDCX3Z"}},{"cell_type":"markdown","source":["Put everything back. Back to the original image and the fully-trained model."],"metadata":{"id":"ga_PvVUfIP2X"}},{"cell_type":"code","source":["file_path = os.path.join(images_folder, 'Dog.jpg')\n","\n","image = Image.open(file_path)\n","image_orig = image.copy()\n","\n","image_cropped = preproc_simple(image)\n","input_tensor = preprocess(image).unsqueeze(0)\n","\n","print(\"Loading AlexNet model...\")\n","model = models.alexnet(pretrained=True)\n","\n","# You can also try pre-trained=False here if you want - how do the kernels change?\n","# model = models.alexnet(pretrained=False)\n","model.eval()  # Set to evaluation mode\n"],"metadata":{"collapsed":true,"id":"OUcLImLb7s59"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 3: Investigating the inner workings of the model.**\n","\n","Now we're going to visualize the internal representations of the model (i.e., activations) in response to an image.\n","\n","Reminder about some key terms:\n","- Weights: Parameters of the model that define how it processes the image. Learned during training.\n","- Activations: Response of model units to a specific image.\n","- Kernel (i.e., filter): The set of weights that is slid across the image when the convolution operation is applied. Like a 2D image filter aimed at detecting one particular feature. There are multiple kernels per CNN layer.\n","- Feature maps: The set of activations resulting from convolution with one kernel. It's a spatial map that indicates where in the image the corresponding feature is located."],"metadata":{"id":"yZ0HLxAucMc-"}},{"cell_type":"code","source":["# Because we want to visualize activations, not just get the outputs of the model,\n","# we have to add another function here.\n","\n","# Function to register hooks for activation extraction\n","activations = {}\n","def get_activation(name):\n","    def hook(model, input, output):\n","        activations[name] = output.detach()\n","    return hook\n","\n","# Register hooks for all the conv layers\n","conv_layer_inds = [0,3,6,8,10]\n","conv_layer_names = ['conv1', 'conv2', 'conv3', 'conv4', 'conv5']\n","for i, layer_name in enumerate(conv_layer_names):\n","    model.features[conv_layer_inds[i]].register_forward_hook(get_activation(layer_name))\n","\n","\n","# Forward pass to extract activations\n","print(\"Running forward pass...\")\n","with torch.no_grad():\n","    output = model(input_tensor)\n","print('Done')"],"metadata":{"id":"Tu-Do74_evZ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's visualize a few of the kernels from the first convolutional layer (conv1). The middle column shows the kernel, the right column shows the corresponding feature map activations. Basically, the middle column is \"slid\" across the image to create the right column."],"metadata":{"id":"au4bjWxMXg2a"}},{"cell_type":"code","source":["layer = model.features[0]\n","layer_weights = layer.weight.data.clone()\n","\n","num_kernels_plot = 16;\n","kernels = layer_weights[:num_kernels_plot]\n","\n","# Normalize kernels for visualization\n","kernels = (kernels - kernels.min()) / (kernels.max() - kernels.min())\n","\n","feature_maps = activations['conv1'].squeeze(0)  # Remove batch dimension\n","\n","kk_plot = [14,12,2]\n","\n","fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n","axes = axes.flatten()\n","\n","pi=-1\n","for ki, kk in enumerate(kk_plot):\n","\n","  pi+=1\n","  axes[pi].imshow(image_cropped)\n","  axes[pi].axis('off')\n","  # if ki==0:\n","  axes[pi].set_title('Original Image')\n","\n","  pi+=1\n","  kernel = kernels[kk].permute(1, 2, 0).cpu().numpy()  # CHW to HWC\n","  axes[pi].imshow(kernel)\n","  axes[pi].axis('off')\n","  axes[pi].set_title('Kernel %d'%(kk+1))\n","\n","  pi+=1\n","  axes[pi].imshow(feature_maps[kk])\n","  axes[pi].axis('off')\n","  axes[pi].set_title('Feature map %d'%(kk+1))\n","\n","plt.suptitle('conv1')"],"metadata":{"id":"FcfA4o1eT2xa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","***Question 5:***\n","\n","What do we notice about the relationship between these kernels and their feature maps? How would you describe the feature that each kernel is \"looking for\"?"],"metadata":{"id":"A9lZr1C3X1Ev"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"wap8r0BcX1Ew"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"lsJkSAbQX1Ew"}},{"cell_type":"markdown","source":["Now let's make the same plot but for a later layer (conv3). Notice in this case the kernels won't be in color. This is because the first layer has exactly 3 input channels (R, G, and B colors), so it's easy to plot them in color. But the later layers have more input channels, which don't correspond to color channels."],"metadata":{"id":"TKPvgHW1ZVGz"}},{"cell_type":"code","source":["\n","layer = model.features[6]\n","layer_weights = layer.weight.data.clone()\n","\n","num_kernels_plot = 16;\n","kernels = layer_weights[:num_kernels_plot]\n","\n","# Normalize kernels for visualization\n","kernels = (kernels - kernels.min()) / (kernels.max() - kernels.min())\n","\n","feature_maps = activations['conv3'].squeeze(0)  # Remove batch dimension\n","\n","kk_plot = [10, 11, 15]\n","\n","fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n","axes = axes.flatten()\n","\n","pi=-1\n","for ki, kk in enumerate(kk_plot):\n","\n","  pi+=1\n","  axes[pi].imshow(image_cropped)\n","  axes[pi].axis('off')\n","  # if ki==0:\n","  axes[pi].set_title('Original Image')\n","\n","  pi+=1\n","  # the [0] index here takes weights for the first input channel\n","  # there are actually more, but we can't plot them all...\n","  kernel = kernels[kk][0].cpu().numpy()\n","  axes[pi].imshow(kernel, cmap='gray')\n","  axes[pi].axis('off')\n","  axes[pi].set_title('Kernel %d'%(kk+1))\n","\n","  pi+=1\n","  axes[pi].imshow(feature_maps[kk])\n","  axes[pi].axis('off')\n","  axes[pi].set_title('Feature map %d'%(kk+1))\n","\n","  plt.suptitle('conv3')\n"],"metadata":{"id":"Lt_bCjtrZTHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","***Question 6:***\n","\n","What are the differences between the conv3 feature maps and conv1 feature maps? What features do you think the conv3 filters are \"looking for\" in the image?"],"metadata":{"id":"St8xs-2-Z2lf"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"rRbGny2OZ2li"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"2lwGZLsXZ2li"}},{"cell_type":"markdown","source":["Let's plot more of the kernels and corresponding feature maps from each layer. Notice the variety of filters.\n","\n"],"metadata":{"id":"ph7iOoMxJlC_"}},{"cell_type":"code","source":["for li, ll, layer_name in zip(range(5), conv_layer_inds, conv_layer_names):\n","\n","  layer = model.features[ll]\n","  layer_weights = layer.weight.data.clone()\n","\n","  # how many of these to plot now?\n","  num_kernels_plot = 16;\n","  kernels = layer_weights[:num_kernels_plot]\n","\n","  # Normalize kernels for visualization\n","  kernels = (kernels - kernels.min()) / (kernels.max() - kernels.min())\n","\n","  grid_size = int(np.ceil(np.sqrt(num_kernels_plot)))\n","  fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n","  fig.suptitle(f'Kernels from {layer_name}', fontsize=16)\n","\n","  for i in range(grid_size * grid_size):\n","      row, col = i // grid_size, i % grid_size\n","      if i < num_kernels_plot:\n","          if li==0:\n","            kernel = kernels[i].permute(1, 2, 0).cpu().numpy()  # CHW to HWC\n","          else:\n","            kernel = kernels[i][0].cpu().numpy()  # CHW to HWC\n","          # kernel = kernels[i].permute(1, 2, 0).cpu().numpy()  # CHW to HWC\n","          axes[row, col].imshow(kernel)\n","          axes[row, col].set_title(f'Kernel {i+1}')\n","      axes[row, col].axis('off')\n","  feature_maps = activations[layer_name].squeeze(0)  # Remove batch dimension\n","\n","  fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n","  fig.suptitle(f'Feature Maps from {layer_name}', fontsize=16)\n","\n","  for i in range(grid_size * grid_size):\n","      row, col = i // grid_size, i % grid_size\n","      if i < num_kernels_plot:\n","          feature_map = feature_maps[i].cpu().numpy()\n","          axes[row, col].imshow(feature_map, cmap='viridis')\n","          axes[row, col].set_title(f'Filter {i+1}')\n","      axes[row, col].axis('off')\n","\n","  # plt.tight_layout()\n","  # plt.show()"],"metadata":{"id":"FwZEHMqOYUZX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","***Question 7:***\n","\n","Let's connect this back to neuroscience!\n","\n","For each of the plots we just made, describe the analogous component in visual cortex. In other words, what's the \"brain inspiration\" for these elements:\n","- Kernels\n","- Each pixel within a feature map\n","- Feature maps within each layer\n","- Feature maps from different layers"],"metadata":{"id":"6BM5EX_1d6Kj"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"kJNLuDy1d6Kj"}},{"cell_type":"markdown","source":["\n","---\n","\n"],"metadata":{"id":"PsFDBX0Yggr-"}},{"cell_type":"markdown","source":["\n","And keep in mind the caveat: this is just a loose, theoretical correspondence. AlexNet should not be interpreted as an accurate model of visual cortex. More on this in future classes..."],"metadata":{"id":"rRgAI9qgI8jq"}}]}