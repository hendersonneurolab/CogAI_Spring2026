{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hendersonneurolab/CogAI_Fall2025/blob/master/Lab07_Encoding_Model.ipynb)\n","\n","## Week 7: Build an fMRI encoding model.\n","\n","In this tutorial, we'll learn how to build a simple encoding model for fMRI data, using data from the Natural Scenes Dataset and features from the AlexNet model. We will fit the model weights using ridge regression, and evaluate its performance on a held-out dataset.\n","\n","**Learning objectives**\n","- Understand the ingredients for an fMRI encoding model, including the format of data and feature inputs.\n","- Know the basic steps involved in fitting a ridge regression model, and how to select the $\\lambda$ parameter.\n","- Be able to interpret metrics related to model performance on held-out data, and how these relate to the reliability of the fMRI data.\n","\n"],"metadata":{"id":"nbtEAybij279"}},{"cell_type":"code","source":["import numpy as np\n","import urllib.request\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import scipy\n","import os, sys\n","import h5py\n","import time\n","import torch\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"5eFcRNENAnAD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 1: Download the data files and features."],"metadata":{"id":"TUudpEoClPil"}},{"cell_type":"markdown","source":["Download the data to your google drive here:\n"],"metadata":{"id":"0iQMw9EuV90p"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# Navigate to the Colab Notebooks folder\n","colab_notebooks_path = '/content/drive/MyDrive/Colab Notebooks/'\n","os.chdir(colab_notebooks_path)\n","os.makedirs('CogAI', exist_ok=True)\n","os.makedirs('CogAI/data', exist_ok=True)\n","data_folder = os.path.join(colab_notebooks_path, 'CogAI', 'data')\n","print(data_folder)"],"metadata":{"id":"mvWAbPsc1lKw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Info about ROIs\n","dbox_link = 'https://www.dropbox.com/scl/fi/kilrzj841mrpm17aj9gid/S1_voxel_roi_info.npy?rlkey=jgt1zje70ta8qpmaib8kmjskp&st=n9b3bxft&dl=1'\n","filename = os.path.join(data_folder, 'S1_voxel_roi_info.npy')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"IyZyF0ND5CoB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Info about images\n","dbox_link = 'https://www.dropbox.com/scl/fi/caabn3on6l9q8w32uxq8z/S1_image_info.csv?rlkey=cwb4mfruzcyrozrmdrfgerpp2&st=fg9i8ml3&dl=1'\n","filename = os.path.join(data_folder, 'S1_image_info.csv')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"WvLXJ6rT4z6X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["fMRI data: this one can take a while to download, but shouldn't take more than 2 minutes. If it's taking too long, check with the professor."],"metadata":{"id":"md2A4kgOk37c"}},{"cell_type":"code","source":["# fMRI data\n","dbox_link = 'https://www.dropbox.com/scl/fi/e040hit5avrptp4hbnijy/S1_betas_avg_bigmask.hdf5?rlkey=s8bpoara1fln1dhf1ydjpuldn&st=323ct1jm&dl=1'\n","filename = os.path.join(data_folder, 'S1_betas_avg_bigmask.hdf5')\n","if not os.path.exists(filename):\n","  st = time.time()\n","  print('downloading to %s...'%filename)\n","  urllib.request.urlretrieve(dbox_link, filename)\n","  print('elapsed = %.2f seconds'%(time.time() - st))\n","else:\n","  print('We already have: %s'%filename)"],"metadata":{"id":"dtq7XKDp1VjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download DNN features:"],"metadata":{"id":"8h0dVsKvkswC"}},{"cell_type":"code","source":["\n","dbox_links = ['https://www.dropbox.com/scl/fi/uw12ee05ecgniq3m814z8/NSD_S1_ims224pix_Conv1_ReLU_maxpool.hdf5?rlkey=hkromug6xybh4ddoqxwml18q9&st=dsoudo6e&dl=1', \\\n","              'https://www.dropbox.com/scl/fi/h9henxdggojf6sdtpai62/NSD_S1_ims224pix_Conv2_ReLU_maxpool.hdf5?rlkey=oqkur18hdnhuyl4hbzjpsqxp4&st=pzuhewly&dl=1', \\\n","              'https://www.dropbox.com/scl/fi/vbeb6ijvsc0qeo6dyo9tu/NSD_S1_ims224pix_Conv3_ReLU_maxpool.hdf5?rlkey=u5ipevvu9yosqnbytm81an0jj&st=y9f0f7yy&dl=1', \\\n","              'https://www.dropbox.com/scl/fi/v01uigo7u41ucpb22ymc4/NSD_S1_ims224pix_Conv4_ReLU_maxpool.hdf5?rlkey=4olepwu5sayo7l6w6t4vum6ul&st=8voneijm&dl=1', \\\n","              'https://www.dropbox.com/scl/fi/mzufg5ucj3ilv323hn9vd/NSD_S1_ims224pix_Conv5_ReLU_maxpool.hdf5?rlkey=gs8nxqnhkg2qwn47i8suklv3o&st=0zn59w87&dl=1']\n","layers = ['Conv1', 'Conv2', 'Conv3', 'Conv4', 'Conv5']\n","\n","for link, layer in zip(dbox_links, layers):\n","\n","  filename = os.path.join(data_folder, 'S1_%s.hdf5'%layer)\n","  if not os.path.exists(filename):\n","    st = time.time()\n","    print('downloading to %s...'%filename)\n","    urllib.request.urlretrieve(link, filename)\n","    print('elapsed = %.2f seconds'%(time.time() - st))\n","  else:\n","    print('We already have: %s'%filename)"],"metadata":{"id":"cUwQ7uop5Wjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that the files are downloaded, we need to load them into Python.\n"],"metadata":{"id":"lQ1ZRIjU7HHT"}},{"cell_type":"markdown","source":["First, load the **image information**. This is a .csv file that contains info about all the images shown."],"metadata":{"id":"Y677VPjk64Rk"}},{"cell_type":"code","source":["info_fn = os.path.join(data_folder, 'S1_image_info.csv')\n","print(info_fn)\n","info = pd.read_csv(info_fn)\n","# this df has [10,000] elements. Each element is 1 unique image.\n","# it contains info about the images and where they came from (within the MS COCO dataset).\n","# the rows of this correspond exactly to the features files (which will be loaded below).\n","\n","image_order = np.array(info['unique_ims'])\n","n_reps = np.array(info['n_reps'])"],"metadata":{"id":"8E868yQ-B26i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check out what's in \"info\""],"metadata":{"id":"kYwK0_FdmRhw"}},{"cell_type":"code","source":["info"],"metadata":{"id":"ZNqtjknXmT3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Next, load the **fMRI data**.\n","\n","The data (betas_avg_bigmask) is organized as: [images x voxels]\n","\n","Each image was shown multiple times, and these values capture average response to each image.\n","\n","Keep in mind this is already several steps of preprocessing removed from the raw data. Steps like motion correction have already been performed to improve signal quality.\n","\n","Single-trial beta weights were already extracted using a GLM analysis (similar to what we did last week).\n","\n","Data have also been z-scored, within each session, and the beta weights for repetitions of each image have been averaged.\n","\n","Note also that the voxels in this matrix are not the entire brain. They represent a wide portion of visual cortex, including all voxels with reliable signal.\n","   \n"],"metadata":{"id":"6W_BGkrSl_Aj"}},{"cell_type":"code","source":["data_filename = os.path.join(data_folder, 'S1_betas_avg_bigmask.hdf5')\n","print(data_filename)\n","\n","t = time.time()\n","with h5py.File(data_filename, 'r') as data_set:\n","    values = np.copy(data_set['/betas'])\n","    data_set.close()\n","elapsed = time.time() - t\n","print('Took %.5f seconds to load file'%elapsed)\n","\n","# Some of these values may be nans, only for some subjects\n","# this is for subjects who didn't complete all 40 sessions of NSD experiment.\n","# make sure we remove the nans now.\n","# for subject 1: we should have all the data, no nans.\n","good_values = ~np.isnan(values[:,0])\n","print(values.shape)\n","print(np.sum(~good_values))\n","\n","voxel_data = values[good_values,:]\n","print(voxel_data.shape)\n","\n","# check that nans are exactly where we expect\n","# nans happen when n_reps=0\n","assert(np.all(good_values[n_reps>0]))\n","assert(np.all(~good_values[n_reps==0]))"],"metadata":{"id":"kIwt3yTB7EC_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load info about the **ROIs (regions of interest)** in this dataset. Conveniently, the labels for these regions are already provided for us."],"metadata":{"id":"aShMdaYNrVL6"}},{"cell_type":"code","source":["# ROI = region of interest. These are visual areas we want to focus on for analysis.\n","fn = os.path.join(data_folder, 'S1_voxel_roi_info.npy')\n","print(fn)\n","rinfo = np.load(fn, allow_pickle=True).item()\n","# this is a dictionary that contains information about which voxels our data will include.\n","# voxel_mask is the whole set of voxels we're focusing on. basically all of visual cortex.\n","voxel_mask = rinfo['voxel_mask']"],"metadata":{"id":"TCN3IruBrUWB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the **DNN features (activations)**, which will be used to construct the encoding model.\n","\n","These are features from AlexNet, a large CNN model pre-trained on ImageNet. We pre-computed these features ahead of time, from several different layers of the model.\n","\n","These features are organized as [n_images x n_features], where the images are in the same order as the fMRI data. So, each row in the features matrices corresponds to one row in the fMRI data matrix."],"metadata":{"id":"d1j9HrI67mfY"}},{"cell_type":"code","source":["fdata = []\n","for layer in layers:\n","  filename = os.path.join(data_folder, 'S1_%s.hdf5'%layer)\n","  print(filename)\n","  with h5py.File(filename, 'r') as f:\n","      # Explore the file structure\n","      print(\"Keys in file:\", list(f.keys()))\n","\n","      # # Load your data (adjust based on your file structure)\n","      data = np.array(f['features'])\n","      fdata += [data]\n","\n","[f.shape for f in fdata]"],"metadata":{"id":"_Z4BQidA7ntt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's explore the ROIs in this dataset. We have ROIs defined in several ways: retinotopic early visual areas, and higher-level category selective regions."],"metadata":{"id":"cQ7hGWcRmlx5"}},{"cell_type":"code","source":["# Retinotopic ROI definitions:\n","print(rinfo['ret_prf_roi_names'])\n","# Face-selective ROI definitions:\n","print(rinfo['floc_face_roi_names'])\n","# Place-selective ROI definitions:\n","print(rinfo['floc_place_roi_names'])\n","# Body-selective ROI definitions:\n","print(rinfo['floc_body_roi_names'])"],"metadata":{"id":"uSGvvURem6aZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write a function that returns the set of voxels in any ROI."],"metadata":{"id":"FkSbEzeuzzur"}},{"cell_type":"code","source":["\n","def get_roi_vox(roi_name = 'FFA-2'):\n","\n","  if roi_name in rinfo['ret_prf_roi_names']:\n","    ind_num = rinfo['ret_prf_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_retino'][voxel_mask]==ind_num\n","\n","  elif roi_name in rinfo['floc_face_roi_names']:\n","    ind_num = rinfo['floc_face_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_face'][voxel_mask]==ind_num\n","\n","  elif roi_name in rinfo['floc_place_roi_names']:\n","    ind_num = rinfo['floc_place_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_place'][voxel_mask]==ind_num\n","\n","  elif roi_name in rinfo['floc_body_roi_names']:\n","    ind_num = rinfo['floc_body_roi_names'][roi_name]\n","    roi_inds = rinfo['roi_labels_body'][voxel_mask]==ind_num\n","\n","  return roi_inds\n"],"metadata":{"id":"Lt7AhL_FzmCF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Focusing on the retinotopic areas for now:"],"metadata":{"id":"D_6Ofr_9nZKG"}},{"cell_type":"code","source":["roi_name = 'V1v' # ventral part of V1.\n","\n","roi_inds = get_roi_vox(roi_name = roi_name)\n","\n","print('%s is %d voxels in size'%(roi_name, np.sum(roi_inds)))"],"metadata":{"id":"PDk038mgnSrB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 1:***\n","\n","Print out the sizes of the following retinotopic ROIs:\n","- V1d, V2v, V2d, V3v, V3d, hV4\n"],"metadata":{"id":"y-kCT3twoESa"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"Sou9IbAiojVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"A63pkjWlokfv"}},{"cell_type":"markdown","source":["Let's focus on one voxel for the next step. We're going to pick one from a target area, and picking the one with the best reliability.\n","\n","Recall that the **noise ceiling (NC)** is a measure that captures voxel reliability. It captures the proportion of total variance in the voxel's response that can be accounted for by its signal, as opposed to noise. We compute noise ceiling by measuring how much the voxel's response varies in response to repeats of a given image. The idea is, if it varies a lot in response to the same image, it's not very reliable. If it's not reliable, NC is low, and thus our encoding models will not fit well. So NC sets an approximate \"ceiling\" on encoding model performance."],"metadata":{"id":"VK9sWG08oqqS"}},{"cell_type":"code","source":["# Pick one ROI to focus on here.\n","roi_name = 'FFA-1'  # (FFA has 2 parts, we'll just grab the first part here.)\n","roi_inds = get_roi_vox(roi_name = roi_name)\n","roi_inds_num = np.where(roi_inds)[0]\n","\n","print('%s has %d voxels'%(roi_name, np.sum(roi_inds)))\n","\n","# Now getting the FFA voxel with the best noise ceiling.\n","noise_ceiling = rinfo['noise_ceiling_avgreps'] / 100\n","\n","# I'm going to choose the \"best voxel\" in my ROI of interest (highest NC)\n","# You can also try other voxels and see how this will differ.\n","best_inds = np.flip(np.argsort(noise_ceiling[roi_inds]))\n","best_inds = best_inds[0:1]\n","best_in_roi = roi_inds_num[best_inds]\n","\n","voxel_inds = best_in_roi\n","\n","print('best voxel inds are:')\n","print(voxel_inds)\n","print('NC of these voxels is:')\n","print(noise_ceiling[voxel_inds])\n"],"metadata":{"id":"Ef4kaewC7Vb_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Split data into train / test partitions\n","\n","During ridge regression, we typically split our data into 3 independent sets of images:\n","\n","- **Training data:** used to fit the model weights\n","- **Holdout data** (nested validation): used to choose best pRF and ridge parameters\n","- **Validation data**: held out until the very end, used to compute the validation set $R^2$.\n"],"metadata":{"id":"uuPbbdv_7Yd9"}},{"cell_type":"code","source":["# fixed random seed, to make sure shuffling is repeatable\n","rndseeds = [171301]; si = 0\n","\n","# Always holding out 1000 \"shared images\", which were seen by all NSD participants, as\n","# the validation set.\n","val_inds = np.array(info['shared1000'])\n","\n","# Then take a random 10% of the remaining data, as the nested \"holdout\" set.\n","# Holdout set is used to choose ridge parameters and pRF parameters.\n","# You could experiment with different % holdout, 10% usually works well.\n","pct_holdout = 0.10\n","n_images_total = info.shape[0]\n","n_images_notval = np.sum(~val_inds);\n","n_images_holdout = int(np.ceil(n_images_notval*pct_holdout))\n","n_images_trn = n_images_notval - n_images_holdout\n","\n","inds_notval = np.where(~val_inds)[0]\n","np.random.seed(rndseeds[si])\n","np.random.shuffle(inds_notval) # this is the only random part\n","\n","inds_trn = inds_notval[0:n_images_trn]\n","inds_holdout = inds_notval[n_images_trn:]\n","assert(len(inds_holdout)==n_images_holdout)\n","\n","trn_inds = np.isin(np.arange(0, n_images_total), inds_trn)\n","holdout_inds = np.isin(np.arange(0, n_images_total), inds_holdout)\n","\n","# remove nan rows here\n","trn_inds = trn_inds[good_values]\n","val_inds = val_inds[good_values]\n","holdout_inds = holdout_inds[good_values]\n","\n","# apply these indices to split the voxel data and image labels.\n","voxel_data_trn = voxel_data[trn_inds, :]\n","voxel_data_val = voxel_data[val_inds, :]\n","voxel_data_holdout = voxel_data[holdout_inds, :]\n","\n","n_voxels = voxel_data_trn.shape[1]\n","print(voxel_data_trn.shape, voxel_data_val.shape, voxel_data_holdout.shape)\n","\n","image_order_use = image_order[good_values]\n","\n","image_inds_trn = image_order_use[trn_inds]\n","image_inds_val = image_order_use[val_inds]\n","image_inds_holdout = image_order_use[holdout_inds]"],"metadata":{"id":"1W1jlSo67ZcK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we need to split the model features the same way we split the data.\n","\n","In this step, we're also going to z-score the features within each column. This is helpful because the different features can have very different variances, and normalizing these variances helps stabilize the resulting fits.\n"],"metadata":{"id":"bhfKARar82Sh"}},{"cell_type":"code","source":["f = fdata[4] # just grabbing the last Conv layer here. you can try others too...\n","\n","f_trn = f[trn_inds,:]\n","f_val = f[val_inds,:]\n","f_out = f[holdout_inds,:]\n","\n","# Z-score the data - this is a step that helps with fit stability.\n","# I'm computing the normalization parameters (mean and std) on my training data only\n","# (plus the nested held-out partition), but not the val set.\n","# this helps reduce leakage of data between train and val partitions.\n","# then apply those same normalization parameters to the val set too.\n","f_concat = np.concatenate([f_trn, f_out], axis=0)\n","# f_concat = f_trn\n","\n","features_m = np.mean(f_concat, axis=0, keepdims=True) #[:trn_size]\n","# print(features_m[0,0:10])\n","features_s = np.std(f_concat, axis=0, keepdims=True) + 1e-6\n","\n","f_trn -= features_m\n","f_trn /= features_s\n","f_out -= features_m\n","f_out /= features_s\n","f_val -= features_m\n","f_val /= features_s\n","\n","# add the intercept: a column of ones\n","f_trn = np.concatenate([f_trn, np.ones(shape=(len(f_trn), 1), dtype=f_trn.dtype)], axis=1)\n","f_out = np.concatenate([f_out, np.ones(shape=(len(f_out), 1), dtype=f_out.dtype)], axis=1)\n","f_val = np.concatenate([f_val, np.ones(shape=(len(f_val), 1), dtype=f_val.dtype)], axis=1)\n","\n","print('Size of features matrices:')\n","print(f_trn.shape, f_val.shape, f_out.shape)"],"metadata":{"id":"Sk2tDecG84rG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Send all numpy arrays to PyTorch tensors.\n","\n","The main reason we're using torch is because it allows using a GPU for matrix operations.\n","When we have many voxels to fit, this will be way faster than CPU.\n","For just a few voxels, this won't take too long even on a CPU.\n"],"metadata":{"id":"-9W9bEJKsSeD"}},{"cell_type":"code","source":["# v is our fMRI data, these are the 3 different splits\n","vtrn = torch.Tensor(voxel_data_trn[:,voxel_inds]).to(device).to(torch.float64)\n","vout = torch.Tensor(voxel_data_holdout[:,voxel_inds]).to(device).to(torch.float64)\n","vval = torch.Tensor(voxel_data_val[:,voxel_inds]).to(device).to(torch.float64)\n","\n","# x is our features, same 3 splits\n","xtrn = torch.Tensor(f_trn).to(device).to(torch.float64)\n","xout = torch.Tensor(f_out).to(device).to(torch.float64)\n","xval = torch.Tensor(f_val).to(device).to(torch.float64)"],"metadata":{"id":"Juw_v9R8sKmK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Solve for weights using ridge regression\n","\n"],"metadata":{"id":"odZ9pRol9hHp"}},{"cell_type":"markdown","source":["\n","First let's set up the candidate $\\lambda$ (lambda) values. Recall that lambda is the regularization parameter here. It determines strength of regularization (i.e., how strongly do we push weights to zero).\n","\n"," Each value in lambdas is a candidate value for the regularizaton strength."],"metadata":{"id":"9v0bsGfJtCVe"}},{"cell_type":"code","source":["# lambda is the ridge penalty, bigger = more regularization\n","n_lambdas = 20\n","lambdas = np.logspace(np.log(0.0001),np.log(10**8+0.01),n_lambdas, dtype=np.float32, base=np.e) - 0.01"],"metadata":{"id":"nNw3RtCz9ii3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Solve for the weights for each possible lambda. Here we loop over the candidate values, and we get the loss for each one.\n","\n","\n","The regression problem is:\n","\n","$$y = X w$$\n","\n","where:\n","- $y$ = voxel data\n","- $X$ = design matrix (features)\n","- $w$ = regression weights\n","\n","To solve using ridge regression (L2-regularization):\n","\n","$$\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y$$\n","\n","where:\n","- $X^T$ = transpose of $X$\n","- $\\lambda$ = regularization parameter\n","- $I$ = identity matrix\n","- $(X^T X + \\lambda I)^{-1}$ = matrix inverse\n"],"metadata":{"id":"jdAl7bfRgHp0"}},{"cell_type":"markdown","source":["**Note:** These equations look a lot like the general linear model (GLM) methdod that we saw last week, used to estimate fMRI responses. The modeling here is a very similar idea to the GLM (except for the addition of the ridge constraint), but it is working with the output of a GLM that has already been run. That is, the fMRI data in Y consists of $\\beta$ weights that were extracted using a separate GLM. So we're running a linear model on the output of another linear model."],"metadata":{"id":"sTunw-rXtPwg"}},{"cell_type":"code","source":["losses_per_lambda = np.zeros((n_lambdas, 1))\n","n_features = xtrn.size()[1]\n","weights_per_lambda = np.zeros((n_lambdas, n_features))\n","\n","# loop over all my possible lambda values\n","for li, ll in enumerate(lambdas):\n","\n","  # make my identity matrix: this goes into the ridge formula\n","  identity_matrix = torch.eye(n_features, device=device, dtype=torch.float64)\n","\n","  # solve for the beta weights\n","  # computes: w = (X^T X + Î»I)^(-1) X^T y\n","  weights = torch.linalg.solve(xtrn.T @ xtrn + ll*identity_matrix, xtrn.T @ vtrn)\n","  # weights is [n_features x n_voxels]\n","\n","  # predict the response on held-out data, using features from held-out data (xout)\n","  pred = xout @ weights\n","  # pred is [n_images x n_voxels]\n","\n","  # compute loss for held-out data (sum of squared error)\n","  # this will tell us the loss for each of the possible lambda values\n","  loss = torch.sum(torch.pow(vout - pred, 2), dim=0)\n","\n","  losses_per_lambda[li,:] = loss.item()\n","\n","  weights_per_lambda[li,:] = weights[:, 0]\n"],"metadata":{"id":"Q7Vc1eQOE4oa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","***Question 2:***\n","\n","Write some code to find the best lambda value out of the candidate values (lowest loss). Print out the following:\n","- index of the best lambda\n","- the lambda value itself\n","- associated loss for the best lambda\n","- associated weights for the best lambda\n","\n"],"metadata":{"id":"4bEpoM1Tglpo"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"e0MWWQRWhDOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"qdhc134_hDrc"}},{"cell_type":"markdown","source":["Now let's visualize a plot of loss versus the regularization strength (lambda). The minimum on this plot is what we want to use."],"metadata":{"id":"PhdS4hQBfadb"}},{"cell_type":"code","source":["loss_array = losses_per_lambda\n","\n","plt.figure(figsize=(4,3))\n","plt.plot(lambdas, loss_array, '.-')\n","\n","plt.title('loss vs ridge params')\n","plt.xlabel('lambda parameter')\n","plt.ylabel('Sum of Squared Error')\n","# plt.yscale('log')\n","plt.xscale('log')\n","\n","vi = 0;\n","best_lambda_ind = np.argmin(losses_per_lambda)\n","best_lambda = lambdas[best_lambda_ind]\n","\n","plt.plot(best_lambda, loss_array[best_lambda_ind], 'o', color='r')"],"metadata":{"id":"sQnPbgqXeDT1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 4: Predict responses on held-out data"],"metadata":{"id":"DB4JTYyn-ypG"}},{"cell_type":"code","source":["# remember that we need to get the weights corresponding to the best lambda value\n","# (you already found these above)\n","weights_use = weights_per_lambda[best_lambda_ind,:]\n","\n","# predict voxel response in held-out validation data here.\n","# yhat = X @ W\n","pred = xval @ weights_use[:,None]\n","\n","# remember to turn these back into numpy, from torch.\n","# sometimes tensors will give errors in your subsequent numpy code.\n","actual_array = vval.numpy()\n","pred_array = pred.numpy()"],"metadata":{"id":"GJGCDrlB-0NB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can evaluate how good the predictions are."],"metadata":{"id":"ypjIW153iaxX"}},{"cell_type":"code","source":["# first, let's write a function to compute R2\n","def get_r2(actual,predicted):\n","    \"\"\"\n","    This computes the coefficient of determination (R2).\n","    Always goes along first dimension (i.e. the trials/samples dimension)\n","    \"\"\"\n","    ssres = np.sum(np.power((predicted - actual),2), axis=0);\n","    sstot = np.sum(np.power((actual - np.mean(actual, axis=0)),2), axis=0);\n","    r2 = 1-(ssres/sstot)\n","\n","    return r2"],"metadata":{"id":"qSkhKptJ-7XU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 3:***\n","\n","Write some code to print out the following:\n","- R2 for the model on the validation set\n","- Correlation coefficient for the model on the validation set"],"metadata":{"id":"anpXHd5_ifFK"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"5B7Y8qBPi0KX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"CRa4d9Dqi1RK"}},{"cell_type":"markdown","source":["Let's visualize the actual and predicted responses for our example voxel."],"metadata":{"id":"lSZyFR1VjNXo"}},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","plt.plot(actual_array[:,0])\n","plt.plot(pred_array[:,0])\n","\n","# i'm zooming in on just the first part of this plot, to see better\n","plt.xlim([0, 200])\n","plt.legend(['Actual','Predicted'])\n","plt.xlabel('Image number')\n","plt.ylabel('Voxel response')\n","\n","plt.title('Voxel %d, $R^2$ = %.2f'%(voxel_inds[0], get_r2(actual_array, pred_array)))"],"metadata":{"id":"ZyshUg9q_gI9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 5: Run a batch of voxels\n","\n","Now that we've tested this out for one voxel, let's see how things look across a larger set of voxels."],"metadata":{"id":"x7TAYChQu6bW"}},{"cell_type":"markdown","source":["Use the above function and pull out data from our target ROI."],"metadata":{"id":"KL0k2XOvz4zs"}},{"cell_type":"code","source":["# still using FFA here, but can use anything.\n","target_roi_name = 'FFA-1'\n","\n","voxel_inds = get_roi_vox(roi_name = target_roi_name)\n","\n","print('%s has %d voxels'%(target_roi_name, np.sum(voxel_inds)))\n","\n","# v is our fMRI data, these are the 3 different splits\n","vtrn = torch.Tensor(voxel_data_trn[:,voxel_inds]).to(device).to(torch.float64)\n","vout = torch.Tensor(voxel_data_holdout[:,voxel_inds]).to(device).to(torch.float64)\n","vval = torch.Tensor(voxel_data_val[:,voxel_inds]).to(device).to(torch.float64)\n"],"metadata":{"id":"tjUC5VfZ9oJX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, define a function that allows us to solve the ridge problem for many voxels together. The implementation is a bit different than how we did it above, because we want to vectorize the code for speed. But it's doing the same math as we looked at before."],"metadata":{"id":"lUH01ilZ0rXr"}},{"cell_type":"code","source":["def solve_ridge_fast(xtrn, vtrn, lambdas):\n","\n","  n_features = xtrn.shape[1]\n","  # xT * x\n","  mult = xtrn.T @ xtrn\n","\n","  # make an identity matrix\n","  ridge_term = torch.eye(xtrn.size()[1], device=device, dtype=torch.float64)\n","\n","  # make versions of this matrix that are adjusted by each possible lambda value\n","  # this is: (X^T @ X + lambda*I)^(-1)\n","  # first dim is the different lambda values.\n","  lambda_matrices = torch.stack([(mult+ridge_term*l).inverse() \\\n","            for l in lambdas], axis=0)\n","\n","  cofactor = torch.tensordot(lambda_matrices, xtrn, dims=[[2],[1]])\n","\n","  # solve for weights\n","  weights = torch.tensordot(cofactor, vtrn, dims=[[2], [0]]) # [#lambdas, #feature, #voxel]\n","\n","  # predict the response on held-out data, using features from held-out data (xout)\n","  pred = torch.tensordot(xout, weights, dims=[[1],[1]]) # [#samples, #lambdas, #voxels]\n","\n","  # compute loss for held-out data\n","  # this will tell us the loss for each of the possible lambda values\n","  loss = torch.sum(torch.pow(vout[:,None,:] - pred, 2), dim=0) # [#lambdas, #voxels]\n","\n","  weights_use = torch.zeros((n_features, vtrn.shape[1]),dtype=weights.dtype)\n","\n","  # for each voxel, find its best weights\n","  for vi in range(vtrn.shape[1]):\n","    # choose the best lambda value, based on min loss\n","    best_lambda_ind = np.argmin(loss[:,vi])\n","    best_lambda = lambdas[best_lambda_ind]\n","    # print(vi, best_lambda_ind, best_lambda)\n","    weights_use[:, vi] = weights[best_lambda_ind,:,vi]\n","\n","  return weights_use"],"metadata":{"id":"YF3Pr85V9plV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use this function to solve for our ridge weights, for all voxels."],"metadata":{"id":"G5CFNaGB09BF"}},{"cell_type":"code","source":["st = time.time()\n","weights_use = solve_ridge_fast(xtrn, vtrn, lambdas)\n","elapsed = time.time() - st\n","print('elapsed = %.5f s'%elapsed)"],"metadata":{"id":"pruFizsMxXlc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, predict the held-out data."],"metadata":{"id":"aww3hrU_1SIG"}},{"cell_type":"code","source":["# predict voxel response in held-out validation data here.\n","# yhat = X @ W\n","pred = xval @ weights_use\n","\n","# remember to turn these back into numpy, from torch.\n","# sometimes tensors will give errors in your subsequent numpy code.\n","actual_array = vval.numpy()\n","pred_array = pred.numpy()"],"metadata":{"id":"bkINdeZ9wUyb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute R2 for the model fit, for each voxel:"],"metadata":{"id":"trZh2OJa1H2z"}},{"cell_type":"code","source":["r2 = get_r2(actual_array, pred_array)"],"metadata":{"id":"6JhW1i6sxHxQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","***Question 4:***\n","\n","Print out the maximum, minimum, and median values of $R^2$, across this set of voxels."],"metadata":{"id":"cdr8R6Ee1vaE"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"n--UNupW12MJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"pKoOmRbI13jX"}},{"cell_type":"markdown","source":["Make a plot of R2 versus the noise ceiling."],"metadata":{"id":"Zy_h5oPe1ceo"}},{"cell_type":"code","source":["nc = noise_ceiling[voxel_inds]\n","\n","plt.figure(figsize=(4, 4))\n","plt.plot(nc, r2,'.')\n","plt.axhline(0, color=[0.8, 0.8, 0.8])\n","plt.axvline(0, color=[0.8, 0.8, 0.8])\n","plt.xlim([-0.1, 1])\n","plt.ylim([-0.1, 1])\n","plt.plot([-0.1, 1], [-0.1, 1], '-', color='k')\n","plt.xlabel('Noise Ceiling')\n","plt.ylabel('Actual R2')\n","\n","plt.title('%s'%target_roi_name)"],"metadata":{"id":"auCGg6H3xdBG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","***Question 5:***\n","\n","What relationship do you notice between $R^2$ and the noise ceiling (NC)? Why do they have this relationship? What does this tell us about the overall goodness of the model fit?"],"metadata":{"id":"H6BeW0bR1e9L"}},{"cell_type":"markdown","source":["[answer here]"],"metadata":{"id":"1U4e76ok2Awh"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"5uRQhfcO2B88"}},{"cell_type":"markdown","source":["***Question 6:***\n","\n","Run the model fitting process for a different ROI. To do this you can copy and paste some of the code from the above sections into the below section and modify it.\n","\n","For a list of possible ROIs, check back at the top of the code, under the cell \"Let's explore the ROIs in this dataset.\" For example, you could try PPA, or hV4.\n","\n","Then, do the following:\n","- Print the min, max, and median $R^2$ for the voxels in this new ROI\n","- Make a scatter plot of $R^2$ versus noise ceiling (just like the one a few cells above) with your new ROI.\n","- Interpret this: how do your results compare to the FFA results?\n"],"metadata":{"id":"gxoaHHLZ2EX4"}},{"cell_type":"code","source":["# [answer here]"],"metadata":{"id":"NwrqfBYS2DSL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"BXMz8R4W-gun"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"1f9NlmiBrFdXbDEZgVtLzqO7GDyVu8iB1","timestamp":1759854758070},{"file_id":"1DtFzI_0NLMqd0X9-7YDbUDj8KEmcr9Cg","timestamp":1759107107904},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/cad5020cab595c3bf83a518b7e4d4125/neural_style_tutorial.ipynb","timestamp":1755445164657}],"gpuType":"T4","toc_visible":true}},"nbformat":4,"nbformat_minor":0}