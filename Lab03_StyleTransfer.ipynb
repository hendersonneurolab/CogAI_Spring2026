{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hendersonneurolab/CogAI_Spring2026/blob/master/Lab03_StyleTransfer.ipynb)\n","\n","## Week 3: Style Transfer with CNNs\n","\n","In this tutorial, we'll implement a version of style transfer: a neural network algorithm for mapping the style of one image onto the content of another image.\n","\n","1. Start with a pre-trained CNN (VGG-19), modify it by adding a \"style loss\" and a \"content loss\".\n","2. Optimize an image to minimize these losses.\n","3. Explore the possible images that can be created.\n","\n","**Learning objectives:**\n","- Know the difference between \"style\" (i.e., texture) and \"content\" (i.e., shape), and how these are computed from CNN activations.\n","- Understand how we can use image optimization (via gradient descent) to generate synthetic images.\n","\n"],"metadata":{"id":"nbtEAybij279"}},{"cell_type":"markdown","source":["\n","Credit: This code was adapted from a [PyTorch tutorial](https://docs.pytorch.org/tutorials/advanced/neural_style_tutorial.html)\n","\n","**Original Author**: [Alexis Jacq](https://alexis-jacq.github.io)\n","\n","**Edited by**: [Winston Herring](https://github.com/winston6)\n"],"metadata":{"id":"8R_-3LKdkTxH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLvyFfm_r2s0"},"outputs":[],"source":["# Import what we need: torch, torchvision\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import torchvision.transforms as transforms\n","from torchvision.models import vgg19, VGG19_Weights\n","\n","import copy\n","import requests\n","import os\n","\n","# Check if CUDA (GPU) is available - this will speed up training significantly\n","# If it says \"cpu\", use the menu at top right to select: \"change runtime type\"\n","# Then choose: T4 GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')\n","if torch.cuda.is_available():\n","    print(f'GPU: {torch.cuda.get_device_name(0)}')\n"]},{"cell_type":"markdown","source":["##Step 1: Load the style and content images\n","\n","We're going to implement the [\"neural algorithm of artistic style\"](https://arxiv.org/abs/1508.06576). This is a CNN-based algorithm that generates an image with the \"style\" of one image, and the \"content\" of another.\n","\n","To do this, we need a style source image and a content source image.\n","\n","Let's start with some examples for now."],"metadata":{"id":"sNUUYJe9x_Qx"}},{"cell_type":"code","source":["# First, mount your Google Drive (if not already mounted)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to the Colab Notebooks folder\n","colab_notebooks_path = '/content/drive/MyDrive/Colab Notebooks/'\n","os.chdir(colab_notebooks_path)\n","os.makedirs('CogAI', exist_ok=True)\n","os.makedirs('CogAI/images', exist_ok=True)\n","\n","images_folder = os.path.join(colab_notebooks_path, 'CogAI', 'images')\n","print(images_folder)"],"metadata":{"id":"HPQ-Zkutt7sC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper function for image downloads\n","def download_image(url, filepath):\n","    try:\n","        headers = {\n","              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n","          }\n","        response = requests.get(url, headers=headers, timeout=10)\n","        # response = requests.get(url, timeout=10)\n","        response.raise_for_status()  # Raises exception for bad status codes\n","\n","        # Verify it's an image\n","        content_type = response.headers.get('content-type', '')\n","        if not content_type.startswith('image/'):\n","            print(f\"Warning: Content-Type is {content_type}, not an image\")\n","            return False\n","\n","        with open(filepath, 'wb') as f:\n","            f.write(response.content)\n","\n","        # Verify file size\n","        if os.path.getsize(filepath) < 100:  # Very small files are likely errors\n","            print(\"Warning: Downloaded file is suspiciously small\")\n","            return False\n","\n","        return True\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Request failed: {e}\")\n","        return False"],"metadata":{"id":"RAYYSOGX1FE9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Your image URLs\n","# These are just images from the internet - you can use your own too.\n","image_urls = [\"https://apollo-magazine.com/wp-content/uploads/2021/09/Web-lead-image_FINAL_art_diary2.jpg?w=1000\", \\\n","              \"https://birdlifedata.blob.core.windows.net/species-images/22697748.jpg\"]\n","\n","names = ['Style_Cubism.jpg','Content_Penguin.jpg']\n","\n","for url, name in zip(image_urls, names):\n","\n","  # filename = url.split(os.sep)[-1]\n","  file_path = os.path.join(images_folder, name)\n","\n","  file_path = os.path.join(images_folder, name)\n","  print(file_path)\n","  success = download_image(url, file_path)\n","  print('Success = %s'%success)\n"],"metadata":{"id":"EcoMO1GBtVgX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's create some code to process the images. We need to resize them to the same size."],"metadata":{"id":"xB5iZ4Ph2SZm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQcMr04cr2s1"},"outputs":[],"source":["# desired size of the output image\n","imsize = 512 if torch.cuda.is_available() else 128  # use small size if no GPU\n","\n","loader = transforms.Compose([\n","    transforms.Resize(imsize),  # scale imported image\n","    transforms.CenterCrop(imsize),  # crop imported image\n","    transforms.ToTensor()])  # transform it into a torch tensor\n","\n","\n","def image_loader(image_name):\n","    image = Image.open(image_name)\n","    image = image.convert('RGB')\n","    # fake batch dimension required to fit network's input dimensions\n","    image = loader(image).unsqueeze(0)\n","    return image.to(device, torch.float)\n","\n","style_image_path = os.path.join(images_folder, \"Style_Cubism.jpg\")\n","content_image_path = os.path.join(images_folder, \"Content_Penguin.jpg\")\n","print('Style image: %s'%style_image_path)\n","print('Content image: %s'%content_image_path)\n","\n","style_img = image_loader(style_image_path)\n","content_img = image_loader(content_image_path)\n","\n","assert style_img.size() == content_img.size(), \\\n","    \"we need to import style and content images of the same size\""]},{"cell_type":"markdown","metadata":{"id":"7JHS2Sszr2s1"},"source":["Now, let\\'s create a function that displays an image, to ensure they're loaded correctly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceNHAYvur2s1"},"outputs":[],"source":["unloader = transforms.ToPILImage()  # reconvert into PIL image\n","\n","plt.ion()\n","\n","def imshow(tensor, title=None):\n","    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n","    image = image.squeeze(0)      # remove the fake batch dimension\n","    image = unloader(image)\n","    plt.imshow(image)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001) # pause a bit so that plots are updated\n","\n","\n","plt.figure()\n","imshow(style_img, title='Style Image')\n","\n","plt.figure()\n","imshow(content_img, title='Content Image')"]},{"cell_type":"markdown","metadata":{"id":"q1q6g-Hxr2s2"},"source":["##Step 2: Load the CNN\n","\n","To run style transfer, we will start with a pre-trained CNN model. We'll then modify it to support our algorithm. The reason for using an already-trained model here is that it has already learned complex and descriptive image features during its training. Therefore, we can use these learned features in order to guide our synthesis procedure toward complex image properties.\n","\n","As in the original paper, we'll use a pre-trained\n","[VGG-19](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.vgg19.html) model -- but in principle, any CNN would work.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc-5kPfCr2s2"},"outputs":[],"source":["cnn = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n","cnn = cnn.to(device)\n","cnn"]},{"cell_type":"markdown","source":["Check out the number of layers. Why do we think it's called VGG-19?"],"metadata":{"id":"Glrp90Mj4fb2"}},{"cell_type":"markdown","source":["\n","PyTorch's implementation of VGG is a module divided into two child\n","`Sequential` modules: `features` (containing convolution and pooling\n","layers), and `classifier` (containing fully connected layers). We will\n","use the `features` module because we need the output of the individual\n","convolution layers to measure content and style loss. Some layers have\n","different behavior during training than evaluation, so we must set the\n","network to evaluation mode using `.eval()`.\n"],"metadata":{"id":"9zFCXVq95OPZ"}},{"cell_type":"markdown","metadata":{"id":"qdZwTNkIr2s2"},"source":["\n","Next, we need to set up a module for image normalization, which is another image pre-processing step. VGG networks are trained on images with each channel\n","normalized by mean=\\[0.485, 0.456, 0.406\\] and std=\\[0.229, 0.224,\n","0.225\\]. We will use them to normalize the image before sending it into\n","the network.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOX0odD2r2s2"},"outputs":[],"source":["cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n","cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n","\n","# create a module to normalize input image so we can easily put it in a\n","# ``nn.Sequential``\n","class Normalization(nn.Module):\n","    def __init__(self, mean, std):\n","        super(Normalization, self).__init__()\n","        # .view the mean and std to make them [C x 1 x 1] so that they can\n","        # directly work with image Tensor of shape [B x C x H x W].\n","        # B is batch size. C is number of channels. H is height and W is width.\n","        self.mean = torch.tensor(mean).view(-1, 1, 1)\n","        self.std = torch.tensor(std).view(-1, 1, 1)\n","\n","    def forward(self, img):\n","        # normalize ``img``\n","        return (img - self.mean) / self.std"]},{"cell_type":"markdown","source":["##Step 3: Create the loss functions\n","\n","To synthesize our target image, we're aiming to minimize the loss (i.e., difference) between our \"source\" images which we just loaded, and the generated image that we're working on optimizing. The loss is defined in two parts:\n","\n","- **Content loss:** This captures the distance between feature maps corresponding to source and input images, at a specified set of network layers.  \n","  - For content loss we use deeper CNN layers, because they retain more high-level object & scene content.\n","  - Content loss *takes into account spatial position* -- it retains which features are where.\n","- **Style loss:** This captures the distances between Gram matrices (computed from feature maps) corresponding to source and target images.\n","  - Gram matrix: based on dot product between different CNN feature maps at a given layer (see later sections for more).\n","  - Style loss *discards spatial position* -- it only retains \"texture-like\" information about which features are present anywhere. Like a \"summary statistic\" representation.\n","\n","As we optimize the image, each of these losses will go down, as the image becomes more aligned with each of the source images.\n"],"metadata":{"id":"Y8VuKShd3MsS"}},{"cell_type":"markdown","metadata":{"id":"Z6g4lb-rr2s1"},"source":["####First, compute the content loss:\n","\n","This is a new module that we're going to attach to the end of the CNN layers in our pre-trained model. It computes the content distance based on that layer.\n","\n","To get this, we'll compute mean-squared error (MSE) between the feature maps for our source content image ($C$) and our input image ($X$), as:\n","$\\|F_{XL} - F_{CL}\\|^2$\n","\n","Where $F_{XL}$\n"," is the feature map from layer $L$ processing input $X$, and $F_{CL}$ is the feature map from layer $L$ processing input $C$. This can be computed using `nn.MSELoss`.\n","\n","The total content loss will be computed as a weighted sum of the content distance (MSE) for individual layers.\n","\n","We will add this content loss module directly after the convolution\n","layer(s) that are being used to compute the content distance. This way\n","each time the network is fed an input image, the content losses will be\n","computed at the desired layers and because of autograd, all the\n","gradients will be computed. Now, in order to make the content loss layer\n","transparent we must define a `forward` method that computes the content\n","loss and then returns the layer's input. The computed loss is saved as a\n","parameter of the module.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHuSaJSDr2s1"},"outputs":[],"source":["class ContentLoss(nn.Module):\n","\n","    def __init__(self, target,):\n","        super(ContentLoss, self).__init__()\n","        # we 'detach' the target content from the tree used\n","        # to dynamically compute the gradient: this is a stated value,\n","        # not a variable. Otherwise the forward method of the criterion\n","        # will throw an error.\n","        self.target = target.detach()\n","\n","    def forward(self, input):\n","        self.loss = F.mse_loss(input, self.target)\n","        return input"]},{"cell_type":"markdown","metadata":{"id":"Oe6ex_dzr2s1"},"source":["####Next, compute the style loss:\n","\n","The style loss module is implemented similarly to the content loss\n","module. It will be attached to CNN layers to compute\n","the style loss of each layer.\n","\n","In order to calculate the style loss, we\n","need to compute the **gram matrix $G_{XL}$**. A gram matrix is the result of\n","multiplying a given matrix by its transposed matrix. In this application,\n","the given matrix is a reshaped version of the feature maps $F_{XL}$ of a\n","layer $L$. $F_{XL}$ is reshaped to form $\\hat{F}_{XL}$, a $K$x$N$\n","matrix, where $K$ is the number of feature maps at layer $L$ and $N$ is\n","the length of any vectorized feature map $F_{XL}^k$. For example, the\n","first line of $\\hat{F}_{XL}$ corresponds to the first vectorized feature\n","map $F_{XL}^1$.\n","\n","We then compute $\\hat{F}_{XL} \\cdot \\hat{F}_{XL}^T$\n","\n","Finally, the gram matrix must be normalized by dividing each element by\n","the total number of elements in the matrix. This normalization is to\n","counteract the fact that $\\hat{F}_{XL}$ matrices with a large $N$\n","dimension yield larger values in the Gram matrix. These larger values\n","will cause the first layers (before pooling layers) to have a larger\n","impact during the gradient descent. Style features tend to be in the\n","deeper layers of the network so this normalization step is crucial.\n","\n","$G_{XL}$ is now  [$K$ x $K$], where $K$ is the number of feature maps. Notice, it no longer has a spatial dimension!\n"]},{"cell_type":"markdown","source":["Intuitively: the gram matrix takes a dot product between each pair of feature maps, looking at how correlated they are. This is like asking: how much do the different CNN feature maps each align with one another? Which pairs of feature maps spatially co-occur?"],"metadata":{"id":"OkXrwotD867Q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYU6Get6r2s1"},"outputs":[],"source":["def gram_matrix(input):\n","    a, b, c, d = input.size()  # a=batch size(=1)\n","    # b=number of feature maps\n","    # (c,d)=dimensions of a f. map (N=c*d)\n","\n","    features = input.view(a * b, c * d)  # resize F_XL into \\hat F_XL\n","\n","    G = torch.mm(features, features.t())  # compute the gram product\n","\n","    # we 'normalize' the values of the gram matrix\n","    # by dividing by the number of element in each feature maps.\n","    return G.div(a * b * c * d)"]},{"cell_type":"markdown","metadata":{"id":"fqPcwbZer2s1"},"source":["Now the style loss module looks almost exactly like the content loss\n","module. The style distance is also computed using the mean square error\n","between $G_{XL}$ and $G_{SL}$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlxLhni2r2s1"},"outputs":[],"source":["class StyleLoss(nn.Module):\n","\n","    def __init__(self, target_feature):\n","        super(StyleLoss, self).__init__()\n","        self.target = gram_matrix(target_feature).detach()\n","\n","    def forward(self, input):\n","        G = gram_matrix(input)\n","        self.loss = F.mse_loss(G, self.target)\n","        return input"]},{"cell_type":"markdown","source":["Next, let's make a function to compute these losses, and combine them across layers."],"metadata":{"id":"67GQExsfXACo"}},{"cell_type":"markdown","metadata":{"id":"hB502vpAr2s2"},"source":["A `Sequential` module contains an ordered list of child modules. For\n","instance, `vgg19.features` contains a sequence (`Conv2d`, `ReLU`,\n","`MaxPool2d`, `Conv2d`, `ReLU`...) aligned in the right order of depth.\n","We need to add our content loss and style loss layers immediately after\n","the convolution layer they are detecting. To do this we must create a\n","new `Sequential` module that has content loss and style loss modules\n","correctly inserted.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAE5XtzAr2s2"},"outputs":[],"source":["# desired depth layers to compute style/content losses :\n","content_layers_default = ['conv_4']\n","style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n","\n","# ^ you can try changing the above to a different set of layers\n","# and see how the results differ...\n","\n","\n","def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n","                               style_img, content_img,\n","                               content_layers=content_layers_default,\n","                               style_layers=style_layers_default):\n","    # normalization module\n","    normalization = Normalization(normalization_mean, normalization_std)\n","\n","    # just in order to have an iterable access to or list of content/style\n","    # losses\n","    content_losses = []\n","    style_losses = []\n","\n","    # assuming that ``cnn`` is a ``nn.Sequential``, so we make a new ``nn.Sequential``\n","    # to put in modules that are supposed to be activated sequentially\n","    model = nn.Sequential(normalization)\n","\n","    i = 0  # increment every time we see a conv\n","    for layer in cnn.children():\n","        if isinstance(layer, nn.Conv2d):\n","            i += 1\n","            name = 'conv_{}'.format(i)\n","        elif isinstance(layer, nn.ReLU):\n","            name = 'relu_{}'.format(i)\n","            # The in-place version doesn't play very nicely with the ``ContentLoss``\n","            # and ``StyleLoss`` we insert below. So we replace with out-of-place\n","            # ones here.\n","            layer = nn.ReLU(inplace=False)\n","        elif isinstance(layer, nn.MaxPool2d):\n","            name = 'pool_{}'.format(i)\n","        elif isinstance(layer, nn.BatchNorm2d):\n","            name = 'bn_{}'.format(i)\n","        else:\n","            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n","\n","        model.add_module(name, layer)\n","\n","        if name in content_layers:\n","            # add content loss:\n","            target = model(content_img).detach()\n","            content_loss = ContentLoss(target)\n","            model.add_module(\"content_loss_{}\".format(i), content_loss)\n","            content_losses.append(content_loss)\n","\n","        if name in style_layers:\n","            # add style loss:\n","            target_feature = model(style_img).detach()\n","            style_loss = StyleLoss(target_feature)\n","            model.add_module(\"style_loss_{}\".format(i), style_loss)\n","            style_losses.append(style_loss)\n","\n","    # now we trim off the layers after the last content and style losses\n","    for i in range(len(model) - 1, -1, -1):\n","        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n","            break\n","\n","    model = model[:(i + 1)]\n","\n","    return model, style_losses, content_losses"]},{"cell_type":"markdown","source":["##Step 4: Synthesize the image\n","\n","To create our desired image, the first step is to start with an input.\n","Let's try starting with a random image here. You can also experiment with starting from something else, like the content image itself. Sometimes starting from a real image can give better results.\n","\n","From this starting image, we will iteratively perturb it, meaning we'll slowly change its pixels step-by-step. We'll continue until it turns into what we're looking for.\n"],"metadata":{"id":"zz9yG8bIQvHP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIhLxwQar2s2"},"outputs":[],"source":["input_img = torch.randn(content_img.data.size()).to(device)\n","# input_img = content_img.clone()\n","\n","# add the original input image to the figure:\n","plt.figure()\n","imshow(input_img, title='Input Image')"]},{"cell_type":"markdown","metadata":{"id":"w9Ye6nM9r2s2"},"source":["Next, we'll create our optimizer. This is the algorithm that tells us how to update the image pixels based on the loss function, using gradient descent.\n","\n","Here, we're using the [L-BFGS](https://docs.pytorch.org/docs/stable/generated/torch.optim.LBFGS.html) optimizer, which works well for image synthesis.\n","\n","**Important note:** This is different from how optimization works when we are training a neural network. In the usual case of network training (like we did last week), we were updating the *weights* of the model in response to images. Here, we're actually leaving the model weights unchanged, and we're updating the image *pixels*. The model stays the same, but our image will change into something new!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKGUtZ37r2s2"},"outputs":[],"source":["def get_input_optimizer(input_img):\n","    # this line to show that input is a parameter that requires a gradient\n","    optimizer = optim.LBFGS([input_img])\n","    return optimizer"]},{"cell_type":"markdown","source":["Now let's create a function to put this all together.\n","\n","Similarly to model training... we'll make a loop over training steps. At each step, we'll get the loss (based on our style and content losses defined above). We'll use the ``optimizer.backward()`` function to compute gradients. The gradients tell us how much to change the input image, to decrease the loss.\n","\n","This particular optimizer requires that we define a \"closure\" function, which gets called during optimization. It re-evaluates the model and returns the loss.\n","\n","We still have one final constraint to address. The network may try to\n","optimize the input with values that exceed the 0 to 1 tensor range for\n","the image. We can address this by correcting the input values to be\n","between 0 to 1 each time the network is run.\n"],"metadata":{"id":"cjmRcXYwWODv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3fQ8uwIr2s2"},"outputs":[],"source":["def run_style_transfer(cnn, normalization_mean, normalization_std,\n","                       content_img, style_img, input_img, num_steps=300,\n","                       style_weight=1000000, content_weight=1, \\\n","                       content_layers=content_layers_default,\n","                       style_layers=style_layers_default\n","                       ):\n","    \"\"\"Run the style transfer.\"\"\"\n","    print('Building the style transfer model..')\n","    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n","        normalization_mean, normalization_std, style_img, content_img, \\\n","        content_layers=content_layers,\n","        style_layers=style_layers)\n","\n","    # We want to optimize the input and not the model parameters so we\n","    # update all the requires_grad fields accordingly\n","    input_img.requires_grad_(True)\n","    # We also put the model in evaluation mode, so that specific layers\n","    # such as dropout or batch normalization layers behave correctly.\n","    model.eval()\n","    model.requires_grad_(False)\n","\n","    optimizer = get_input_optimizer(input_img)\n","\n","    print('Optimizing..')\n","    run = [0]\n","    while run[0] <= num_steps:\n","\n","        def closure():\n","            # correct the values of updated input image\n","            with torch.no_grad():\n","                input_img.clamp_(0, 1)\n","\n","            optimizer.zero_grad()\n","            model(input_img)\n","            style_score = 0\n","            content_score = 0\n","\n","            for sl in style_losses:\n","                style_score += sl.loss\n","            for cl in content_losses:\n","                content_score += cl.loss\n","\n","            # print(style_score, content_score)\n","            style_score *= style_weight\n","            content_score *= content_weight\n","\n","            loss = style_score + content_score\n","            loss.backward()\n","            # compute the gradients here.\n","\n","            run[0] += 1\n","            if run[0] % 50 == 0:\n","                print(\"run {}:\".format(run))\n","                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n","                    style_score.item(), content_score.item()))\n","                print()\n","\n","            return style_score + content_score\n","\n","        optimizer.step(closure)\n","        # this is the optimizer function.\n","        # In here is where the actual pixel values get updated at each step.\n","        # The optimizer algorithm determines how and when to do those updates.\n","\n","    # a last correction...\n","    with torch.no_grad():\n","        input_img.clamp_(0, 1)\n","\n","    return input_img"]},{"cell_type":"markdown","metadata":{"id":"RYkjULGbr2s2"},"source":["Finally, we can run the algorithm.\n","\n","TIP: make sure you're attached to the GPU for this! It will take a very long time on a CPU.\n","\n","(upper right dropdown menu > change runtime type > T4 GPU)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWZi2ekur2s2"},"outputs":[],"source":["output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n","                            content_img, style_img, input_img, style_weight = 10**6, content_weight=10)\n","\n","plt.figure()\n","imshow(output, title='Output Image')\n","\n","plt.ioff()\n","plt.show()"]},{"cell_type":"markdown","source":["## Step 5: Try it yourself"],"metadata":{"id":"K0saViDCiSSA"}},{"cell_type":"markdown","source":["\n","---\n","***Question 1:***\n","\n","\n","Try uploading your own images:\n","\n","Navigate to: https://drive.google.com/drive/my-drive\n","\n","Then navigate to \"Colab Notebooks > CogAI > images\"\n","\n","In this folder, you should see the files \"Style_Cubism.jpg\", and \"Content_Penguin.jpg\"\n","\n","Now upload your own images here: one for style, and one for content.\n"],"metadata":{"id":"KzfTz9LfiYvF"}},{"cell_type":"code","source":["# modify these paths:\n","style_image_path = os.path.join(images_folder, \"your_style.jpg\")\n","content_image_path = os.path.join(images_folder, \"your_content.jpg\")\n","# style_image_path = os.path.join(images_folder, \"Cat1.jpg\")\n","# content_image_path = os.path.join(images_folder, \"Cat2.jpg\")\n","\n","print('Style image: %s'%style_image_path)\n","print('Content image: %s'%content_image_path)\n","\n","style_img = image_loader(style_image_path)\n","content_img = image_loader(content_image_path)\n","\n","plt.figure()\n","imshow(style_img, title='Style Image')\n","\n","plt.figure()\n","imshow(content_img, title='Content Image')"],"metadata":{"id":"TYTFsQa3jTfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_img = torch.randn(content_img.data.size()).to(device)\n","output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n","                            content_img, style_img, input_img, style_weight = 10**6, content_weight=10)\n","\n","plt.figure()\n","imshow(output, title='Output Image')\n","\n","plt.ioff()\n","plt.show()"],"metadata":{"id":"eBHPPGEXrFCx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you get an image you like, save it:"],"metadata":{"id":"ax7VK2i5loL0"}},{"cell_type":"code","source":["output_im = unloader(output[0])\n","save_filename = os.path.join(images_folder, 'My_Style_Transfer.jpg')\n","print(save_filename)\n","output_im.save(save_filename)"],"metadata":{"id":"D-5DwvCqkhHO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Did your image work well? Anything surprising?"],"metadata":{"id":"wIUt0i4npUpS"}},{"cell_type":"markdown","source":["[your answer here]"],"metadata":{"id":"U-0DesitpX24"}},{"cell_type":"markdown","source":["Try running the code again, for the exact same input images. Are the results identical? Why?"],"metadata":{"id":"lNbEGMSLpdK5"}},{"cell_type":"markdown","source":["[your answer here]"],"metadata":{"id":"XupxgRfAqKs8"}},{"cell_type":"markdown","source":["Try switching the style and content images. What happens to the result? Anything surprising?"],"metadata":{"id":"fHzPBoJCpFvp"}},{"cell_type":"markdown","source":["[your answer here]"],"metadata":{"id":"8Ccn39ANpNDX"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"H8oxKl6qmBTv"}},{"cell_type":"markdown","source":["***Question 2:***\n","\n","Experiment with parameters of the synthesis. For example, we can change \"style weight\" and \"content weight\" in this code:"],"metadata":{"id":"Mw-G3MkEn2Lv"}},{"cell_type":"code","source":["input_img = torch.randn(content_img.data.size()).to(device)\n","\n","# style=10**6 and content=10 are the default params - these tend to give a good balance.\n","# the exact numbers are kind of arbitrary. Try turning up and down from these values.\n","style_weight = 10**6\n","content_weight = 10\n","\n","output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n","                            content_img, style_img, input_img, \\\n","                            style_weight = style_weight, \\\n","                            content_weight = content_weight)\n","\n","plt.figure()\n","imshow(output, title='Output Image')\n","\n","# sphinx_gallery_thumbnail_number = 4\n","plt.ioff()\n","plt.show()"],"metadata":{"id":"HyLykhz5khu-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What happens when you change style_weight and content_weight? Which values give you the best results?"],"metadata":{"id":"0R9qUddIo7Zu"}},{"cell_type":"markdown","source":["[your answer here]"],"metadata":{"id":"HtYYAxWdrg0I"}},{"cell_type":"markdown","source":["We can also change the set of CNN layers that we include in the synthesis procedure. By default, conv4 contributes to content loss, and conv1-conv5 contribute to the style loss. Let's try changing the set of layers we include:"],"metadata":{"id":"gZ3U4LEQr04w"}},{"cell_type":"code","source":["input_img = torch.randn(content_img.data.size()).to(device)\n","\n","# this is default:\n","style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n","content_layers = ['conv_4']\n","\n","# try something else...\n","# style_layers = ['conv_5']\n","# content_layers = ['conv_1']\n","\n","\n","style_weight = 10**6\n","content_weight = 10\n","\n","output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n","                            content_img, style_img, input_img, \\\n","                            style_weight = style_weight, \\\n","                            content_weight = content_weight, \\\n","                            style_layers = style_layers, \\\n","                            content_layers = content_layers)\n","\n","plt.figure()\n","imshow(output, title='Output Image')\n","\n","# sphinx_gallery_thumbnail_number = 4\n","plt.ioff()\n","plt.show()"],"metadata":{"id":"zA9QSyLPtsil"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What happens to the images when we change the layers? Why?"],"metadata":{"id":"AxTHKzNQt6kO"}},{"cell_type":"markdown","source":["[your answer here]"],"metadata":{"id":"BG9p4cs2t-Eh"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/cad5020cab595c3bf83a518b7e4d4125/neural_style_tutorial.ipynb","timestamp":1755445164657}],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}